{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7-final"
    },
    "orig_nbformat": 2,
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vu-topics-in-big-data-2023/Team05/blob/main/Fateen_(Copy_of_spark_jobs).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "6ZR3iReaz-_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rRZIkSqT__Dv"
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#install spark. we are using the one that uses hadoop as the underlying scheduler.\n",
        "!wget -q https://downloads.apache.org/spark/spark-3.2.4/spark-3.2.4-bin-hadoop3.2.tgz\n",
        "!tar xf  spark-3.2.4-bin-hadoop3.2.tgz\n",
        "!ls -l\n",
        "\n",
        "#Provides findspark.init() to make pyspark importable as a regular library.\n",
        "os.environ[\"SPARK_HOME\"] = \"spark-3.2.4-bin-hadoop3.2\"\n",
        "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.4.7 pyspark-shell'"
      ],
      "metadata": {
        "id": "_BcbpaYeKNlG",
        "outputId": "adde5661-9d4e-43f1-c113-98176fa58152",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 294136\n",
            "drwxr-xr-x  1 root root      4096 Apr 21 13:38 sample_data\n",
            "drwxr-xr-x 13 1000 1000      4096 Apr  9 21:17 spark-3.2.4-bin-hadoop3.2\n",
            "-rw-r--r--  1 root root 301183180 Apr  9 21:35 spark-3.2.4-bin-hadoop3.2.tgz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q findspark pyspark\n",
        "import findspark\n",
        "findspark.init()"
      ],
      "metadata": {
        "id": "xB4TPZH16jmR",
        "outputId": "ddcb9269-1bad-4ddc-d587-664d99231760",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKheJ7h0AiQW"
      },
      "source": [
        "## Step 1: Upload the three files included in the data folder to colab. \n",
        "\n",
        "Upload the zip version and then use the !unzip command on the shell to unzip them"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Start by connecting gdrive into the google colab\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "vnzD2ayAu7_-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"ReadParquet\").getOrCreate()\n",
        "\n",
        "parquet_file_path = \"/content/gdrive/MyDrive/bigdata_final/month=1/part-00007-0c8142a9-ffb0-4186-8d5f-060d4d2afe7d.c000.snappy.parquet\"\n",
        "\n",
        "df = spark.read.parquet(parquet_file_path)\n",
        "\n",
        "df.show(20)\n"
      ],
      "metadata": {
        "id": "XQq4yLL08Wao"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"ReadParquet\").getOrCreate()\n",
        "\n",
        "parquet_file_path = \"/content/gdrive/MyDrive/bigdata_final/month=1/nfd_incidents_xd_seg.parquet\"\n",
        "\n",
        "df = spark.read.parquet(parquet_file_path)\n",
        "\n",
        "df.show(20)\n"
      ],
      "metadata": {
        "id": "Rp9LBgPLyLIF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import count\n",
        "\n",
        "freq_by_tract = df.groupBy('emdCardNumber').agg(count('Incident_ID').alias('num_incidents'))\n",
        "freq_by_tract.show()\n"
      ],
      "metadata": {
        "id": "e6JWOIAm6L7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot 1: Frequency of incidents by census tract across Nashville.\n",
        "incident_freq = df.groupby('census_tract').agg({'incident_id': 'count'}).reset_index()\n",
        "fig1 = go.Choroplethmapbox(\n",
        "    geojson=census_tract_geojson,  # assuming you have the geojson file for census tracts\n",
        "    locations=incident_freq['census_tract'],\n",
        "    z=incident_freq['incident_id'],\n",
        "    colorscale='Blues',\n",
        "    zmin=0,\n",
        "    zmax=incident_freq['incident_id'].max(),\n",
        "    marker_opacity=0.5,\n",
        "    marker_line_width=0,\n",
        ")\n",
        "fig1.update_layout(\n",
        "    mapbox_style=\"carto-positron\",\n",
        "    mapbox_zoom=9,\n",
        "    mapbox_center={\"lat\": 36.1627, \"lon\": -86.7816},\n",
        "    margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0},\n",
        ")\n",
        "fig1.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 623
        },
        "id": "5dsvG8Lv6gAv",
        "outputId": "f2880097-b90c-4bb1-e896-38d60a8e6fc7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-275a23514876>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Plot 1: Frequency of incidents by census tract across Nashville.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mincident_freq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'census_tract'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'incident_id'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'count'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m fig1 = go.Choroplethmapbox(\n\u001b[1;32m      4\u001b[0m     \u001b[0mgeojson\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcensus_tract_geojson\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# assuming you have the geojson file for census tracts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mlocations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mincident_freq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'census_tract'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.2.4-bin-hadoop3.2/python/pyspark/sql/group.py\u001b[0m in \u001b[0;36magg\u001b[0;34m(self, *exprs)\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mexprs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"exprs should not be empty\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexprs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexprs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m             \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jgd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexprs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0;31m# Columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mspark-3.2.4-bin-hadoop3.2/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.2.4-bin-hadoop3.2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: cannot resolve 'census_tract' given input columns: [Dist_to_Seg, ID_Original, Incident_ID, XDSegID, day_of_week, emdCardNumber, geometry, latitude, longitude, response_time_sec, time_local, time_utc, weekend_or_not];\n'Aggregate ['census_tract], ['census_tract, count(incident_id#279) AS count(incident_id)#405L]\n+- Relation [ID_Original#269,latitude#270,longitude#271,emdCardNumber#272,time_utc#273,time_local#274,response_time_sec#275,day_of_week#276L,weekend_or_not#277,geometry#278,Incident_ID#279,Dist_to_Seg#280,XDSegID#281] parquet\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total_rows = df.count()\n",
        "print(\"Total number of rows in the dataframe:\", total_rows)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8maKdPG_5WT1",
        "outputId": "f5719a7f-b08e-40d6-ecd0-9a6597b0b895"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of rows in the dataframe: 29765\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "# upload dataset\n",
        "uploaded = files.upload()\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))"
      ],
      "metadata": {
        "id": "9plBOXbHg-pF",
        "outputId": "ab483a16-b35d-43d1-943f-b59cf18c5e7b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-ec20ca4a-cee3-43c2-81c9-a58273ea94ad\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-ec20ca4a-cee3-43c2-81c9-a58273ea94ad\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving nashville-tweets-2019-01-28.zip to nashville-tweets-2019-01-28.zip\n",
            "Saving Salaries.csv.zip to Salaries.csv.zip\n",
            "Saving Batting.csv.zip to Batting.csv.zip\n",
            "User uploaded file \"nashville-tweets-2019-01-28.zip\" with length 3237977 bytes\n",
            "User uploaded file \"Salaries.csv.zip\" with length 142130 bytes\n",
            "User uploaded file \"Batting.csv.zip\" with length 1843868 bytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vwScuHQQAt6-",
        "outputId": "4ef7ebb3-f19a-46bb-d48f-866ed0c2073f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!unzip nashville-tweets-2019-01-28.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  nashville-tweets-2019-01-28.zip\n",
            "  inflating: nashville-tweets-2019-01-28  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VRS3zN-wG1YQ",
        "outputId": "7c0ddfcc-35ce-4af6-d04d-bb345fa2894b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#Note that this is not a csv file. So be careful. Do not load using read.csv in spark\n",
        "!head -3 nashville-tweets-2019-01-28.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PK\u0003\u0004\u0014\u0000\b\u0000\b\u0000\u000bYyR\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000�\u0011~\u0001\u001b\u0000 \u0000nashville-tweets-2019-01-28UT\r\u0000\u0007v�\\`u�\\`��a`ux\u000b\u0000\u0001\u0004�\u0001\u0000\u0000\u0004\u0014\u0000\u0000\u0000�iw�8�.���\u0015HrW�\u0019\u001c�\u0000A\u0002��};�<&�㊝�N��EI�Ę\"\u0015�������\rj )J�dY���T9�\bN�\u0006�<{���� v�����_�\u0012m��l\u0006~\u001c��A\u001c�\u0011|�\u000f<\u000f��Áߴc�\u0005_�m/r�;�\u0007���M�.�پ%�F���EzA�~���aܵI7���n�\u001aW�A�����o�>��\t�p���\t�؎\u0007Q��\u001d:}ox\u001d\u0007���.ޜj\u0016ӄFMiR.uɄ\u000e����ט6�\u001a��2�\u0014\u001a�L8Զo��ͽ���n�:�\b�|9���_\u0005>~�?�L��\f]X\u0012nk�'l�Mu���\u0005(�_uWx�\u0010;e�\u0014�3j���_�v����\u000eÃ�oG����%�&�l���g9��ns���\u001a�l�\"��M?\u000b=�}�H�+���;N���/|�h�k\u0004�zC��kG���L~\u001f��c=���IO9?���rZ�p\u001c_k2�wn\u001c;!\bA��\u0001Z؞W�A�\u001c�W�\u0011�\u0016�W��\r�J��Zn������}�YI_4+Z���\n",
            "�zf�صj�n�oDx�^G]8�s����m9�u�\u001d�\u001a��a��s��K�V��yۯ|�w^f��&�d\u001a�LX#9p�\u0018��g�\u001dG}��O�Ԕ\u0014�q<�����\u000e>s���g���Vh�/7�p�z�I{jd�7à?>\u0001���Wv\u0002�l�\u0018\u0006\n",
            "B<�\u0013԰��O�Tw�\u0007`��k��_���zX�:�D5� l�>,\\���9 p��uO\r:58�)L3���(\u0005��������\u0015:���>~�����;�\u000b�VՅ����_\u0012�eh�\u0007����_����9i\u0007!q/�������e~}[�:��Tjv�g\u0005.�ס�+��\u001f\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EhzxQ6bnAyLS",
        "outputId": "108a2051-d5f1-44c4-c523-d1c9ebd711ba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!unzip Salaries.csv.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  Salaries.csv.zip\n",
            "  inflating: Salaries.csv            \n",
            "  inflating: __MACOSX/._Salaries.csv  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5K6OjtfA2us",
        "outputId": "96dcf19c-704f-4c6b-8ccf-6c8bd0d00f05",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!unzip Batting.csv.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  Batting.csv.zip\n",
            "  inflating: Batting.csv             \n",
            "  inflating: __MACOSX/._Batting.csv  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Write Spark Code Locally and test the Code and Save it to your repository"
      ],
      "metadata": {
        "id": "o1prQTAn7Mbu"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pz7dMIVJ__Dy"
      },
      "source": [
        "# Step 2. Complete Spark Jobs Below Locally. \n",
        "\n",
        "Once they work you can submit them to EMR"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxUib2yt__Dy"
      },
      "source": [
        "## Job 1. Count the number of tweets.\n",
        "\n",
        "I have almost completed this for you. You still have to do the reduce and add - look into the wordcount example. But then use this as the template to finish the rest."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YDL78heZ__Dz",
        "outputId": "79dd2963-7888-4206-e9cd-6a86ed2c8025",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%file 1_count.py\n",
        "'''\n",
        "TODO:\n",
        "Count the number of tweets.\n",
        "Parse tweets with json.loads -- note how the tweets are huge JSON blobs.\n",
        "Ignore tweets that error on load.\n",
        "'''\n",
        "\n",
        "from pyspark import SparkContext, SparkConf\n",
        "import json\n",
        "from operator import add\n",
        "#import other things as required\n",
        "\n",
        "#create spark context. This is very important. Do this similarly for the other parts\n",
        "# Note to read a file directly from s3 into an rdd you may have to do something like this\n",
        "\n",
        "def checkjson(entry):\n",
        "  try:\n",
        "    json.loads(entry)\n",
        "    #if load succeeded. We use correct as the key\n",
        "    return \"correct\", 1\n",
        "  except:\n",
        "    #there was an error in loading. We use incorrect as the key\n",
        "    return \"incorrect\", 1\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  # replace this line with the s3 pass when testing over EMR\n",
        "  # conf = SparkConf().setAppName('1_count').set('spark.hadoop.validateOutputSpecs', False)\n",
        "  # sc = SparkContext(conf=conf).getOrCreate()\n",
        "  sc = SparkContext(appName=\"1_count\")\n",
        "\n",
        "\n",
        "  try:\n",
        "    #@todo: fix the path as required\n",
        "    tweets=sc.textFile('s3://far-bigdata-spark/datasets/nashville-tweets-2019-01-28')\n",
        "    # review the page rank example for how to use the map operation\n",
        "    # review word count for reduce and add\n",
        "    # see how we use map to parse each row\n",
        "    counts = tweets.map(lambda tweet: checkjson(tweet))\n",
        "    print(counts.take(2))\n",
        "\n",
        "    # @todo: create an output rdd that uses reduce by key and add operator to add up the correct entries and incorrect entries\n",
        "    output = counts.reduceByKey(add)\n",
        "\n",
        "    # @todo: the s3 version will have to save it to correct s3 path\n",
        "    output.repartition(1).saveAsTextFile(\"s3://far-bigdata-spark/hw6/1_count.out\")\n",
        "\n",
        "  finally:\n",
        "    # very important: stop the context. Otherwise you may get an error that context is still alive. if you are on colab just restart the runtime if you face problem\n",
        "    #finally is used to make sure the context is stopped even with errors\n",
        "    sc.stop()\n",
        "  \n",
        "\n",
        " \n",
        "  \n",
        "  pass"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting 1_count.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test local Execution Results"
      ],
      "metadata": {
        "id": "PH2TFBJp7q0O"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3y2A34y__Dz",
        "outputId": "9e4424fd-97bc-4fad-cd08-a5ac70e60472",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# execute locally and ensure everything works. If it works you should get the 1_count.out/part-00000 file. \n",
        "!spark-submit --packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.4.7 1_count.py\n",
        "# note the cell magic command %%file 1_count.py is used to create a local copy of the content of cell as a file 1_count.py on colab"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ":: loading settings :: url = jar:file:/content/spark-3.2.3-bin-hadoop3.2/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
            "Ivy Default Cache set to: /root/.ivy2/cache\n",
            "The jars for the packages stored in: /root/.ivy2/jars\n",
            "org.apache.spark#spark-streaming-kafka-0-8_2.11 added as a dependency\n",
            ":: resolving dependencies :: org.apache.spark#spark-submit-parent-44155f9a-1db1-4c37-8095-1d5a9020ab38;1.0\n",
            "\tconfs: [default]\n",
            "\tfound org.apache.spark#spark-streaming-kafka-0-8_2.11;2.4.7 in central\n",
            "\tfound org.apache.kafka#kafka_2.11;0.8.2.1 in central\n",
            "\tfound org.scala-lang.modules#scala-xml_2.11;1.0.2 in central\n",
            "\tfound com.yammer.metrics#metrics-core;2.2.0 in central\n",
            "\tfound org.slf4j#slf4j-api;1.7.16 in central\n",
            "\tfound org.scala-lang.modules#scala-parser-combinators_2.11;1.1.0 in central\n",
            "\tfound com.101tec#zkclient;0.3 in central\n",
            "\tfound log4j#log4j;1.2.17 in central\n",
            "\tfound org.apache.kafka#kafka-clients;0.8.2.1 in central\n",
            "\tfound net.jpountz.lz4#lz4;1.2.0 in central\n",
            "\tfound org.xerial.snappy#snappy-java;1.1.7.5 in central\n",
            "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
            ":: resolution report :: resolve 883ms :: artifacts dl 25ms\n",
            "\t:: modules in use:\n",
            "\tcom.101tec#zkclient;0.3 from central in [default]\n",
            "\tcom.yammer.metrics#metrics-core;2.2.0 from central in [default]\n",
            "\tlog4j#log4j;1.2.17 from central in [default]\n",
            "\tnet.jpountz.lz4#lz4;1.2.0 from central in [default]\n",
            "\torg.apache.kafka#kafka-clients;0.8.2.1 from central in [default]\n",
            "\torg.apache.kafka#kafka_2.11;0.8.2.1 from central in [default]\n",
            "\torg.apache.spark#spark-streaming-kafka-0-8_2.11;2.4.7 from central in [default]\n",
            "\torg.scala-lang.modules#scala-parser-combinators_2.11;1.1.0 from central in [default]\n",
            "\torg.scala-lang.modules#scala-xml_2.11;1.0.2 from central in [default]\n",
            "\torg.slf4j#slf4j-api;1.7.16 from central in [default]\n",
            "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
            "\torg.xerial.snappy#snappy-java;1.1.7.5 from central in [default]\n",
            "\t---------------------------------------------------------------------\n",
            "\t|                  |            modules            ||   artifacts   |\n",
            "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
            "\t---------------------------------------------------------------------\n",
            "\t|      default     |   12  |   0   |   0   |   0   ||   12  |   0   |\n",
            "\t---------------------------------------------------------------------\n",
            ":: retrieving :: org.apache.spark#spark-submit-parent-44155f9a-1db1-4c37-8095-1d5a9020ab38\n",
            "\tconfs: [default]\n",
            "\t0 artifacts copied, 12 already retrieved (0kB/26ms)\n",
            "23/04/13 00:13:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
            "23/04/13 00:13:13 INFO SparkContext: Running Spark version 3.2.3\n",
            "23/04/13 00:13:13 INFO ResourceUtils: ==============================================================\n",
            "23/04/13 00:13:13 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "23/04/13 00:13:13 INFO ResourceUtils: ==============================================================\n",
            "23/04/13 00:13:13 INFO SparkContext: Submitted application: 1_count\n",
            "23/04/13 00:13:13 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "23/04/13 00:13:13 INFO ResourceProfile: Limiting resource is cpu\n",
            "23/04/13 00:13:13 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "23/04/13 00:13:13 INFO SecurityManager: Changing view acls to: root\n",
            "23/04/13 00:13:13 INFO SecurityManager: Changing modify acls to: root\n",
            "23/04/13 00:13:13 INFO SecurityManager: Changing view acls groups to: \n",
            "23/04/13 00:13:13 INFO SecurityManager: Changing modify acls groups to: \n",
            "23/04/13 00:13:13 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\n",
            "23/04/13 00:13:14 INFO Utils: Successfully started service 'sparkDriver' on port 37447.\n",
            "23/04/13 00:13:14 INFO SparkEnv: Registering MapOutputTracker\n",
            "23/04/13 00:13:14 INFO SparkEnv: Registering BlockManagerMaster\n",
            "23/04/13 00:13:14 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "23/04/13 00:13:14 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "23/04/13 00:13:14 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "23/04/13 00:13:14 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-c3a0eade-26e0-44e2-84dc-8955a6bd094c\n",
            "23/04/13 00:13:14 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB\n",
            "23/04/13 00:13:14 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "23/04/13 00:13:15 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
            "23/04/13 00:13:15 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://a1ad50929c8d:4040\n",
            "23/04/13 00:13:15 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar at spark://a1ad50929c8d:37447/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1681344793495\n",
            "23/04/13 00:13:15 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar at spark://a1ad50929c8d:37447/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1681344793495\n",
            "23/04/13 00:13:15 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar at spark://a1ad50929c8d:37447/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1681344793495\n",
            "23/04/13 00:13:15 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar at spark://a1ad50929c8d:37447/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1681344793495\n",
            "23/04/13 00:13:15 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar at spark://a1ad50929c8d:37447/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1681344793495\n",
            "23/04/13 00:13:15 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar at spark://a1ad50929c8d:37447/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1681344793495\n",
            "23/04/13 00:13:15 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar at spark://a1ad50929c8d:37447/jars/com.101tec_zkclient-0.3.jar with timestamp 1681344793495\n",
            "23/04/13 00:13:15 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar at spark://a1ad50929c8d:37447/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1681344793495\n",
            "23/04/13 00:13:15 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar at spark://a1ad50929c8d:37447/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1681344793495\n",
            "23/04/13 00:13:15 INFO SparkContext: Added JAR file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar at spark://a1ad50929c8d:37447/jars/log4j_log4j-1.2.17.jar with timestamp 1681344793495\n",
            "23/04/13 00:13:15 INFO SparkContext: Added JAR file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar at spark://a1ad50929c8d:37447/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1681344793495\n",
            "23/04/13 00:13:15 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar at spark://a1ad50929c8d:37447/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1681344793495\n",
            "23/04/13 00:13:15 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar at file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1681344793495\n",
            "23/04/13 00:13:15 INFO Utils: Copying /root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar to /tmp/spark-7cb4f0b4-77d0-496a-b187-201930e85f58/userFiles-420ac172-91cb-4b58-a8f6-d99513599410/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar\n",
            "23/04/13 00:13:15 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar at file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1681344793495\n",
            "23/04/13 00:13:15 INFO Utils: Copying /root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar to /tmp/spark-7cb4f0b4-77d0-496a-b187-201930e85f58/userFiles-420ac172-91cb-4b58-a8f6-d99513599410/org.apache.kafka_kafka_2.11-0.8.2.1.jar\n",
            "23/04/13 00:13:15 INFO SparkContext: Added file file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar at file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1681344793495\n",
            "23/04/13 00:13:15 INFO Utils: Copying /root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-7cb4f0b4-77d0-496a-b187-201930e85f58/userFiles-420ac172-91cb-4b58-a8f6-d99513599410/org.spark-project.spark_unused-1.0.0.jar\n",
            "23/04/13 00:13:15 INFO SparkContext: Added file file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar at file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1681344793495\n",
            "23/04/13 00:13:15 INFO Utils: Copying /root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar to /tmp/spark-7cb4f0b4-77d0-496a-b187-201930e85f58/userFiles-420ac172-91cb-4b58-a8f6-d99513599410/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar\n",
            "23/04/13 00:13:15 INFO SparkContext: Added file file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar at file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1681344793495\n",
            "23/04/13 00:13:15 INFO Utils: Copying /root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar to /tmp/spark-7cb4f0b4-77d0-496a-b187-201930e85f58/userFiles-420ac172-91cb-4b58-a8f6-d99513599410/com.yammer.metrics_metrics-core-2.2.0.jar\n",
            "23/04/13 00:13:15 INFO SparkContext: Added file file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar at file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1681344793495\n",
            "23/04/13 00:13:15 INFO Utils: Copying /root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar to /tmp/spark-7cb4f0b4-77d0-496a-b187-201930e85f58/userFiles-420ac172-91cb-4b58-a8f6-d99513599410/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar\n",
            "23/04/13 00:13:15 INFO SparkContext: Added file file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar at file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar with timestamp 1681344793495\n",
            "23/04/13 00:13:15 INFO Utils: Copying /root/.ivy2/jars/com.101tec_zkclient-0.3.jar to /tmp/spark-7cb4f0b4-77d0-496a-b187-201930e85f58/userFiles-420ac172-91cb-4b58-a8f6-d99513599410/com.101tec_zkclient-0.3.jar\n",
            "23/04/13 00:13:15 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar at file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1681344793495\n",
            "23/04/13 00:13:15 INFO Utils: Copying /root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar to /tmp/spark-7cb4f0b4-77d0-496a-b187-201930e85f58/userFiles-420ac172-91cb-4b58-a8f6-d99513599410/org.apache.kafka_kafka-clients-0.8.2.1.jar\n",
            "23/04/13 00:13:15 INFO SparkContext: Added file file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar at file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1681344793495\n",
            "23/04/13 00:13:15 INFO Utils: Copying /root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar to /tmp/spark-7cb4f0b4-77d0-496a-b187-201930e85f58/userFiles-420ac172-91cb-4b58-a8f6-d99513599410/org.slf4j_slf4j-api-1.7.16.jar\n",
            "23/04/13 00:13:15 INFO SparkContext: Added file file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar at file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar with timestamp 1681344793495\n",
            "23/04/13 00:13:15 INFO Utils: Copying /root/.ivy2/jars/log4j_log4j-1.2.17.jar to /tmp/spark-7cb4f0b4-77d0-496a-b187-201930e85f58/userFiles-420ac172-91cb-4b58-a8f6-d99513599410/log4j_log4j-1.2.17.jar\n",
            "23/04/13 00:13:15 INFO SparkContext: Added file file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar at file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1681344793495\n",
            "23/04/13 00:13:15 INFO Utils: Copying /root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar to /tmp/spark-7cb4f0b4-77d0-496a-b187-201930e85f58/userFiles-420ac172-91cb-4b58-a8f6-d99513599410/net.jpountz.lz4_lz4-1.2.0.jar\n",
            "23/04/13 00:13:15 INFO SparkContext: Added file file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar at file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1681344793495\n",
            "23/04/13 00:13:15 INFO Utils: Copying /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar to /tmp/spark-7cb4f0b4-77d0-496a-b187-201930e85f58/userFiles-420ac172-91cb-4b58-a8f6-d99513599410/org.xerial.snappy_snappy-java-1.1.7.5.jar\n",
            "23/04/13 00:13:15 INFO Executor: Starting executor ID driver on host a1ad50929c8d\n",
            "23/04/13 00:13:15 INFO Executor: Fetching file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar with timestamp 1681344793495\n",
            "23/04/13 00:13:15 INFO Utils: /root/.ivy2/jars/log4j_log4j-1.2.17.jar has been previously copied to /tmp/spark-7cb4f0b4-77d0-496a-b187-201930e85f58/userFiles-420ac172-91cb-4b58-a8f6-d99513599410/log4j_log4j-1.2.17.jar\n",
            "23/04/13 00:13:15 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1681344793495\n",
            "23/04/13 00:13:15 INFO Utils: /root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar has been previously copied to /tmp/spark-7cb4f0b4-77d0-496a-b187-201930e85f58/userFiles-420ac172-91cb-4b58-a8f6-d99513599410/org.apache.kafka_kafka-clients-0.8.2.1.jar\n",
            "23/04/13 00:13:15 INFO Executor: Fetching file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1681344793495\n",
            "23/04/13 00:13:15 INFO Utils: /root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar has been previously copied to /tmp/spark-7cb4f0b4-77d0-496a-b187-201930e85f58/userFiles-420ac172-91cb-4b58-a8f6-d99513599410/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar\n",
            "23/04/13 00:13:15 INFO Executor: Fetching file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1681344793495\n",
            "23/04/13 00:13:15 INFO Utils: /root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar has been previously copied to /tmp/spark-7cb4f0b4-77d0-496a-b187-201930e85f58/userFiles-420ac172-91cb-4b58-a8f6-d99513599410/org.slf4j_slf4j-api-1.7.16.jar\n",
            "23/04/13 00:13:15 INFO Executor: Fetching file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1681344793495\n",
            "23/04/13 00:13:15 INFO Utils: /root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar has been previously copied to /tmp/spark-7cb4f0b4-77d0-496a-b187-201930e85f58/userFiles-420ac172-91cb-4b58-a8f6-d99513599410/net.jpountz.lz4_lz4-1.2.0.jar\n",
            "23/04/13 00:13:15 INFO Executor: Fetching file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1681344793495\n",
            "23/04/13 00:13:15 INFO Utils: /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar has been previously copied to /tmp/spark-7cb4f0b4-77d0-496a-b187-201930e85f58/userFiles-420ac172-91cb-4b58-a8f6-d99513599410/org.xerial.snappy_snappy-java-1.1.7.5.jar\n",
            "23/04/13 00:13:15 INFO Executor: Fetching file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1681344793495\n",
            "23/04/13 00:13:15 INFO Utils: /root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar has been previously copied to /tmp/spark-7cb4f0b4-77d0-496a-b187-201930e85f58/userFiles-420ac172-91cb-4b58-a8f6-d99513599410/org.spark-project.spark_unused-1.0.0.jar\n",
            "23/04/13 00:13:15 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1681344793495\n",
            "23/04/13 00:13:15 INFO Utils: /root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar has been previously copied to /tmp/spark-7cb4f0b4-77d0-496a-b187-201930e85f58/userFiles-420ac172-91cb-4b58-a8f6-d99513599410/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar\n",
            "23/04/13 00:13:15 INFO Executor: Fetching file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1681344793495\n",
            "23/04/13 00:13:15 INFO Utils: /root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar has been previously copied to /tmp/spark-7cb4f0b4-77d0-496a-b187-201930e85f58/userFiles-420ac172-91cb-4b58-a8f6-d99513599410/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar\n",
            "23/04/13 00:13:15 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1681344793495\n",
            "23/04/13 00:13:15 INFO Utils: /root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar has been previously copied to /tmp/spark-7cb4f0b4-77d0-496a-b187-201930e85f58/userFiles-420ac172-91cb-4b58-a8f6-d99513599410/org.apache.kafka_kafka_2.11-0.8.2.1.jar\n",
            "23/04/13 00:13:15 INFO Executor: Fetching file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar with timestamp 1681344793495\n",
            "23/04/13 00:13:15 INFO Utils: /root/.ivy2/jars/com.101tec_zkclient-0.3.jar has been previously copied to /tmp/spark-7cb4f0b4-77d0-496a-b187-201930e85f58/userFiles-420ac172-91cb-4b58-a8f6-d99513599410/com.101tec_zkclient-0.3.jar\n",
            "23/04/13 00:13:15 INFO Executor: Fetching file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1681344793495\n",
            "23/04/13 00:13:15 INFO Utils: /root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar has been previously copied to /tmp/spark-7cb4f0b4-77d0-496a-b187-201930e85f58/userFiles-420ac172-91cb-4b58-a8f6-d99513599410/com.yammer.metrics_metrics-core-2.2.0.jar\n",
            "23/04/13 00:13:15 INFO Executor: Fetching spark://a1ad50929c8d:37447/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1681344793495\n",
            "23/04/13 00:13:15 INFO TransportClientFactory: Successfully created connection to a1ad50929c8d/172.28.0.12:37447 after 52 ms (0 ms spent in bootstraps)\n",
            "23/04/13 00:13:15 INFO Utils: Fetching spark://a1ad50929c8d:37447/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar to /tmp/spark-7cb4f0b4-77d0-496a-b187-201930e85f58/userFiles-420ac172-91cb-4b58-a8f6-d99513599410/fetchFileTemp4317963978437689002.tmp\n",
            "23/04/13 00:13:16 INFO Utils: /tmp/spark-7cb4f0b4-77d0-496a-b187-201930e85f58/userFiles-420ac172-91cb-4b58-a8f6-d99513599410/fetchFileTemp4317963978437689002.tmp has been previously copied to /tmp/spark-7cb4f0b4-77d0-496a-b187-201930e85f58/userFiles-420ac172-91cb-4b58-a8f6-d99513599410/org.apache.kafka_kafka_2.11-0.8.2.1.jar\n",
            "23/04/13 00:13:16 INFO Executor: Adding file:/tmp/spark-7cb4f0b4-77d0-496a-b187-201930e85f58/userFiles-420ac172-91cb-4b58-a8f6-d99513599410/org.apache.kafka_kafka_2.11-0.8.2.1.jar to class loader\n",
            "23/04/13 00:13:16 INFO Executor: Fetching spark://a1ad50929c8d:37447/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1681344793495\n",
            "23/04/13 00:13:16 INFO Utils: Fetching spark://a1ad50929c8d:37447/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar to /tmp/spark-7cb4f0b4-77d0-496a-b187-201930e85f58/userFiles-420ac172-91cb-4b58-a8f6-d99513599410/fetchFileTemp434908921098832346.tmp\n",
            "23/04/13 00:13:16 INFO Utils: /tmp/spark-7cb4f0b4-77d0-496a-b187-201930e85f58/userFiles-420ac172-91cb-4b58-a8f6-d99513599410/fetchFileTemp434908921098832346.tmp has been previously copied to /tmp/spark-7cb4f0b4-77d0-496a-b187-201930e85f58/userFiles-420ac172-91cb-4b58-a8f6-d99513599410/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar\n",
            "23/04/13 00:13:16 INFO Executor: Adding file:/tmp/spark-7cb4f0b4-77d0-496a-b187-201930e85f58/userFiles-420ac172-91cb-4b58-a8f6-d99513599410/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar to class loader\n",
            "23/04/13 00:13:16 INFO Executor: Fetching spark://a1ad50929c8d:37447/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1681344793495\n",
            "23/04/13 00:13:16 INFO Utils: Fetching spark://a1ad50929c8d:37447/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-7cb4f0b4-77d0-496a-b187-201930e85f58/userFiles-420ac172-91cb-4b58-a8f6-d99513599410/fetchFileTemp7654817655290340284.tmp\n",
            "23/04/13 00:13:16 INFO Utils: /tmp/spark-7cb4f0b4-77d0-496a-b187-201930e85f58/userFiles-420ac172-91cb-4b58-a8f6-d99513599410/fetchFileTemp7654817655290340284.tmp has been previously copied to /tmp/spark-7cb4f0b4-77d0-496a-b187-201930e85f58/userFiles-420ac172-91cb-4b58-a8f6-d99513599410/org.spark-project.spark_unused-1.0.0.jar\n",
            "23/04/13 00:13:16 INFO Executor: Adding file:/tmp/spark-7cb4f0b4-77d0-496a-b187-201930e85f58/userFiles-420ac172-91cb-4b58-a8f6-d99513599410/org.spark-project.spark_unused-1.0.0.jar to class loader\n",
            "23/04/13 00:13:16 INFO Executor: Fetching spark://a1ad50929c8d:37447/jars/log4j_log4j-1.2.17.jar with timestamp 1681344793495\n",
            "23/04/13 00:13:16 INFO Utils: Fetching spark://a1ad50929c8d:37447/jars/log4j_log4j-1.2.17.jar to /tmp/spark-7cb4f0b4-77d0-496a-b187-201930e85f58/userFiles-420ac172-91cb-4b58-a8f6-d99513599410/fetchFileTemp5667201619112826385.tmp\n",
            "23/04/13 00:13:16 INFO Utils: /tmp/spark-7cb4f0b4-77d0-496a-b187-201930e85f58/userFiles-420ac172-91cb-4b58-a8f6-d99513599410/fetchFileTemp5667201619112826385.tmp has been previously copied to /tmp/spark-7cb4f0b4-77d0-496a-b187-201930e85f58/userFiles-420ac172-91cb-4b58-a8f6-d99513599410/log4j_log4j-1.2.17.jar\n",
            "23/04/13 00:13:16 INFO Executor: Adding file:/tmp/spark-7cb4f0b4-77d0-496a-b187-201930e85f58/userFiles-420ac172-91cb-4b58-a8f6-d99513599410/log4j_log4j-1.2.17.jar to class loader\n",
            "23/04/13 00:13:16 INFO Executor: Fetching spark://a1ad50929c8d:37447/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1681344793495\n",
            "23/04/13 00:13:16 INFO Utils: Fetching spark://a1ad50929c8d:37447/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar to /tmp/spark-7cb4f0b4-77d0-496a-b187-201930e85f58/userFiles-420ac172-91cb-4b58-a8f6-d99513599410/fetchFileTemp2551800172510891944.tmp\n",
            "23/04/13 00:13:16 INFO Utils: /tmp/spark-7cb4f0b4-77d0-496a-b187-201930e85f58/userFiles-420ac172-91cb-4b58-a8f6-d99513599410/fetchFileTemp2551800172510891944.tmp has been previously copied to /tmp/spark-7cb4f0b4-77d0-496a-b187-201930e85f58/userFiles-420ac172-91cb-4b58-a8f6-d99513599410/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar\n",
            "23/04/13 00:13:16 INFO Executor: Adding file:/tmp/spark-7cb4f0b4-77d0-496a-b187-201930e85f58/userFiles-420ac172-91cb-4b58-a8f6-d99513599410/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar to class loader\n",
            "23/04/13 00:13:16 INFO Executor: Fetching spark://a1ad50929c8d:37447/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1681344793495\n",
            "23/04/13 00:13:16 INFO Utils: Fetching spark://a1ad50929c8d:37447/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar to /tmp/spark-7cb4f0b4-77d0-496a-b187-201930e85f58/userFiles-420ac172-91cb-4b58-a8f6-d99513599410/fetchFileTemp4183870363425975888.tmp\n",
            "23/04/13 00:13:16 INFO Utils: /tmp/spark-7cb4f0b4-77d0-496a-b187-201930e85f58/userFiles-420ac172-91cb-4b58-a8f6-d99513599410/fetchFileTemp4183870363425975888.tmp has been previously copied to /tmp/spark-7cb4f0b4-77d0-496a-b187-201930e85f58/userFiles-420ac172-91cb-4b58-a8f6-d99513599410/org.apache.kafka_kafka-clients-0.8.2.1.jar\n",
            "23/04/13 00:13:16 INFO Executor: Adding file:/tmp/spark-7cb4f0b4-77d0-496a-b187-201930e85f58/userFiles-420ac172-91cb-4b58-a8f6-d99513599410/org.apache.kafka_kafka-clients-0.8.2.1.jar to class loader\n",
            "23/04/13 00:13:16 INFO Executor: Fetching spark://a1ad50929c8d:37447/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1681344793495\n",
            "23/04/13 00:13:16 INFO Utils: Fetching spark://a1ad50929c8d:37447/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar to /tmp/spark-7cb4f0b4-77d0-496a-b187-201930e85f58/userFiles-420ac172-91cb-4b58-a8f6-d99513599410/fetchFileTemp2478328054786777119.tmp\n",
            "23/04/13 00:13:16 INFO Utils: /tmp/spark-7cb4f0b4-77d0-496a-b187-201930e85f58/userFiles-420ac172-91cb-4b58-a8f6-d99513599410/fetchFileTemp2478328054786777119.tmp has been previously copied to /tmp/spark-7cb4f0b4-77d0-496a-b187-201930e85f58/userFiles-420ac172-91cb-4b58-a8f6-d99513599410/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar\n",
            "23/04/13 00:13:16 INFO Executor: Adding file:/tmp/spark-7cb4f0b4-77d0-496a-b187-201930e85f58/userFiles-420ac172-91cb-4b58-a8f6-d99513599410/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar to class loader\n",
            "23/04/13 00:13:16 INFO Executor: Fetching spark://a1ad50929c8d:37447/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1681344793495\n",
            "23/04/13 00:13:16 INFO Utils: Fetching spark://a1ad50929c8d:37447/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar to /tmp/spark-7cb4f0b4-77d0-496a-b187-201930e85f58/userFiles-420ac172-91cb-4b58-a8f6-d99513599410/fetchFileTemp4172986848645412888.tmp\n",
            "23/04/13 00:13:16 INFO Utils: /tmp/spark-7cb4f0b4-77d0-496a-b187-201930e85f58/userFiles-420ac172-91cb-4b58-a8f6-d99513599410/fetchFileTemp4172986848645412888.tmp has been previously copied to /tmp/spark-7cb4f0b4-77d0-496a-b187-201930e85f58/userFiles-420ac172-91cb-4b58-a8f6-d99513599410/org.xerial.snappy_snappy-java-1.1.7.5.jar\n",
            "23/04/13 00:13:16 INFO Executor: Adding file:/tmp/spark-7cb4f0b4-77d0-496a-b187-201930e85f58/userFiles-420ac172-91cb-4b58-a8f6-d99513599410/org.xerial.snappy_snappy-java-1.1.7.5.jar to class loader\n",
            "23/04/13 00:13:16 INFO Executor: Fetching spark://a1ad50929c8d:37447/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1681344793495\n",
            "23/04/13 00:13:16 INFO Utils: Fetching spark://a1ad50929c8d:37447/jars/net.jpountz.lz4_lz4-1.2.0.jar to /tmp/spark-7cb4f0b4-77d0-496a-b187-201930e85f58/userFiles-420ac172-91cb-4b58-a8f6-d99513599410/fetchFileTemp774777276177429727.tmp\n",
            "23/04/13 00:13:16 INFO Utils: /tmp/spark-7cb4f0b4-77d0-496a-b187-201930e85f58/userFiles-420ac172-91cb-4b58-a8f6-d99513599410/fetchFileTemp774777276177429727.tmp has been previously copied to /tmp/spark-7cb4f0b4-77d0-496a-b187-201930e85f58/userFiles-420ac172-91cb-4b58-a8f6-d99513599410/net.jpountz.lz4_lz4-1.2.0.jar\n",
            "23/04/13 00:13:16 INFO Executor: Adding file:/tmp/spark-7cb4f0b4-77d0-496a-b187-201930e85f58/userFiles-420ac172-91cb-4b58-a8f6-d99513599410/net.jpountz.lz4_lz4-1.2.0.jar to class loader\n",
            "23/04/13 00:13:16 INFO Executor: Fetching spark://a1ad50929c8d:37447/jars/com.101tec_zkclient-0.3.jar with timestamp 1681344793495\n",
            "23/04/13 00:13:16 INFO Utils: Fetching spark://a1ad50929c8d:37447/jars/com.101tec_zkclient-0.3.jar to /tmp/spark-7cb4f0b4-77d0-496a-b187-201930e85f58/userFiles-420ac172-91cb-4b58-a8f6-d99513599410/fetchFileTemp5351998031346340408.tmp\n",
            "23/04/13 00:13:16 INFO Utils: /tmp/spark-7cb4f0b4-77d0-496a-b187-201930e85f58/userFiles-420ac172-91cb-4b58-a8f6-d99513599410/fetchFileTemp5351998031346340408.tmp has been previously copied to /tmp/spark-7cb4f0b4-77d0-496a-b187-201930e85f58/userFiles-420ac172-91cb-4b58-a8f6-d99513599410/com.101tec_zkclient-0.3.jar\n",
            "23/04/13 00:13:16 INFO Executor: Adding file:/tmp/spark-7cb4f0b4-77d0-496a-b187-201930e85f58/userFiles-420ac172-91cb-4b58-a8f6-d99513599410/com.101tec_zkclient-0.3.jar to class loader\n",
            "23/04/13 00:13:16 INFO Executor: Fetching spark://a1ad50929c8d:37447/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1681344793495\n",
            "23/04/13 00:13:16 INFO Utils: Fetching spark://a1ad50929c8d:37447/jars/org.slf4j_slf4j-api-1.7.16.jar to /tmp/spark-7cb4f0b4-77d0-496a-b187-201930e85f58/userFiles-420ac172-91cb-4b58-a8f6-d99513599410/fetchFileTemp385727357308795072.tmp\n",
            "23/04/13 00:13:16 INFO Utils: /tmp/spark-7cb4f0b4-77d0-496a-b187-201930e85f58/userFiles-420ac172-91cb-4b58-a8f6-d99513599410/fetchFileTemp385727357308795072.tmp has been previously copied to /tmp/spark-7cb4f0b4-77d0-496a-b187-201930e85f58/userFiles-420ac172-91cb-4b58-a8f6-d99513599410/org.slf4j_slf4j-api-1.7.16.jar\n",
            "23/04/13 00:13:16 INFO Executor: Adding file:/tmp/spark-7cb4f0b4-77d0-496a-b187-201930e85f58/userFiles-420ac172-91cb-4b58-a8f6-d99513599410/org.slf4j_slf4j-api-1.7.16.jar to class loader\n",
            "23/04/13 00:13:16 INFO Executor: Fetching spark://a1ad50929c8d:37447/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1681344793495\n",
            "23/04/13 00:13:16 INFO Utils: Fetching spark://a1ad50929c8d:37447/jars/com.yammer.metrics_metrics-core-2.2.0.jar to /tmp/spark-7cb4f0b4-77d0-496a-b187-201930e85f58/userFiles-420ac172-91cb-4b58-a8f6-d99513599410/fetchFileTemp7644519895196972252.tmp\n",
            "23/04/13 00:13:16 INFO Utils: /tmp/spark-7cb4f0b4-77d0-496a-b187-201930e85f58/userFiles-420ac172-91cb-4b58-a8f6-d99513599410/fetchFileTemp7644519895196972252.tmp has been previously copied to /tmp/spark-7cb4f0b4-77d0-496a-b187-201930e85f58/userFiles-420ac172-91cb-4b58-a8f6-d99513599410/com.yammer.metrics_metrics-core-2.2.0.jar\n",
            "23/04/13 00:13:16 INFO Executor: Adding file:/tmp/spark-7cb4f0b4-77d0-496a-b187-201930e85f58/userFiles-420ac172-91cb-4b58-a8f6-d99513599410/com.yammer.metrics_metrics-core-2.2.0.jar to class loader\n",
            "23/04/13 00:13:16 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39561.\n",
            "23/04/13 00:13:16 INFO NettyBlockTransferService: Server created on a1ad50929c8d:39561\n",
            "23/04/13 00:13:16 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "23/04/13 00:13:16 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, a1ad50929c8d, 39561, None)\n",
            "23/04/13 00:13:16 INFO BlockManagerMasterEndpoint: Registering block manager a1ad50929c8d:39561 with 366.3 MiB RAM, BlockManagerId(driver, a1ad50929c8d, 39561, None)\n",
            "23/04/13 00:13:16 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, a1ad50929c8d, 39561, None)\n",
            "23/04/13 00:13:16 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, a1ad50929c8d, 39561, None)\n",
            "23/04/13 00:13:17 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 354.2 KiB, free 366.0 MiB)\n",
            "23/04/13 00:13:17 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 32.0 KiB, free 365.9 MiB)\n",
            "23/04/13 00:13:17 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on a1ad50929c8d:39561 (size: 32.0 KiB, free: 366.3 MiB)\n",
            "23/04/13 00:13:17 INFO SparkContext: Created broadcast 0 from textFile at NativeMethodAccessorImpl.java:0\n",
            "23/04/13 00:13:17 INFO SparkUI: Stopped Spark web UI at http://a1ad50929c8d:4040\n",
            "23/04/13 00:13:17 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "23/04/13 00:13:17 INFO MemoryStore: MemoryStore cleared\n",
            "23/04/13 00:13:17 INFO BlockManager: BlockManager stopped\n",
            "23/04/13 00:13:17 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "23/04/13 00:13:17 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "23/04/13 00:13:17 INFO SparkContext: Successfully stopped SparkContext\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/1_count.py\", line 40, in <module>\n",
            "    print(counts.take(2))\n",
            "  File \"/content/spark-3.2.3-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/rdd.py\", line 1535, in take\n",
            "  File \"/content/spark-3.2.3-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/rdd.py\", line 2937, in getNumPartitions\n",
            "  File \"/content/spark-3.2.3-bin-hadoop3.2/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\", line 1321, in __call__\n",
            "  File \"/content/spark-3.2.3-bin-hadoop3.2/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py\", line 326, in get_return_value\n",
            "py4j.protocol.Py4JJavaError: An error occurred while calling o23.partitions.\n",
            ": org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme \"s3\"\n",
            "\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3443)\n",
            "\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)\n",
            "\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\n",
            "\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)\n",
            "\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)\n",
            "\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)\n",
            "\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n",
            "\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:275)\n",
            "\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:244)\n",
            "\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:332)\n",
            "\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:205)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n",
            "\tat scala.Option.getOrElse(Option.scala:189)\n",
            "\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n",
            "\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n",
            "\tat scala.Option.getOrElse(Option.scala:189)\n",
            "\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n",
            "\tat org.apache.spark.api.java.JavaRDDLike.partitions(JavaRDDLike.scala:61)\n",
            "\tat org.apache.spark.api.java.JavaRDDLike.partitions$(JavaRDDLike.scala:61)\n",
            "\tat org.apache.spark.api.java.AbstractJavaRDDLike.partitions(JavaRDDLike.scala:45)\n",
            "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
            "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
            "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
            "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
            "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
            "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
            "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
            "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
            "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
            "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
            "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "\n",
            "23/04/13 00:13:18 INFO ShutdownHookManager: Shutdown hook called\n",
            "23/04/13 00:13:18 INFO ShutdownHookManager: Deleting directory /tmp/spark-c91a4e7a-5ffd-4dd7-8604-2b620555a81f\n",
            "23/04/13 00:13:18 INFO ShutdownHookManager: Deleting directory /tmp/spark-7cb4f0b4-77d0-496a-b187-201930e85f58/pyspark-08278e52-7cab-44ce-93e6-ccef99820145\n",
            "23/04/13 00:13:18 INFO ShutdownHookManager: Deleting directory /tmp/spark-7cb4f0b4-77d0-496a-b187-201930e85f58\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TrkWukH-__Dz",
        "outputId": "502db1e9-3f8d-4eb7-bddd-e3b8903458b4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def test1(lines):\n",
        "  if '6294' in lines[0] and 'correct' in lines[0]:\n",
        "    print(\"passed\")\n",
        "  else:\n",
        "    print(\"failed\")\n",
        "\n",
        "# test local execution results\n",
        "with open('1_count.out/part-00000') as f:\n",
        "  lines = f.readlines()\n",
        "  print(lines)\n",
        "  test1(lines)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"('correct', 6294)\\n\"]\n",
            "passed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jy9ZZT06__Dy"
      },
      "source": [
        "### Please save the output of each job as a single text file into your S3 bucket.\n",
        "\n",
        "Hint:\n",
        "\n",
        "1. You may call the **saveAsTextFile** function to populate the output file. \n",
        "2. Note spark may generate multiple output files due to partitioning, you can use the **repartition** or **coalesce** function to merge them to a single one.\n",
        "\n",
        "**You need to replace all s3 uri shown in below cells with yours.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNPsauvU__Dz"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install boto3 pyspark\n",
        "# Please fill your aws credential information here\n",
        "credentials = {\n",
        "    'region_name': 'us-east-1',\n",
        "    'aws_access_key_id': 'ASIAZIHGSPJHXYK55VU3',\n",
        "    'aws_secret_access_key': '/kmTThIzqFvJHF+bE11fxmlctDb+i7vypscWfaXo',\n",
        "    'aws_session_token': 'FwoGZXIvYXdzEPP//////////wEaDLR4OSa4pnwbHFqijSLMAc8TAQWvbg+CjpSw6m9k5b8Su6Is8K/iSeihQcsavBtmXRRAWp8jHxh+3RN9jaPqzARE/t8daBvurQBU63coBLVjOhK+9smchS6GU0dkazDxiuFSPloKggUmpLaBKIExnS5jjARSpXBQ31KwsrewmAzj6dD3eWoCwlE0xquvwZRiKM3HT3EMHVAsWDXBnn5+QZIYlc+JOv/an7spAQA9xEZ+inx5VhxTfXWOEHHPjLpTwBGylmCj+9CuSrtGdZwTgHWUARbh9IOxdduNYijIs92hBjIt3CXGMVDW4Rjep9M35+5YSe9hXq2GYVr9EYufdT4MHz/xII/MVEyKdCEBpDcQ'\n",
        "    }"
      ],
      "metadata": {
        "id": "JOh0_2tHi7SG",
        "outputId": "ab6bdddb-ade1-477d-db98-2954854d1440",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.9/dist-packages (1.26.112)\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.9/dist-packages (3.3.2)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.9/dist-packages (from boto3) (1.0.1)\n",
            "Requirement already satisfied: botocore<1.30.0,>=1.29.112 in /usr/local/lib/python3.9/dist-packages (from boto3) (1.29.112)\n",
            "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /usr/local/lib/python3.9/dist-packages (from boto3) (0.6.0)\n",
            "Requirement already satisfied: py4j==0.10.9.5 in /usr/local/lib/python3.9/dist-packages (from pyspark) (0.10.9.5)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.9/dist-packages (from botocore<1.30.0,>=1.29.112->boto3) (2.8.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /usr/local/lib/python3.9/dist-packages (from botocore<1.30.0,>=1.29.112->boto3) (1.26.15)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.30.0,>=1.29.112->boto3) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import boto3, json\n",
        "\n",
        "session = boto3.session.Session(**credentials)\n",
        "s3 = session.client('s3')"
      ],
      "metadata": {
        "id": "5X_bXsdNkJQ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WuvRtrzh__Dz"
      },
      "source": [
        "# upload script to S3. This assumes that your bucket name is vandy-bigdata. if not then change the  paths here.\n",
        "s3.upload_file(Filename='1_count.py', Bucket='far-bigdata-spark', Key='hw6/1_count.py')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# replae with your EMR cluster ID\n",
        "CLUSTER_ID = 'j-2KB8M8Y69BCNX'\n",
        "\n",
        "def submit_job(app_name, pyfile_uri):\n",
        "    emr = session.client('emr')\n",
        "    emr.add_job_flow_steps(JobFlowId=CLUSTER_ID, Steps=[{\n",
        "        'Name': app_name,\n",
        "        'ActionOnFailure': 'CANCEL_AND_WAIT',\n",
        "        'HadoopJarStep': {\n",
        "            'Args': ['spark-submit',\n",
        "                     '--master', 'yarn',\n",
        "                     '--deploy-mode', 'cluster',\n",
        "                     pyfile_uri],\n",
        "            'Jar': 'command-runner.jar'\n",
        "        }}])"
      ],
      "metadata": {
        "id": "XDmPpUdwpRAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9aGPm_qa__Dz"
      },
      "source": [
        "# submit spark job to emr. Make all the necessary changes to the path\n",
        "submit_job(app_name='1_count', pyfile_uri='s3://far-bigdata-spark/hw6/1_count.py')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DXU9mXRv__D0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6606f86d-6524-4e06-bf96-7f295bb3974d"
      },
      "source": [
        "# test EMR execution results. Once again, make sure that S3 paths are consistent.\n",
        "output_key = \"hw6/1_count.out/part-00000\"\n",
        "lines = s3.get_object(Bucket='far-bigdata-spark', Key=output_key)['Body'].read().decode().splitlines()\n",
        "test1(lines)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "passed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3BPCSIi0__D0"
      },
      "source": [
        "## Job 2. Count the screen name with the most tweets and its counts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tvw5wFdq__D0",
        "outputId": "e9e06592-f967-4e18-96b5-6509a9554162",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%file 2_group.py\n",
        "\n",
        "'''\n",
        "TODO:\n",
        "Count the screen name with the most tweets and its counts.\n",
        "\n",
        "See http://mike.teczno.com/notes/streaming-data-from-twitter.html for parsing info.\n",
        "Get the screen name by accessing tweet['user']['screen_name']\n",
        "\n",
        "Output:\n",
        "number_of_most_tweets    username\n",
        "\n",
        "'''\n",
        "\n",
        "from pyspark import SparkContext, SparkConf\n",
        "import json\n",
        "from operator import add\n",
        "\n",
        "def checkjson(entry):\n",
        "    try:\n",
        "        json.loads(entry)\n",
        "        return entry\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "def get_screen_name(entry):\n",
        "    if entry is not None:\n",
        "        tweet = json.loads(entry)\n",
        "        return tweet['user']['screen_name'], 1\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # sc = SparkContext(appName=\"2_group\")\n",
        "    # conf = SparkConf().setAppName('2_group').set('spark.hadoop.validateOutputSpecs', False)\n",
        "    # sc = SparkContext(conf=conf).getOrCreate()\n",
        "    sc = SparkContext(appName=\"2_count\")\n",
        "\n",
        "    try:\n",
        "        tweets=sc.textFile('s3://far-bigdata-spark/datasets/nashville-tweets-2019-01-28')\n",
        "        valid_tweets = tweets.map(lambda tweet: checkjson(tweet)).filter(lambda x: x is not None)\n",
        "        screen_names = valid_tweets.map(lambda entry: get_screen_name(entry)).filter(lambda x: x is not None)\n",
        "        counts = screen_names.reduceByKey(add)\n",
        "        max_count = counts.max(key=lambda x: x[1])\n",
        "        output = sc.parallelize([max_count])\n",
        "        output.repartition(1).saveAsTextFile(\"s3://far-bigdata-spark/hw6/2_group.out\")\n",
        "\n",
        "    finally:\n",
        "        sc.stop()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting 2_group.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Execute and test locally"
      ],
      "metadata": {
        "id": "1xe2ZqP18Uso"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tH6TK-Nk__D0",
        "outputId": "a1769f7c-d7fd-489c-cfff-1a49b55f7dcc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "\n",
        "!spark-submit --packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.4.7 2_group.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ":: loading settings :: url = jar:file:/content/spark-3.2.3-bin-hadoop3.2/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
            "Ivy Default Cache set to: /root/.ivy2/cache\n",
            "The jars for the packages stored in: /root/.ivy2/jars\n",
            "org.apache.spark#spark-streaming-kafka-0-8_2.11 added as a dependency\n",
            ":: resolving dependencies :: org.apache.spark#spark-submit-parent-5c8cce4a-1c25-4bfe-8464-23a66d2aab65;1.0\n",
            "\tconfs: [default]\n",
            "\tfound org.apache.spark#spark-streaming-kafka-0-8_2.11;2.4.7 in central\n",
            "\tfound org.apache.kafka#kafka_2.11;0.8.2.1 in central\n",
            "\tfound org.scala-lang.modules#scala-xml_2.11;1.0.2 in central\n",
            "\tfound com.yammer.metrics#metrics-core;2.2.0 in central\n",
            "\tfound org.slf4j#slf4j-api;1.7.16 in central\n",
            "\tfound org.scala-lang.modules#scala-parser-combinators_2.11;1.1.0 in central\n",
            "\tfound com.101tec#zkclient;0.3 in central\n",
            "\tfound log4j#log4j;1.2.17 in central\n",
            "\tfound org.apache.kafka#kafka-clients;0.8.2.1 in central\n",
            "\tfound net.jpountz.lz4#lz4;1.2.0 in central\n",
            "\tfound org.xerial.snappy#snappy-java;1.1.7.5 in central\n",
            "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
            ":: resolution report :: resolve 967ms :: artifacts dl 33ms\n",
            "\t:: modules in use:\n",
            "\tcom.101tec#zkclient;0.3 from central in [default]\n",
            "\tcom.yammer.metrics#metrics-core;2.2.0 from central in [default]\n",
            "\tlog4j#log4j;1.2.17 from central in [default]\n",
            "\tnet.jpountz.lz4#lz4;1.2.0 from central in [default]\n",
            "\torg.apache.kafka#kafka-clients;0.8.2.1 from central in [default]\n",
            "\torg.apache.kafka#kafka_2.11;0.8.2.1 from central in [default]\n",
            "\torg.apache.spark#spark-streaming-kafka-0-8_2.11;2.4.7 from central in [default]\n",
            "\torg.scala-lang.modules#scala-parser-combinators_2.11;1.1.0 from central in [default]\n",
            "\torg.scala-lang.modules#scala-xml_2.11;1.0.2 from central in [default]\n",
            "\torg.slf4j#slf4j-api;1.7.16 from central in [default]\n",
            "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
            "\torg.xerial.snappy#snappy-java;1.1.7.5 from central in [default]\n",
            "\t---------------------------------------------------------------------\n",
            "\t|                  |            modules            ||   artifacts   |\n",
            "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
            "\t---------------------------------------------------------------------\n",
            "\t|      default     |   12  |   0   |   0   |   0   ||   12  |   0   |\n",
            "\t---------------------------------------------------------------------\n",
            ":: retrieving :: org.apache.spark#spark-submit-parent-5c8cce4a-1c25-4bfe-8464-23a66d2aab65\n",
            "\tconfs: [default]\n",
            "\t0 artifacts copied, 12 already retrieved (0kB/14ms)\n",
            "23/04/13 04:00:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
            "23/04/13 04:00:38 INFO SparkContext: Running Spark version 3.2.3\n",
            "23/04/13 04:00:38 INFO ResourceUtils: ==============================================================\n",
            "23/04/13 04:00:38 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "23/04/13 04:00:38 INFO ResourceUtils: ==============================================================\n",
            "23/04/13 04:00:38 INFO SparkContext: Submitted application: 2_count\n",
            "23/04/13 04:00:38 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "23/04/13 04:00:38 INFO ResourceProfile: Limiting resource is cpu\n",
            "23/04/13 04:00:38 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "23/04/13 04:00:38 INFO SecurityManager: Changing view acls to: root\n",
            "23/04/13 04:00:38 INFO SecurityManager: Changing modify acls to: root\n",
            "23/04/13 04:00:38 INFO SecurityManager: Changing view acls groups to: \n",
            "23/04/13 04:00:38 INFO SecurityManager: Changing modify acls groups to: \n",
            "23/04/13 04:00:38 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\n",
            "23/04/13 04:00:39 INFO Utils: Successfully started service 'sparkDriver' on port 42317.\n",
            "23/04/13 04:00:39 INFO SparkEnv: Registering MapOutputTracker\n",
            "23/04/13 04:00:40 INFO SparkEnv: Registering BlockManagerMaster\n",
            "23/04/13 04:00:40 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "23/04/13 04:00:40 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "23/04/13 04:00:40 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "23/04/13 04:00:40 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-365e6c67-d504-4a42-b259-da8b08a32153\n",
            "23/04/13 04:00:40 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB\n",
            "23/04/13 04:00:40 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "23/04/13 04:00:41 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
            "23/04/13 04:00:41 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://a1ad50929c8d:4040\n",
            "23/04/13 04:00:41 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar at spark://a1ad50929c8d:42317/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1681358438092\n",
            "23/04/13 04:00:41 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar at spark://a1ad50929c8d:42317/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1681358438092\n",
            "23/04/13 04:00:41 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar at spark://a1ad50929c8d:42317/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1681358438092\n",
            "23/04/13 04:00:41 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar at spark://a1ad50929c8d:42317/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1681358438092\n",
            "23/04/13 04:00:41 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar at spark://a1ad50929c8d:42317/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1681358438092\n",
            "23/04/13 04:00:41 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar at spark://a1ad50929c8d:42317/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1681358438092\n",
            "23/04/13 04:00:41 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar at spark://a1ad50929c8d:42317/jars/com.101tec_zkclient-0.3.jar with timestamp 1681358438092\n",
            "23/04/13 04:00:41 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar at spark://a1ad50929c8d:42317/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1681358438092\n",
            "23/04/13 04:00:41 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar at spark://a1ad50929c8d:42317/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1681358438092\n",
            "23/04/13 04:00:41 INFO SparkContext: Added JAR file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar at spark://a1ad50929c8d:42317/jars/log4j_log4j-1.2.17.jar with timestamp 1681358438092\n",
            "23/04/13 04:00:41 INFO SparkContext: Added JAR file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar at spark://a1ad50929c8d:42317/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1681358438092\n",
            "23/04/13 04:00:41 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar at spark://a1ad50929c8d:42317/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1681358438092\n",
            "23/04/13 04:00:41 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar at file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1681358438092\n",
            "23/04/13 04:00:41 INFO Utils: Copying /root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar to /tmp/spark-39d63502-6af8-421b-a7ea-38d04e7656a3/userFiles-9add149e-4baf-4c97-8797-698cffc037a8/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar\n",
            "23/04/13 04:00:41 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar at file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1681358438092\n",
            "23/04/13 04:00:41 INFO Utils: Copying /root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar to /tmp/spark-39d63502-6af8-421b-a7ea-38d04e7656a3/userFiles-9add149e-4baf-4c97-8797-698cffc037a8/org.apache.kafka_kafka_2.11-0.8.2.1.jar\n",
            "23/04/13 04:00:41 INFO SparkContext: Added file file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar at file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1681358438092\n",
            "23/04/13 04:00:41 INFO Utils: Copying /root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-39d63502-6af8-421b-a7ea-38d04e7656a3/userFiles-9add149e-4baf-4c97-8797-698cffc037a8/org.spark-project.spark_unused-1.0.0.jar\n",
            "23/04/13 04:00:41 INFO SparkContext: Added file file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar at file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1681358438092\n",
            "23/04/13 04:00:41 INFO Utils: Copying /root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar to /tmp/spark-39d63502-6af8-421b-a7ea-38d04e7656a3/userFiles-9add149e-4baf-4c97-8797-698cffc037a8/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar\n",
            "23/04/13 04:00:41 INFO SparkContext: Added file file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar at file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1681358438092\n",
            "23/04/13 04:00:41 INFO Utils: Copying /root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar to /tmp/spark-39d63502-6af8-421b-a7ea-38d04e7656a3/userFiles-9add149e-4baf-4c97-8797-698cffc037a8/com.yammer.metrics_metrics-core-2.2.0.jar\n",
            "23/04/13 04:00:41 INFO SparkContext: Added file file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar at file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1681358438092\n",
            "23/04/13 04:00:41 INFO Utils: Copying /root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar to /tmp/spark-39d63502-6af8-421b-a7ea-38d04e7656a3/userFiles-9add149e-4baf-4c97-8797-698cffc037a8/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar\n",
            "23/04/13 04:00:41 INFO SparkContext: Added file file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar at file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar with timestamp 1681358438092\n",
            "23/04/13 04:00:41 INFO Utils: Copying /root/.ivy2/jars/com.101tec_zkclient-0.3.jar to /tmp/spark-39d63502-6af8-421b-a7ea-38d04e7656a3/userFiles-9add149e-4baf-4c97-8797-698cffc037a8/com.101tec_zkclient-0.3.jar\n",
            "23/04/13 04:00:41 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar at file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1681358438092\n",
            "23/04/13 04:00:41 INFO Utils: Copying /root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar to /tmp/spark-39d63502-6af8-421b-a7ea-38d04e7656a3/userFiles-9add149e-4baf-4c97-8797-698cffc037a8/org.apache.kafka_kafka-clients-0.8.2.1.jar\n",
            "23/04/13 04:00:41 INFO SparkContext: Added file file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar at file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1681358438092\n",
            "23/04/13 04:00:41 INFO Utils: Copying /root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar to /tmp/spark-39d63502-6af8-421b-a7ea-38d04e7656a3/userFiles-9add149e-4baf-4c97-8797-698cffc037a8/org.slf4j_slf4j-api-1.7.16.jar\n",
            "23/04/13 04:00:41 INFO SparkContext: Added file file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar at file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar with timestamp 1681358438092\n",
            "23/04/13 04:00:41 INFO Utils: Copying /root/.ivy2/jars/log4j_log4j-1.2.17.jar to /tmp/spark-39d63502-6af8-421b-a7ea-38d04e7656a3/userFiles-9add149e-4baf-4c97-8797-698cffc037a8/log4j_log4j-1.2.17.jar\n",
            "23/04/13 04:00:41 INFO SparkContext: Added file file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar at file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1681358438092\n",
            "23/04/13 04:00:41 INFO Utils: Copying /root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar to /tmp/spark-39d63502-6af8-421b-a7ea-38d04e7656a3/userFiles-9add149e-4baf-4c97-8797-698cffc037a8/net.jpountz.lz4_lz4-1.2.0.jar\n",
            "23/04/13 04:00:41 INFO SparkContext: Added file file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar at file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1681358438092\n",
            "23/04/13 04:00:41 INFO Utils: Copying /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar to /tmp/spark-39d63502-6af8-421b-a7ea-38d04e7656a3/userFiles-9add149e-4baf-4c97-8797-698cffc037a8/org.xerial.snappy_snappy-java-1.1.7.5.jar\n",
            "23/04/13 04:00:41 INFO Executor: Starting executor ID driver on host a1ad50929c8d\n",
            "23/04/13 04:00:41 INFO Executor: Fetching file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar with timestamp 1681358438092\n",
            "23/04/13 04:00:41 INFO Utils: /root/.ivy2/jars/log4j_log4j-1.2.17.jar has been previously copied to /tmp/spark-39d63502-6af8-421b-a7ea-38d04e7656a3/userFiles-9add149e-4baf-4c97-8797-698cffc037a8/log4j_log4j-1.2.17.jar\n",
            "23/04/13 04:00:41 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1681358438092\n",
            "23/04/13 04:00:41 INFO Utils: /root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar has been previously copied to /tmp/spark-39d63502-6af8-421b-a7ea-38d04e7656a3/userFiles-9add149e-4baf-4c97-8797-698cffc037a8/org.apache.kafka_kafka-clients-0.8.2.1.jar\n",
            "23/04/13 04:00:42 INFO Executor: Fetching file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1681358438092\n",
            "23/04/13 04:00:42 INFO Utils: /root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar has been previously copied to /tmp/spark-39d63502-6af8-421b-a7ea-38d04e7656a3/userFiles-9add149e-4baf-4c97-8797-698cffc037a8/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar\n",
            "23/04/13 04:00:42 INFO Executor: Fetching file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1681358438092\n",
            "23/04/13 04:00:42 INFO Utils: /root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar has been previously copied to /tmp/spark-39d63502-6af8-421b-a7ea-38d04e7656a3/userFiles-9add149e-4baf-4c97-8797-698cffc037a8/org.slf4j_slf4j-api-1.7.16.jar\n",
            "23/04/13 04:00:42 INFO Executor: Fetching file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1681358438092\n",
            "23/04/13 04:00:42 INFO Utils: /root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar has been previously copied to /tmp/spark-39d63502-6af8-421b-a7ea-38d04e7656a3/userFiles-9add149e-4baf-4c97-8797-698cffc037a8/net.jpountz.lz4_lz4-1.2.0.jar\n",
            "23/04/13 04:00:42 INFO Executor: Fetching file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1681358438092\n",
            "23/04/13 04:00:42 INFO Utils: /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar has been previously copied to /tmp/spark-39d63502-6af8-421b-a7ea-38d04e7656a3/userFiles-9add149e-4baf-4c97-8797-698cffc037a8/org.xerial.snappy_snappy-java-1.1.7.5.jar\n",
            "23/04/13 04:00:42 INFO Executor: Fetching file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1681358438092\n",
            "23/04/13 04:00:42 INFO Utils: /root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar has been previously copied to /tmp/spark-39d63502-6af8-421b-a7ea-38d04e7656a3/userFiles-9add149e-4baf-4c97-8797-698cffc037a8/org.spark-project.spark_unused-1.0.0.jar\n",
            "23/04/13 04:00:42 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1681358438092\n",
            "23/04/13 04:00:42 INFO Utils: /root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar has been previously copied to /tmp/spark-39d63502-6af8-421b-a7ea-38d04e7656a3/userFiles-9add149e-4baf-4c97-8797-698cffc037a8/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar\n",
            "23/04/13 04:00:42 INFO Executor: Fetching file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1681358438092\n",
            "23/04/13 04:00:42 INFO Utils: /root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar has been previously copied to /tmp/spark-39d63502-6af8-421b-a7ea-38d04e7656a3/userFiles-9add149e-4baf-4c97-8797-698cffc037a8/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar\n",
            "23/04/13 04:00:42 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1681358438092\n",
            "23/04/13 04:00:42 INFO Utils: /root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar has been previously copied to /tmp/spark-39d63502-6af8-421b-a7ea-38d04e7656a3/userFiles-9add149e-4baf-4c97-8797-698cffc037a8/org.apache.kafka_kafka_2.11-0.8.2.1.jar\n",
            "23/04/13 04:00:42 INFO Executor: Fetching file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar with timestamp 1681358438092\n",
            "23/04/13 04:00:42 INFO Utils: /root/.ivy2/jars/com.101tec_zkclient-0.3.jar has been previously copied to /tmp/spark-39d63502-6af8-421b-a7ea-38d04e7656a3/userFiles-9add149e-4baf-4c97-8797-698cffc037a8/com.101tec_zkclient-0.3.jar\n",
            "23/04/13 04:00:42 INFO Executor: Fetching file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1681358438092\n",
            "23/04/13 04:00:42 INFO Utils: /root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar has been previously copied to /tmp/spark-39d63502-6af8-421b-a7ea-38d04e7656a3/userFiles-9add149e-4baf-4c97-8797-698cffc037a8/com.yammer.metrics_metrics-core-2.2.0.jar\n",
            "23/04/13 04:00:42 INFO Executor: Fetching spark://a1ad50929c8d:42317/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1681358438092\n",
            "23/04/13 04:00:42 INFO TransportClientFactory: Successfully created connection to a1ad50929c8d/172.28.0.12:42317 after 55 ms (0 ms spent in bootstraps)\n",
            "23/04/13 04:00:42 INFO Utils: Fetching spark://a1ad50929c8d:42317/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar to /tmp/spark-39d63502-6af8-421b-a7ea-38d04e7656a3/userFiles-9add149e-4baf-4c97-8797-698cffc037a8/fetchFileTemp9178540035410667413.tmp\n",
            "23/04/13 04:00:42 INFO Utils: /tmp/spark-39d63502-6af8-421b-a7ea-38d04e7656a3/userFiles-9add149e-4baf-4c97-8797-698cffc037a8/fetchFileTemp9178540035410667413.tmp has been previously copied to /tmp/spark-39d63502-6af8-421b-a7ea-38d04e7656a3/userFiles-9add149e-4baf-4c97-8797-698cffc037a8/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar\n",
            "23/04/13 04:00:42 INFO Executor: Adding file:/tmp/spark-39d63502-6af8-421b-a7ea-38d04e7656a3/userFiles-9add149e-4baf-4c97-8797-698cffc037a8/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar to class loader\n",
            "23/04/13 04:00:42 INFO Executor: Fetching spark://a1ad50929c8d:42317/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1681358438092\n",
            "23/04/13 04:00:42 INFO Utils: Fetching spark://a1ad50929c8d:42317/jars/net.jpountz.lz4_lz4-1.2.0.jar to /tmp/spark-39d63502-6af8-421b-a7ea-38d04e7656a3/userFiles-9add149e-4baf-4c97-8797-698cffc037a8/fetchFileTemp3788382307263568885.tmp\n",
            "23/04/13 04:00:42 INFO Utils: /tmp/spark-39d63502-6af8-421b-a7ea-38d04e7656a3/userFiles-9add149e-4baf-4c97-8797-698cffc037a8/fetchFileTemp3788382307263568885.tmp has been previously copied to /tmp/spark-39d63502-6af8-421b-a7ea-38d04e7656a3/userFiles-9add149e-4baf-4c97-8797-698cffc037a8/net.jpountz.lz4_lz4-1.2.0.jar\n",
            "23/04/13 04:00:42 INFO Executor: Adding file:/tmp/spark-39d63502-6af8-421b-a7ea-38d04e7656a3/userFiles-9add149e-4baf-4c97-8797-698cffc037a8/net.jpountz.lz4_lz4-1.2.0.jar to class loader\n",
            "23/04/13 04:00:42 INFO Executor: Fetching spark://a1ad50929c8d:42317/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1681358438092\n",
            "23/04/13 04:00:42 INFO Utils: Fetching spark://a1ad50929c8d:42317/jars/org.slf4j_slf4j-api-1.7.16.jar to /tmp/spark-39d63502-6af8-421b-a7ea-38d04e7656a3/userFiles-9add149e-4baf-4c97-8797-698cffc037a8/fetchFileTemp4537986321081582332.tmp\n",
            "23/04/13 04:00:42 INFO Utils: /tmp/spark-39d63502-6af8-421b-a7ea-38d04e7656a3/userFiles-9add149e-4baf-4c97-8797-698cffc037a8/fetchFileTemp4537986321081582332.tmp has been previously copied to /tmp/spark-39d63502-6af8-421b-a7ea-38d04e7656a3/userFiles-9add149e-4baf-4c97-8797-698cffc037a8/org.slf4j_slf4j-api-1.7.16.jar\n",
            "23/04/13 04:00:42 INFO Executor: Adding file:/tmp/spark-39d63502-6af8-421b-a7ea-38d04e7656a3/userFiles-9add149e-4baf-4c97-8797-698cffc037a8/org.slf4j_slf4j-api-1.7.16.jar to class loader\n",
            "23/04/13 04:00:42 INFO Executor: Fetching spark://a1ad50929c8d:42317/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1681358438092\n",
            "23/04/13 04:00:42 INFO Utils: Fetching spark://a1ad50929c8d:42317/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar to /tmp/spark-39d63502-6af8-421b-a7ea-38d04e7656a3/userFiles-9add149e-4baf-4c97-8797-698cffc037a8/fetchFileTemp1289636576385219940.tmp\n",
            "23/04/13 04:00:42 INFO Utils: /tmp/spark-39d63502-6af8-421b-a7ea-38d04e7656a3/userFiles-9add149e-4baf-4c97-8797-698cffc037a8/fetchFileTemp1289636576385219940.tmp has been previously copied to /tmp/spark-39d63502-6af8-421b-a7ea-38d04e7656a3/userFiles-9add149e-4baf-4c97-8797-698cffc037a8/org.apache.kafka_kafka-clients-0.8.2.1.jar\n",
            "23/04/13 04:00:42 INFO Executor: Adding file:/tmp/spark-39d63502-6af8-421b-a7ea-38d04e7656a3/userFiles-9add149e-4baf-4c97-8797-698cffc037a8/org.apache.kafka_kafka-clients-0.8.2.1.jar to class loader\n",
            "23/04/13 04:00:42 INFO Executor: Fetching spark://a1ad50929c8d:42317/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1681358438092\n",
            "23/04/13 04:00:42 INFO Utils: Fetching spark://a1ad50929c8d:42317/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar to /tmp/spark-39d63502-6af8-421b-a7ea-38d04e7656a3/userFiles-9add149e-4baf-4c97-8797-698cffc037a8/fetchFileTemp1948938904285736550.tmp\n",
            "23/04/13 04:00:42 INFO Utils: /tmp/spark-39d63502-6af8-421b-a7ea-38d04e7656a3/userFiles-9add149e-4baf-4c97-8797-698cffc037a8/fetchFileTemp1948938904285736550.tmp has been previously copied to /tmp/spark-39d63502-6af8-421b-a7ea-38d04e7656a3/userFiles-9add149e-4baf-4c97-8797-698cffc037a8/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar\n",
            "23/04/13 04:00:42 INFO Executor: Adding file:/tmp/spark-39d63502-6af8-421b-a7ea-38d04e7656a3/userFiles-9add149e-4baf-4c97-8797-698cffc037a8/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar to class loader\n",
            "23/04/13 04:00:42 INFO Executor: Fetching spark://a1ad50929c8d:42317/jars/com.101tec_zkclient-0.3.jar with timestamp 1681358438092\n",
            "23/04/13 04:00:42 INFO Utils: Fetching spark://a1ad50929c8d:42317/jars/com.101tec_zkclient-0.3.jar to /tmp/spark-39d63502-6af8-421b-a7ea-38d04e7656a3/userFiles-9add149e-4baf-4c97-8797-698cffc037a8/fetchFileTemp5305054071556771649.tmp\n",
            "23/04/13 04:00:42 INFO Utils: /tmp/spark-39d63502-6af8-421b-a7ea-38d04e7656a3/userFiles-9add149e-4baf-4c97-8797-698cffc037a8/fetchFileTemp5305054071556771649.tmp has been previously copied to /tmp/spark-39d63502-6af8-421b-a7ea-38d04e7656a3/userFiles-9add149e-4baf-4c97-8797-698cffc037a8/com.101tec_zkclient-0.3.jar\n",
            "23/04/13 04:00:42 INFO Executor: Adding file:/tmp/spark-39d63502-6af8-421b-a7ea-38d04e7656a3/userFiles-9add149e-4baf-4c97-8797-698cffc037a8/com.101tec_zkclient-0.3.jar to class loader\n",
            "23/04/13 04:00:42 INFO Executor: Fetching spark://a1ad50929c8d:42317/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1681358438092\n",
            "23/04/13 04:00:42 INFO Utils: Fetching spark://a1ad50929c8d:42317/jars/com.yammer.metrics_metrics-core-2.2.0.jar to /tmp/spark-39d63502-6af8-421b-a7ea-38d04e7656a3/userFiles-9add149e-4baf-4c97-8797-698cffc037a8/fetchFileTemp5994036783658391898.tmp\n",
            "23/04/13 04:00:42 INFO Utils: /tmp/spark-39d63502-6af8-421b-a7ea-38d04e7656a3/userFiles-9add149e-4baf-4c97-8797-698cffc037a8/fetchFileTemp5994036783658391898.tmp has been previously copied to /tmp/spark-39d63502-6af8-421b-a7ea-38d04e7656a3/userFiles-9add149e-4baf-4c97-8797-698cffc037a8/com.yammer.metrics_metrics-core-2.2.0.jar\n",
            "23/04/13 04:00:42 INFO Executor: Adding file:/tmp/spark-39d63502-6af8-421b-a7ea-38d04e7656a3/userFiles-9add149e-4baf-4c97-8797-698cffc037a8/com.yammer.metrics_metrics-core-2.2.0.jar to class loader\n",
            "23/04/13 04:00:42 INFO Executor: Fetching spark://a1ad50929c8d:42317/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1681358438092\n",
            "23/04/13 04:00:42 INFO Utils: Fetching spark://a1ad50929c8d:42317/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar to /tmp/spark-39d63502-6af8-421b-a7ea-38d04e7656a3/userFiles-9add149e-4baf-4c97-8797-698cffc037a8/fetchFileTemp1427020639632508023.tmp\n",
            "23/04/13 04:00:42 INFO Utils: /tmp/spark-39d63502-6af8-421b-a7ea-38d04e7656a3/userFiles-9add149e-4baf-4c97-8797-698cffc037a8/fetchFileTemp1427020639632508023.tmp has been previously copied to /tmp/spark-39d63502-6af8-421b-a7ea-38d04e7656a3/userFiles-9add149e-4baf-4c97-8797-698cffc037a8/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar\n",
            "23/04/13 04:00:42 INFO Executor: Adding file:/tmp/spark-39d63502-6af8-421b-a7ea-38d04e7656a3/userFiles-9add149e-4baf-4c97-8797-698cffc037a8/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar to class loader\n",
            "23/04/13 04:00:42 INFO Executor: Fetching spark://a1ad50929c8d:42317/jars/log4j_log4j-1.2.17.jar with timestamp 1681358438092\n",
            "23/04/13 04:00:42 INFO Utils: Fetching spark://a1ad50929c8d:42317/jars/log4j_log4j-1.2.17.jar to /tmp/spark-39d63502-6af8-421b-a7ea-38d04e7656a3/userFiles-9add149e-4baf-4c97-8797-698cffc037a8/fetchFileTemp3638824950162948248.tmp\n",
            "23/04/13 04:00:42 INFO Utils: /tmp/spark-39d63502-6af8-421b-a7ea-38d04e7656a3/userFiles-9add149e-4baf-4c97-8797-698cffc037a8/fetchFileTemp3638824950162948248.tmp has been previously copied to /tmp/spark-39d63502-6af8-421b-a7ea-38d04e7656a3/userFiles-9add149e-4baf-4c97-8797-698cffc037a8/log4j_log4j-1.2.17.jar\n",
            "23/04/13 04:00:42 INFO Executor: Adding file:/tmp/spark-39d63502-6af8-421b-a7ea-38d04e7656a3/userFiles-9add149e-4baf-4c97-8797-698cffc037a8/log4j_log4j-1.2.17.jar to class loader\n",
            "23/04/13 04:00:42 INFO Executor: Fetching spark://a1ad50929c8d:42317/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1681358438092\n",
            "23/04/13 04:00:42 INFO Utils: Fetching spark://a1ad50929c8d:42317/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar to /tmp/spark-39d63502-6af8-421b-a7ea-38d04e7656a3/userFiles-9add149e-4baf-4c97-8797-698cffc037a8/fetchFileTemp2273363724928899622.tmp\n",
            "23/04/13 04:00:42 INFO Utils: /tmp/spark-39d63502-6af8-421b-a7ea-38d04e7656a3/userFiles-9add149e-4baf-4c97-8797-698cffc037a8/fetchFileTemp2273363724928899622.tmp has been previously copied to /tmp/spark-39d63502-6af8-421b-a7ea-38d04e7656a3/userFiles-9add149e-4baf-4c97-8797-698cffc037a8/org.apache.kafka_kafka_2.11-0.8.2.1.jar\n",
            "23/04/13 04:00:42 INFO Executor: Adding file:/tmp/spark-39d63502-6af8-421b-a7ea-38d04e7656a3/userFiles-9add149e-4baf-4c97-8797-698cffc037a8/org.apache.kafka_kafka_2.11-0.8.2.1.jar to class loader\n",
            "23/04/13 04:00:42 INFO Executor: Fetching spark://a1ad50929c8d:42317/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1681358438092\n",
            "23/04/13 04:00:42 INFO Utils: Fetching spark://a1ad50929c8d:42317/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-39d63502-6af8-421b-a7ea-38d04e7656a3/userFiles-9add149e-4baf-4c97-8797-698cffc037a8/fetchFileTemp4670178338983329166.tmp\n",
            "23/04/13 04:00:42 INFO Utils: /tmp/spark-39d63502-6af8-421b-a7ea-38d04e7656a3/userFiles-9add149e-4baf-4c97-8797-698cffc037a8/fetchFileTemp4670178338983329166.tmp has been previously copied to /tmp/spark-39d63502-6af8-421b-a7ea-38d04e7656a3/userFiles-9add149e-4baf-4c97-8797-698cffc037a8/org.spark-project.spark_unused-1.0.0.jar\n",
            "23/04/13 04:00:42 INFO Executor: Adding file:/tmp/spark-39d63502-6af8-421b-a7ea-38d04e7656a3/userFiles-9add149e-4baf-4c97-8797-698cffc037a8/org.spark-project.spark_unused-1.0.0.jar to class loader\n",
            "23/04/13 04:00:42 INFO Executor: Fetching spark://a1ad50929c8d:42317/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1681358438092\n",
            "23/04/13 04:00:42 INFO Utils: Fetching spark://a1ad50929c8d:42317/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar to /tmp/spark-39d63502-6af8-421b-a7ea-38d04e7656a3/userFiles-9add149e-4baf-4c97-8797-698cffc037a8/fetchFileTemp3924043791138255627.tmp\n",
            "23/04/13 04:00:42 INFO Utils: /tmp/spark-39d63502-6af8-421b-a7ea-38d04e7656a3/userFiles-9add149e-4baf-4c97-8797-698cffc037a8/fetchFileTemp3924043791138255627.tmp has been previously copied to /tmp/spark-39d63502-6af8-421b-a7ea-38d04e7656a3/userFiles-9add149e-4baf-4c97-8797-698cffc037a8/org.xerial.snappy_snappy-java-1.1.7.5.jar\n",
            "23/04/13 04:00:42 INFO Executor: Adding file:/tmp/spark-39d63502-6af8-421b-a7ea-38d04e7656a3/userFiles-9add149e-4baf-4c97-8797-698cffc037a8/org.xerial.snappy_snappy-java-1.1.7.5.jar to class loader\n",
            "23/04/13 04:00:42 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44127.\n",
            "23/04/13 04:00:42 INFO NettyBlockTransferService: Server created on a1ad50929c8d:44127\n",
            "23/04/13 04:00:42 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "23/04/13 04:00:42 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, a1ad50929c8d, 44127, None)\n",
            "23/04/13 04:00:42 INFO BlockManagerMasterEndpoint: Registering block manager a1ad50929c8d:44127 with 366.3 MiB RAM, BlockManagerId(driver, a1ad50929c8d, 44127, None)\n",
            "23/04/13 04:00:42 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, a1ad50929c8d, 44127, None)\n",
            "23/04/13 04:00:42 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, a1ad50929c8d, 44127, None)\n",
            "23/04/13 04:00:43 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 354.2 KiB, free 366.0 MiB)\n",
            "23/04/13 04:00:44 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 32.0 KiB, free 365.9 MiB)\n",
            "23/04/13 04:00:44 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on a1ad50929c8d:44127 (size: 32.0 KiB, free: 366.3 MiB)\n",
            "23/04/13 04:00:44 INFO SparkContext: Created broadcast 0 from textFile at NativeMethodAccessorImpl.java:0\n",
            "23/04/13 04:00:44 INFO SparkUI: Stopped Spark web UI at http://a1ad50929c8d:4040\n",
            "23/04/13 04:00:44 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "23/04/13 04:00:44 INFO MemoryStore: MemoryStore cleared\n",
            "23/04/13 04:00:44 INFO BlockManager: BlockManager stopped\n",
            "23/04/13 04:00:44 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "23/04/13 04:00:44 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "23/04/13 04:00:44 INFO SparkContext: Successfully stopped SparkContext\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/2_group.py\", line 42, in <module>\n",
            "    counts = screen_names.reduceByKey(add)\n",
            "  File \"/content/spark-3.2.3-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/rdd.py\", line 1893, in reduceByKey\n",
            "  File \"/content/spark-3.2.3-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/rdd.py\", line 2138, in combineByKey\n",
            "  File \"/content/spark-3.2.3-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/rdd.py\", line 2583, in _defaultReducePartitions\n",
            "  File \"/content/spark-3.2.3-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/rdd.py\", line 2937, in getNumPartitions\n",
            "  File \"/content/spark-3.2.3-bin-hadoop3.2/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\", line 1321, in __call__\n",
            "  File \"/content/spark-3.2.3-bin-hadoop3.2/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py\", line 326, in get_return_value\n",
            "py4j.protocol.Py4JJavaError: An error occurred while calling o23.partitions.\n",
            ": org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme \"s3\"\n",
            "\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3443)\n",
            "\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)\n",
            "\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\n",
            "\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)\n",
            "\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)\n",
            "\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)\n",
            "\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n",
            "\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:275)\n",
            "\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:244)\n",
            "\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:332)\n",
            "\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:205)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n",
            "\tat scala.Option.getOrElse(Option.scala:189)\n",
            "\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n",
            "\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n",
            "\tat scala.Option.getOrElse(Option.scala:189)\n",
            "\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n",
            "\tat org.apache.spark.api.java.JavaRDDLike.partitions(JavaRDDLike.scala:61)\n",
            "\tat org.apache.spark.api.java.JavaRDDLike.partitions$(JavaRDDLike.scala:61)\n",
            "\tat org.apache.spark.api.java.AbstractJavaRDDLike.partitions(JavaRDDLike.scala:45)\n",
            "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
            "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
            "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
            "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
            "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
            "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
            "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
            "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
            "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
            "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
            "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "\n",
            "23/04/13 04:00:44 INFO ShutdownHookManager: Shutdown hook called\n",
            "23/04/13 04:00:44 INFO ShutdownHookManager: Deleting directory /tmp/spark-4e630c1f-110e-45c4-b4a1-3077bb5fb4ba\n",
            "23/04/13 04:00:44 INFO ShutdownHookManager: Deleting directory /tmp/spark-39d63502-6af8-421b-a7ea-38d04e7656a3/pyspark-16005473-2bb5-4618-9077-1ea98110fe15\n",
            "23/04/13 04:00:44 INFO ShutdownHookManager: Deleting directory /tmp/spark-39d63502-6af8-421b-a7ea-38d04e7656a3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvpqT474__D0",
        "outputId": "8c7e3ceb-58de-4073-d7b5-132634f677bb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def test2(lines):\n",
        "    assert lines[0].strip() == \"('rpsabo', 88)\"\n",
        "    print(\"passed\")\n",
        "\n",
        "# test local execution results\n",
        "with open('2_group.out/part-00000') as f:\n",
        "  lines = f.readlines()\n",
        "  test2(lines)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "passed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Execute on EMR"
      ],
      "metadata": {
        "id": "pPB6jw0VvT_b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# upload script to S3. This assumes that your bucket name is vandy-bigdata. if not then change the  paths here.\n",
        "s3.upload_file(Filename='2_group.py', Bucket='far-bigdata-spark', Key='hw6/2_group.py')"
      ],
      "metadata": {
        "id": "h384dEtNvZoi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# replae with your EMR cluster ID\n",
        "CLUSTER_ID = 'j-BP8EOODICRFL'\n",
        "\n",
        "def submit_job(app_name, pyfile_uri):\n",
        "    emr = session.client('emr')\n",
        "    emr.add_job_flow_steps(JobFlowId=CLUSTER_ID, Steps=[{\n",
        "        'Name': app_name,\n",
        "        'ActionOnFailure': 'CANCEL_AND_WAIT',\n",
        "        'HadoopJarStep': {\n",
        "            'Args': ['spark-submit',\n",
        "                     '--master', 'yarn',\n",
        "                     '--deploy-mode', 'cluster',\n",
        "                     pyfile_uri],\n",
        "            'Jar': 'command-runner.jar'\n",
        "        }}])"
      ],
      "metadata": {
        "id": "VQvRTcHNwJF7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5xsLbJd0wJF7"
      },
      "source": [
        "# submit spark job to emr. Make all the necessary changes to the path\n",
        "submit_job(app_name='2_group', pyfile_uri='s3://far-bigdata-spark/hw6/2_group.py')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "outputId": "e2409763-23c7-44ee-c66a-6de4782af1e3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b-77tKXNwJF8"
      },
      "source": [
        "# test EMR execution results. Once again, make sure that S3 paths are consistent.\n",
        "output_key = \"hw6/2_group.out/part-00000\"\n",
        "lines = s3.get_object(Bucket='far-bigdata-spark', Key=output_key)['Body'].read().decode().splitlines()\n",
        "test2(lines)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "passed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DeAaRlMR__D1"
      },
      "source": [
        "## Job 3. Count the tweets per day."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fTMTHj9O__D2",
        "outputId": "08f090ef-6ba2-4060-e77c-6f58a77f868d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%file 3_days.py\n",
        "\n",
        "'''\n",
        "TODO:\n",
        "Count the tweets per day.\n",
        "\n",
        "See http://mike.teczno.com/notes/streaming-data-from-twitter.html for parsing info.\n",
        "Get the screen name by accessing tweet['user']['screen_name']\n",
        "\n",
        "Look at tweet['created_at'] for datetime of creation. Just use the first word in the date to get the day.\n",
        "\n",
        "'''\n",
        "\n",
        "\n",
        "from pyspark import SparkContext, SparkConf\n",
        "import json\n",
        "from operator import add\n",
        "from datetime import datetime\n",
        "\n",
        "def checkjson(entry):\n",
        "    try:\n",
        "        json.loads(entry)\n",
        "        return entry\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "def get_day(entry):\n",
        "    if entry is not None:\n",
        "        tweet = json.loads(entry)\n",
        "        created_at = tweet['created_at']\n",
        "        day = datetime.strptime(created_at, '%a %b %d %H:%M:%S %z %Y').strftime('%a')\n",
        "        return day, 1\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    #conf = SparkConf().setAppName('3_days').set('spark.hadoop.validateOutputSpecs', False)\n",
        "    sc = SparkContext(appName=\"3_days\")\n",
        "\n",
        "    try:\n",
        "        tweets = sc.textFile('s3://far-bigdata-spark/datasets/nashville-tweets-2019-01-28')\n",
        "        valid_tweets = tweets.map(lambda tweet: checkjson(tweet)).filter(lambda x: x is not None)\n",
        "        days = valid_tweets.map(lambda entry: get_day(entry)).filter(lambda x: x is not None)\n",
        "        counts = days.reduceByKey(add)\n",
        "        output = counts\n",
        "        output.repartition(1).saveAsTextFile(\"s3://far-bigdata-spark/hw6/3_days.out\")\n",
        "\n",
        "    finally:\n",
        "        sc.stop()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting 3_days.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6phVcUW__D2",
        "outputId": "37113ac2-5737-4f81-e09d-a3c8bf3da41f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# execute locally\n",
        "!spark-submit --packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.4.7 3_days.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ":: loading settings :: url = jar:file:/content/spark-3.2.3-bin-hadoop3.2/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
            "Ivy Default Cache set to: /root/.ivy2/cache\n",
            "The jars for the packages stored in: /root/.ivy2/jars\n",
            "org.apache.spark#spark-streaming-kafka-0-8_2.11 added as a dependency\n",
            ":: resolving dependencies :: org.apache.spark#spark-submit-parent-22a68f82-cfc6-4c72-aba9-2ea7849a4ce7;1.0\n",
            "\tconfs: [default]\n",
            "\tfound org.apache.spark#spark-streaming-kafka-0-8_2.11;2.4.7 in central\n",
            "\tfound org.apache.kafka#kafka_2.11;0.8.2.1 in central\n",
            "\tfound org.scala-lang.modules#scala-xml_2.11;1.0.2 in central\n",
            "\tfound com.yammer.metrics#metrics-core;2.2.0 in central\n",
            "\tfound org.slf4j#slf4j-api;1.7.16 in central\n",
            "\tfound org.scala-lang.modules#scala-parser-combinators_2.11;1.1.0 in central\n",
            "\tfound com.101tec#zkclient;0.3 in central\n",
            "\tfound log4j#log4j;1.2.17 in central\n",
            "\tfound org.apache.kafka#kafka-clients;0.8.2.1 in central\n",
            "\tfound net.jpountz.lz4#lz4;1.2.0 in central\n",
            "\tfound org.xerial.snappy#snappy-java;1.1.7.5 in central\n",
            "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
            ":: resolution report :: resolve 935ms :: artifacts dl 57ms\n",
            "\t:: modules in use:\n",
            "\tcom.101tec#zkclient;0.3 from central in [default]\n",
            "\tcom.yammer.metrics#metrics-core;2.2.0 from central in [default]\n",
            "\tlog4j#log4j;1.2.17 from central in [default]\n",
            "\tnet.jpountz.lz4#lz4;1.2.0 from central in [default]\n",
            "\torg.apache.kafka#kafka-clients;0.8.2.1 from central in [default]\n",
            "\torg.apache.kafka#kafka_2.11;0.8.2.1 from central in [default]\n",
            "\torg.apache.spark#spark-streaming-kafka-0-8_2.11;2.4.7 from central in [default]\n",
            "\torg.scala-lang.modules#scala-parser-combinators_2.11;1.1.0 from central in [default]\n",
            "\torg.scala-lang.modules#scala-xml_2.11;1.0.2 from central in [default]\n",
            "\torg.slf4j#slf4j-api;1.7.16 from central in [default]\n",
            "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
            "\torg.xerial.snappy#snappy-java;1.1.7.5 from central in [default]\n",
            "\t---------------------------------------------------------------------\n",
            "\t|                  |            modules            ||   artifacts   |\n",
            "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
            "\t---------------------------------------------------------------------\n",
            "\t|      default     |   12  |   0   |   0   |   0   ||   12  |   0   |\n",
            "\t---------------------------------------------------------------------\n",
            ":: retrieving :: org.apache.spark#spark-submit-parent-22a68f82-cfc6-4c72-aba9-2ea7849a4ce7\n",
            "\tconfs: [default]\n",
            "\t0 artifacts copied, 12 already retrieved (0kB/24ms)\n",
            "23/04/13 02:49:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
            "23/04/13 02:49:03 INFO SparkContext: Running Spark version 3.2.3\n",
            "23/04/13 02:49:03 INFO ResourceUtils: ==============================================================\n",
            "23/04/13 02:49:03 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "23/04/13 02:49:03 INFO ResourceUtils: ==============================================================\n",
            "23/04/13 02:49:03 INFO SparkContext: Submitted application: 3_days\n",
            "23/04/13 02:49:03 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "23/04/13 02:49:03 INFO ResourceProfile: Limiting resource is cpu\n",
            "23/04/13 02:49:03 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "23/04/13 02:49:03 INFO SecurityManager: Changing view acls to: root\n",
            "23/04/13 02:49:03 INFO SecurityManager: Changing modify acls to: root\n",
            "23/04/13 02:49:03 INFO SecurityManager: Changing view acls groups to: \n",
            "23/04/13 02:49:03 INFO SecurityManager: Changing modify acls groups to: \n",
            "23/04/13 02:49:03 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\n",
            "23/04/13 02:49:04 INFO Utils: Successfully started service 'sparkDriver' on port 39051.\n",
            "23/04/13 02:49:04 INFO SparkEnv: Registering MapOutputTracker\n",
            "23/04/13 02:49:04 INFO SparkEnv: Registering BlockManagerMaster\n",
            "23/04/13 02:49:04 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "23/04/13 02:49:04 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "23/04/13 02:49:04 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "23/04/13 02:49:04 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-af3e19c8-bebd-426b-818b-b1a74b39f705\n",
            "23/04/13 02:49:04 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB\n",
            "23/04/13 02:49:04 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "23/04/13 02:49:05 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
            "23/04/13 02:49:05 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://a1ad50929c8d:4040\n",
            "23/04/13 02:49:05 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar at spark://a1ad50929c8d:39051/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1681354143622\n",
            "23/04/13 02:49:05 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar at spark://a1ad50929c8d:39051/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1681354143622\n",
            "23/04/13 02:49:05 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar at spark://a1ad50929c8d:39051/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1681354143622\n",
            "23/04/13 02:49:05 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar at spark://a1ad50929c8d:39051/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1681354143622\n",
            "23/04/13 02:49:05 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar at spark://a1ad50929c8d:39051/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1681354143622\n",
            "23/04/13 02:49:05 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar at spark://a1ad50929c8d:39051/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1681354143622\n",
            "23/04/13 02:49:05 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar at spark://a1ad50929c8d:39051/jars/com.101tec_zkclient-0.3.jar with timestamp 1681354143622\n",
            "23/04/13 02:49:05 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar at spark://a1ad50929c8d:39051/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1681354143622\n",
            "23/04/13 02:49:05 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar at spark://a1ad50929c8d:39051/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1681354143622\n",
            "23/04/13 02:49:05 INFO SparkContext: Added JAR file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar at spark://a1ad50929c8d:39051/jars/log4j_log4j-1.2.17.jar with timestamp 1681354143622\n",
            "23/04/13 02:49:05 INFO SparkContext: Added JAR file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar at spark://a1ad50929c8d:39051/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1681354143622\n",
            "23/04/13 02:49:05 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar at spark://a1ad50929c8d:39051/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1681354143622\n",
            "23/04/13 02:49:05 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar at file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1681354143622\n",
            "23/04/13 02:49:05 INFO Utils: Copying /root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar to /tmp/spark-86603530-5dc7-46a4-989d-dc34481a9ba9/userFiles-e9b9eee7-e175-4a55-8f2f-2263a951b811/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar\n",
            "23/04/13 02:49:05 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar at file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1681354143622\n",
            "23/04/13 02:49:05 INFO Utils: Copying /root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar to /tmp/spark-86603530-5dc7-46a4-989d-dc34481a9ba9/userFiles-e9b9eee7-e175-4a55-8f2f-2263a951b811/org.apache.kafka_kafka_2.11-0.8.2.1.jar\n",
            "23/04/13 02:49:05 INFO SparkContext: Added file file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar at file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1681354143622\n",
            "23/04/13 02:49:05 INFO Utils: Copying /root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-86603530-5dc7-46a4-989d-dc34481a9ba9/userFiles-e9b9eee7-e175-4a55-8f2f-2263a951b811/org.spark-project.spark_unused-1.0.0.jar\n",
            "23/04/13 02:49:05 INFO SparkContext: Added file file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar at file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1681354143622\n",
            "23/04/13 02:49:05 INFO Utils: Copying /root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar to /tmp/spark-86603530-5dc7-46a4-989d-dc34481a9ba9/userFiles-e9b9eee7-e175-4a55-8f2f-2263a951b811/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar\n",
            "23/04/13 02:49:05 INFO SparkContext: Added file file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar at file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1681354143622\n",
            "23/04/13 02:49:05 INFO Utils: Copying /root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar to /tmp/spark-86603530-5dc7-46a4-989d-dc34481a9ba9/userFiles-e9b9eee7-e175-4a55-8f2f-2263a951b811/com.yammer.metrics_metrics-core-2.2.0.jar\n",
            "23/04/13 02:49:05 INFO SparkContext: Added file file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar at file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1681354143622\n",
            "23/04/13 02:49:05 INFO Utils: Copying /root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar to /tmp/spark-86603530-5dc7-46a4-989d-dc34481a9ba9/userFiles-e9b9eee7-e175-4a55-8f2f-2263a951b811/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar\n",
            "23/04/13 02:49:05 INFO SparkContext: Added file file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar at file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar with timestamp 1681354143622\n",
            "23/04/13 02:49:05 INFO Utils: Copying /root/.ivy2/jars/com.101tec_zkclient-0.3.jar to /tmp/spark-86603530-5dc7-46a4-989d-dc34481a9ba9/userFiles-e9b9eee7-e175-4a55-8f2f-2263a951b811/com.101tec_zkclient-0.3.jar\n",
            "23/04/13 02:49:05 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar at file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1681354143622\n",
            "23/04/13 02:49:05 INFO Utils: Copying /root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar to /tmp/spark-86603530-5dc7-46a4-989d-dc34481a9ba9/userFiles-e9b9eee7-e175-4a55-8f2f-2263a951b811/org.apache.kafka_kafka-clients-0.8.2.1.jar\n",
            "23/04/13 02:49:05 INFO SparkContext: Added file file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar at file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1681354143622\n",
            "23/04/13 02:49:05 INFO Utils: Copying /root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar to /tmp/spark-86603530-5dc7-46a4-989d-dc34481a9ba9/userFiles-e9b9eee7-e175-4a55-8f2f-2263a951b811/org.slf4j_slf4j-api-1.7.16.jar\n",
            "23/04/13 02:49:05 INFO SparkContext: Added file file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar at file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar with timestamp 1681354143622\n",
            "23/04/13 02:49:05 INFO Utils: Copying /root/.ivy2/jars/log4j_log4j-1.2.17.jar to /tmp/spark-86603530-5dc7-46a4-989d-dc34481a9ba9/userFiles-e9b9eee7-e175-4a55-8f2f-2263a951b811/log4j_log4j-1.2.17.jar\n",
            "23/04/13 02:49:05 INFO SparkContext: Added file file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar at file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1681354143622\n",
            "23/04/13 02:49:05 INFO Utils: Copying /root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar to /tmp/spark-86603530-5dc7-46a4-989d-dc34481a9ba9/userFiles-e9b9eee7-e175-4a55-8f2f-2263a951b811/net.jpountz.lz4_lz4-1.2.0.jar\n",
            "23/04/13 02:49:05 INFO SparkContext: Added file file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar at file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1681354143622\n",
            "23/04/13 02:49:05 INFO Utils: Copying /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar to /tmp/spark-86603530-5dc7-46a4-989d-dc34481a9ba9/userFiles-e9b9eee7-e175-4a55-8f2f-2263a951b811/org.xerial.snappy_snappy-java-1.1.7.5.jar\n",
            "23/04/13 02:49:05 INFO Executor: Starting executor ID driver on host a1ad50929c8d\n",
            "23/04/13 02:49:05 INFO Executor: Fetching file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar with timestamp 1681354143622\n",
            "23/04/13 02:49:05 INFO Utils: /root/.ivy2/jars/log4j_log4j-1.2.17.jar has been previously copied to /tmp/spark-86603530-5dc7-46a4-989d-dc34481a9ba9/userFiles-e9b9eee7-e175-4a55-8f2f-2263a951b811/log4j_log4j-1.2.17.jar\n",
            "23/04/13 02:49:05 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1681354143622\n",
            "23/04/13 02:49:05 INFO Utils: /root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar has been previously copied to /tmp/spark-86603530-5dc7-46a4-989d-dc34481a9ba9/userFiles-e9b9eee7-e175-4a55-8f2f-2263a951b811/org.apache.kafka_kafka-clients-0.8.2.1.jar\n",
            "23/04/13 02:49:05 INFO Executor: Fetching file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1681354143622\n",
            "23/04/13 02:49:05 INFO Utils: /root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar has been previously copied to /tmp/spark-86603530-5dc7-46a4-989d-dc34481a9ba9/userFiles-e9b9eee7-e175-4a55-8f2f-2263a951b811/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar\n",
            "23/04/13 02:49:05 INFO Executor: Fetching file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1681354143622\n",
            "23/04/13 02:49:05 INFO Utils: /root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar has been previously copied to /tmp/spark-86603530-5dc7-46a4-989d-dc34481a9ba9/userFiles-e9b9eee7-e175-4a55-8f2f-2263a951b811/org.slf4j_slf4j-api-1.7.16.jar\n",
            "23/04/13 02:49:05 INFO Executor: Fetching file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1681354143622\n",
            "23/04/13 02:49:05 INFO Utils: /root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar has been previously copied to /tmp/spark-86603530-5dc7-46a4-989d-dc34481a9ba9/userFiles-e9b9eee7-e175-4a55-8f2f-2263a951b811/net.jpountz.lz4_lz4-1.2.0.jar\n",
            "23/04/13 02:49:05 INFO Executor: Fetching file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1681354143622\n",
            "23/04/13 02:49:05 INFO Utils: /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar has been previously copied to /tmp/spark-86603530-5dc7-46a4-989d-dc34481a9ba9/userFiles-e9b9eee7-e175-4a55-8f2f-2263a951b811/org.xerial.snappy_snappy-java-1.1.7.5.jar\n",
            "23/04/13 02:49:05 INFO Executor: Fetching file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1681354143622\n",
            "23/04/13 02:49:05 INFO Utils: /root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar has been previously copied to /tmp/spark-86603530-5dc7-46a4-989d-dc34481a9ba9/userFiles-e9b9eee7-e175-4a55-8f2f-2263a951b811/org.spark-project.spark_unused-1.0.0.jar\n",
            "23/04/13 02:49:05 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1681354143622\n",
            "23/04/13 02:49:05 INFO Utils: /root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar has been previously copied to /tmp/spark-86603530-5dc7-46a4-989d-dc34481a9ba9/userFiles-e9b9eee7-e175-4a55-8f2f-2263a951b811/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar\n",
            "23/04/13 02:49:05 INFO Executor: Fetching file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1681354143622\n",
            "23/04/13 02:49:05 INFO Utils: /root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar has been previously copied to /tmp/spark-86603530-5dc7-46a4-989d-dc34481a9ba9/userFiles-e9b9eee7-e175-4a55-8f2f-2263a951b811/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar\n",
            "23/04/13 02:49:05 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1681354143622\n",
            "23/04/13 02:49:05 INFO Utils: /root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar has been previously copied to /tmp/spark-86603530-5dc7-46a4-989d-dc34481a9ba9/userFiles-e9b9eee7-e175-4a55-8f2f-2263a951b811/org.apache.kafka_kafka_2.11-0.8.2.1.jar\n",
            "23/04/13 02:49:05 INFO Executor: Fetching file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar with timestamp 1681354143622\n",
            "23/04/13 02:49:05 INFO Utils: /root/.ivy2/jars/com.101tec_zkclient-0.3.jar has been previously copied to /tmp/spark-86603530-5dc7-46a4-989d-dc34481a9ba9/userFiles-e9b9eee7-e175-4a55-8f2f-2263a951b811/com.101tec_zkclient-0.3.jar\n",
            "23/04/13 02:49:05 INFO Executor: Fetching file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1681354143622\n",
            "23/04/13 02:49:05 INFO Utils: /root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar has been previously copied to /tmp/spark-86603530-5dc7-46a4-989d-dc34481a9ba9/userFiles-e9b9eee7-e175-4a55-8f2f-2263a951b811/com.yammer.metrics_metrics-core-2.2.0.jar\n",
            "23/04/13 02:49:05 INFO Executor: Fetching spark://a1ad50929c8d:39051/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1681354143622\n",
            "23/04/13 02:49:06 INFO TransportClientFactory: Successfully created connection to a1ad50929c8d/172.28.0.12:39051 after 55 ms (0 ms spent in bootstraps)\n",
            "23/04/13 02:49:06 INFO Utils: Fetching spark://a1ad50929c8d:39051/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar to /tmp/spark-86603530-5dc7-46a4-989d-dc34481a9ba9/userFiles-e9b9eee7-e175-4a55-8f2f-2263a951b811/fetchFileTemp7539301258197281487.tmp\n",
            "23/04/13 02:49:06 INFO Utils: /tmp/spark-86603530-5dc7-46a4-989d-dc34481a9ba9/userFiles-e9b9eee7-e175-4a55-8f2f-2263a951b811/fetchFileTemp7539301258197281487.tmp has been previously copied to /tmp/spark-86603530-5dc7-46a4-989d-dc34481a9ba9/userFiles-e9b9eee7-e175-4a55-8f2f-2263a951b811/org.xerial.snappy_snappy-java-1.1.7.5.jar\n",
            "23/04/13 02:49:06 INFO Executor: Adding file:/tmp/spark-86603530-5dc7-46a4-989d-dc34481a9ba9/userFiles-e9b9eee7-e175-4a55-8f2f-2263a951b811/org.xerial.snappy_snappy-java-1.1.7.5.jar to class loader\n",
            "23/04/13 02:49:06 INFO Executor: Fetching spark://a1ad50929c8d:39051/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1681354143622\n",
            "23/04/13 02:49:06 INFO Utils: Fetching spark://a1ad50929c8d:39051/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-86603530-5dc7-46a4-989d-dc34481a9ba9/userFiles-e9b9eee7-e175-4a55-8f2f-2263a951b811/fetchFileTemp7932571237513038479.tmp\n",
            "23/04/13 02:49:06 INFO Utils: /tmp/spark-86603530-5dc7-46a4-989d-dc34481a9ba9/userFiles-e9b9eee7-e175-4a55-8f2f-2263a951b811/fetchFileTemp7932571237513038479.tmp has been previously copied to /tmp/spark-86603530-5dc7-46a4-989d-dc34481a9ba9/userFiles-e9b9eee7-e175-4a55-8f2f-2263a951b811/org.spark-project.spark_unused-1.0.0.jar\n",
            "23/04/13 02:49:06 INFO Executor: Adding file:/tmp/spark-86603530-5dc7-46a4-989d-dc34481a9ba9/userFiles-e9b9eee7-e175-4a55-8f2f-2263a951b811/org.spark-project.spark_unused-1.0.0.jar to class loader\n",
            "23/04/13 02:49:06 INFO Executor: Fetching spark://a1ad50929c8d:39051/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1681354143622\n",
            "23/04/13 02:49:06 INFO Utils: Fetching spark://a1ad50929c8d:39051/jars/net.jpountz.lz4_lz4-1.2.0.jar to /tmp/spark-86603530-5dc7-46a4-989d-dc34481a9ba9/userFiles-e9b9eee7-e175-4a55-8f2f-2263a951b811/fetchFileTemp5099709493451644649.tmp\n",
            "23/04/13 02:49:06 INFO Utils: /tmp/spark-86603530-5dc7-46a4-989d-dc34481a9ba9/userFiles-e9b9eee7-e175-4a55-8f2f-2263a951b811/fetchFileTemp5099709493451644649.tmp has been previously copied to /tmp/spark-86603530-5dc7-46a4-989d-dc34481a9ba9/userFiles-e9b9eee7-e175-4a55-8f2f-2263a951b811/net.jpountz.lz4_lz4-1.2.0.jar\n",
            "23/04/13 02:49:06 INFO Executor: Adding file:/tmp/spark-86603530-5dc7-46a4-989d-dc34481a9ba9/userFiles-e9b9eee7-e175-4a55-8f2f-2263a951b811/net.jpountz.lz4_lz4-1.2.0.jar to class loader\n",
            "23/04/13 02:49:06 INFO Executor: Fetching spark://a1ad50929c8d:39051/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1681354143622\n",
            "23/04/13 02:49:06 INFO Utils: Fetching spark://a1ad50929c8d:39051/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar to /tmp/spark-86603530-5dc7-46a4-989d-dc34481a9ba9/userFiles-e9b9eee7-e175-4a55-8f2f-2263a951b811/fetchFileTemp241904378272128517.tmp\n",
            "23/04/13 02:49:06 INFO Utils: /tmp/spark-86603530-5dc7-46a4-989d-dc34481a9ba9/userFiles-e9b9eee7-e175-4a55-8f2f-2263a951b811/fetchFileTemp241904378272128517.tmp has been previously copied to /tmp/spark-86603530-5dc7-46a4-989d-dc34481a9ba9/userFiles-e9b9eee7-e175-4a55-8f2f-2263a951b811/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar\n",
            "23/04/13 02:49:06 INFO Executor: Adding file:/tmp/spark-86603530-5dc7-46a4-989d-dc34481a9ba9/userFiles-e9b9eee7-e175-4a55-8f2f-2263a951b811/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar to class loader\n",
            "23/04/13 02:49:06 INFO Executor: Fetching spark://a1ad50929c8d:39051/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1681354143622\n",
            "23/04/13 02:49:06 INFO Utils: Fetching spark://a1ad50929c8d:39051/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar to /tmp/spark-86603530-5dc7-46a4-989d-dc34481a9ba9/userFiles-e9b9eee7-e175-4a55-8f2f-2263a951b811/fetchFileTemp7393022324085889856.tmp\n",
            "23/04/13 02:49:06 INFO Utils: /tmp/spark-86603530-5dc7-46a4-989d-dc34481a9ba9/userFiles-e9b9eee7-e175-4a55-8f2f-2263a951b811/fetchFileTemp7393022324085889856.tmp has been previously copied to /tmp/spark-86603530-5dc7-46a4-989d-dc34481a9ba9/userFiles-e9b9eee7-e175-4a55-8f2f-2263a951b811/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar\n",
            "23/04/13 02:49:06 INFO Executor: Adding file:/tmp/spark-86603530-5dc7-46a4-989d-dc34481a9ba9/userFiles-e9b9eee7-e175-4a55-8f2f-2263a951b811/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar to class loader\n",
            "23/04/13 02:49:06 INFO Executor: Fetching spark://a1ad50929c8d:39051/jars/com.101tec_zkclient-0.3.jar with timestamp 1681354143622\n",
            "23/04/13 02:49:06 INFO Utils: Fetching spark://a1ad50929c8d:39051/jars/com.101tec_zkclient-0.3.jar to /tmp/spark-86603530-5dc7-46a4-989d-dc34481a9ba9/userFiles-e9b9eee7-e175-4a55-8f2f-2263a951b811/fetchFileTemp1517167584604737315.tmp\n",
            "23/04/13 02:49:06 INFO Utils: /tmp/spark-86603530-5dc7-46a4-989d-dc34481a9ba9/userFiles-e9b9eee7-e175-4a55-8f2f-2263a951b811/fetchFileTemp1517167584604737315.tmp has been previously copied to /tmp/spark-86603530-5dc7-46a4-989d-dc34481a9ba9/userFiles-e9b9eee7-e175-4a55-8f2f-2263a951b811/com.101tec_zkclient-0.3.jar\n",
            "23/04/13 02:49:06 INFO Executor: Adding file:/tmp/spark-86603530-5dc7-46a4-989d-dc34481a9ba9/userFiles-e9b9eee7-e175-4a55-8f2f-2263a951b811/com.101tec_zkclient-0.3.jar to class loader\n",
            "23/04/13 02:49:06 INFO Executor: Fetching spark://a1ad50929c8d:39051/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1681354143622\n",
            "23/04/13 02:49:06 INFO Utils: Fetching spark://a1ad50929c8d:39051/jars/org.slf4j_slf4j-api-1.7.16.jar to /tmp/spark-86603530-5dc7-46a4-989d-dc34481a9ba9/userFiles-e9b9eee7-e175-4a55-8f2f-2263a951b811/fetchFileTemp8762992993819700238.tmp\n",
            "23/04/13 02:49:06 INFO Utils: /tmp/spark-86603530-5dc7-46a4-989d-dc34481a9ba9/userFiles-e9b9eee7-e175-4a55-8f2f-2263a951b811/fetchFileTemp8762992993819700238.tmp has been previously copied to /tmp/spark-86603530-5dc7-46a4-989d-dc34481a9ba9/userFiles-e9b9eee7-e175-4a55-8f2f-2263a951b811/org.slf4j_slf4j-api-1.7.16.jar\n",
            "23/04/13 02:49:06 INFO Executor: Adding file:/tmp/spark-86603530-5dc7-46a4-989d-dc34481a9ba9/userFiles-e9b9eee7-e175-4a55-8f2f-2263a951b811/org.slf4j_slf4j-api-1.7.16.jar to class loader\n",
            "23/04/13 02:49:06 INFO Executor: Fetching spark://a1ad50929c8d:39051/jars/log4j_log4j-1.2.17.jar with timestamp 1681354143622\n",
            "23/04/13 02:49:06 INFO Utils: Fetching spark://a1ad50929c8d:39051/jars/log4j_log4j-1.2.17.jar to /tmp/spark-86603530-5dc7-46a4-989d-dc34481a9ba9/userFiles-e9b9eee7-e175-4a55-8f2f-2263a951b811/fetchFileTemp8205916950324219427.tmp\n",
            "23/04/13 02:49:06 INFO Utils: /tmp/spark-86603530-5dc7-46a4-989d-dc34481a9ba9/userFiles-e9b9eee7-e175-4a55-8f2f-2263a951b811/fetchFileTemp8205916950324219427.tmp has been previously copied to /tmp/spark-86603530-5dc7-46a4-989d-dc34481a9ba9/userFiles-e9b9eee7-e175-4a55-8f2f-2263a951b811/log4j_log4j-1.2.17.jar\n",
            "23/04/13 02:49:06 INFO Executor: Adding file:/tmp/spark-86603530-5dc7-46a4-989d-dc34481a9ba9/userFiles-e9b9eee7-e175-4a55-8f2f-2263a951b811/log4j_log4j-1.2.17.jar to class loader\n",
            "23/04/13 02:49:06 INFO Executor: Fetching spark://a1ad50929c8d:39051/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1681354143622\n",
            "23/04/13 02:49:06 INFO Utils: Fetching spark://a1ad50929c8d:39051/jars/com.yammer.metrics_metrics-core-2.2.0.jar to /tmp/spark-86603530-5dc7-46a4-989d-dc34481a9ba9/userFiles-e9b9eee7-e175-4a55-8f2f-2263a951b811/fetchFileTemp2903617219809161296.tmp\n",
            "23/04/13 02:49:06 INFO Utils: /tmp/spark-86603530-5dc7-46a4-989d-dc34481a9ba9/userFiles-e9b9eee7-e175-4a55-8f2f-2263a951b811/fetchFileTemp2903617219809161296.tmp has been previously copied to /tmp/spark-86603530-5dc7-46a4-989d-dc34481a9ba9/userFiles-e9b9eee7-e175-4a55-8f2f-2263a951b811/com.yammer.metrics_metrics-core-2.2.0.jar\n",
            "23/04/13 02:49:06 INFO Executor: Adding file:/tmp/spark-86603530-5dc7-46a4-989d-dc34481a9ba9/userFiles-e9b9eee7-e175-4a55-8f2f-2263a951b811/com.yammer.metrics_metrics-core-2.2.0.jar to class loader\n",
            "23/04/13 02:49:06 INFO Executor: Fetching spark://a1ad50929c8d:39051/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1681354143622\n",
            "23/04/13 02:49:06 INFO Utils: Fetching spark://a1ad50929c8d:39051/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar to /tmp/spark-86603530-5dc7-46a4-989d-dc34481a9ba9/userFiles-e9b9eee7-e175-4a55-8f2f-2263a951b811/fetchFileTemp6051030241803373320.tmp\n",
            "23/04/13 02:49:06 INFO Utils: /tmp/spark-86603530-5dc7-46a4-989d-dc34481a9ba9/userFiles-e9b9eee7-e175-4a55-8f2f-2263a951b811/fetchFileTemp6051030241803373320.tmp has been previously copied to /tmp/spark-86603530-5dc7-46a4-989d-dc34481a9ba9/userFiles-e9b9eee7-e175-4a55-8f2f-2263a951b811/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar\n",
            "23/04/13 02:49:06 INFO Executor: Adding file:/tmp/spark-86603530-5dc7-46a4-989d-dc34481a9ba9/userFiles-e9b9eee7-e175-4a55-8f2f-2263a951b811/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar to class loader\n",
            "23/04/13 02:49:06 INFO Executor: Fetching spark://a1ad50929c8d:39051/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1681354143622\n",
            "23/04/13 02:49:06 INFO Utils: Fetching spark://a1ad50929c8d:39051/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar to /tmp/spark-86603530-5dc7-46a4-989d-dc34481a9ba9/userFiles-e9b9eee7-e175-4a55-8f2f-2263a951b811/fetchFileTemp3021386885796913363.tmp\n",
            "23/04/13 02:49:06 INFO Utils: /tmp/spark-86603530-5dc7-46a4-989d-dc34481a9ba9/userFiles-e9b9eee7-e175-4a55-8f2f-2263a951b811/fetchFileTemp3021386885796913363.tmp has been previously copied to /tmp/spark-86603530-5dc7-46a4-989d-dc34481a9ba9/userFiles-e9b9eee7-e175-4a55-8f2f-2263a951b811/org.apache.kafka_kafka-clients-0.8.2.1.jar\n",
            "23/04/13 02:49:06 INFO Executor: Adding file:/tmp/spark-86603530-5dc7-46a4-989d-dc34481a9ba9/userFiles-e9b9eee7-e175-4a55-8f2f-2263a951b811/org.apache.kafka_kafka-clients-0.8.2.1.jar to class loader\n",
            "23/04/13 02:49:06 INFO Executor: Fetching spark://a1ad50929c8d:39051/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1681354143622\n",
            "23/04/13 02:49:06 INFO Utils: Fetching spark://a1ad50929c8d:39051/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar to /tmp/spark-86603530-5dc7-46a4-989d-dc34481a9ba9/userFiles-e9b9eee7-e175-4a55-8f2f-2263a951b811/fetchFileTemp4691220502140593395.tmp\n",
            "23/04/13 02:49:06 INFO Utils: /tmp/spark-86603530-5dc7-46a4-989d-dc34481a9ba9/userFiles-e9b9eee7-e175-4a55-8f2f-2263a951b811/fetchFileTemp4691220502140593395.tmp has been previously copied to /tmp/spark-86603530-5dc7-46a4-989d-dc34481a9ba9/userFiles-e9b9eee7-e175-4a55-8f2f-2263a951b811/org.apache.kafka_kafka_2.11-0.8.2.1.jar\n",
            "23/04/13 02:49:06 INFO Executor: Adding file:/tmp/spark-86603530-5dc7-46a4-989d-dc34481a9ba9/userFiles-e9b9eee7-e175-4a55-8f2f-2263a951b811/org.apache.kafka_kafka_2.11-0.8.2.1.jar to class loader\n",
            "23/04/13 02:49:06 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 32843.\n",
            "23/04/13 02:49:06 INFO NettyBlockTransferService: Server created on a1ad50929c8d:32843\n",
            "23/04/13 02:49:06 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "23/04/13 02:49:06 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, a1ad50929c8d, 32843, None)\n",
            "23/04/13 02:49:06 INFO BlockManagerMasterEndpoint: Registering block manager a1ad50929c8d:32843 with 366.3 MiB RAM, BlockManagerId(driver, a1ad50929c8d, 32843, None)\n",
            "23/04/13 02:49:06 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, a1ad50929c8d, 32843, None)\n",
            "23/04/13 02:49:06 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, a1ad50929c8d, 32843, None)\n",
            "23/04/13 02:49:07 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 354.3 KiB, free 366.0 MiB)\n",
            "23/04/13 02:49:08 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 32.0 KiB, free 365.9 MiB)\n",
            "23/04/13 02:49:08 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on a1ad50929c8d:32843 (size: 32.0 KiB, free: 366.3 MiB)\n",
            "23/04/13 02:49:08 INFO SparkContext: Created broadcast 0 from textFile at NativeMethodAccessorImpl.java:0\n",
            "23/04/13 02:49:08 INFO FileInputFormat: Total input files to process : 1\n",
            "23/04/13 02:49:08 INFO deprecation: mapred.output.dir is deprecated. Instead, use mapreduce.output.fileoutputformat.outputdir\n",
            "23/04/13 02:49:08 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
            "23/04/13 02:49:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "23/04/13 02:49:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "23/04/13 02:49:09 INFO SparkContext: Starting job: runJob at SparkHadoopWriter.scala:83\n",
            "23/04/13 02:49:09 INFO DAGScheduler: Registering RDD 3 (reduceByKey at /content/3_days.py:43) as input to shuffle 1\n",
            "23/04/13 02:49:09 INFO DAGScheduler: Registering RDD 7 (coalesce at NativeMethodAccessorImpl.java:0) as input to shuffle 0\n",
            "23/04/13 02:49:09 INFO DAGScheduler: Got job 0 (runJob at SparkHadoopWriter.scala:83) with 1 output partitions\n",
            "23/04/13 02:49:09 INFO DAGScheduler: Final stage: ResultStage 2 (runJob at SparkHadoopWriter.scala:83)\n",
            "23/04/13 02:49:09 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 1)\n",
            "23/04/13 02:49:09 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 1)\n",
            "23/04/13 02:49:09 INFO DAGScheduler: Submitting ShuffleMapStage 0 (PairwiseRDD[3] at reduceByKey at /content/3_days.py:43), which has no missing parents\n",
            "23/04/13 02:49:10 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 13.7 KiB, free 365.9 MiB)\n",
            "23/04/13 02:49:10 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 8.1 KiB, free 365.9 MiB)\n",
            "23/04/13 02:49:10 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on a1ad50929c8d:32843 (size: 8.1 KiB, free: 366.3 MiB)\n",
            "23/04/13 02:49:10 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1478\n",
            "23/04/13 02:49:10 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 0 (PairwiseRDD[3] at reduceByKey at /content/3_days.py:43) (first 15 tasks are for partitions Vector(0, 1))\n",
            "23/04/13 02:49:10 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks resource profile 0\n",
            "23/04/13 02:49:10 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (a1ad50929c8d, executor driver, partition 0, PROCESS_LOCAL, 4499 bytes) taskResourceAssignments Map()\n",
            "23/04/13 02:49:10 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1) (a1ad50929c8d, executor driver, partition 1, PROCESS_LOCAL, 4499 bytes) taskResourceAssignments Map()\n",
            "23/04/13 02:49:10 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
            "23/04/13 02:49:10 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)\n",
            "23/04/13 02:49:11 INFO HadoopRDD: Input split: file:/content/nashville-tweets-2019-01-28:12519625+12519626\n",
            "23/04/13 02:49:11 INFO HadoopRDD: Input split: file:/content/nashville-tweets-2019-01-28:0+12519625\n",
            "23/04/13 02:49:13 INFO PythonRunner: Times: total = 2148, boot = 714, init = 23, finish = 1411\n",
            "23/04/13 02:49:13 INFO PythonRunner: Times: total = 1787, boot = 696, init = 71, finish = 1020\n",
            "23/04/13 02:49:13 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 1655 bytes result sent to driver\n",
            "23/04/13 02:49:13 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1612 bytes result sent to driver\n",
            "23/04/13 02:49:13 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 3567 ms on a1ad50929c8d (executor driver) (1/2)\n",
            "23/04/13 02:49:13 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 3641 ms on a1ad50929c8d (executor driver) (2/2)\n",
            "23/04/13 02:49:13 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
            "23/04/13 02:49:13 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 55837\n",
            "23/04/13 02:49:13 INFO DAGScheduler: ShuffleMapStage 0 (reduceByKey at /content/3_days.py:43) finished in 4.641 s\n",
            "23/04/13 02:49:13 INFO DAGScheduler: looking for newly runnable stages\n",
            "23/04/13 02:49:13 INFO DAGScheduler: running: Set()\n",
            "23/04/13 02:49:13 INFO DAGScheduler: waiting: Set(ShuffleMapStage 1, ResultStage 2)\n",
            "23/04/13 02:49:13 INFO DAGScheduler: failed: Set()\n",
            "23/04/13 02:49:14 INFO DAGScheduler: Submitting ShuffleMapStage 1 (MapPartitionsRDD[7] at coalesce at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/04/13 02:49:14 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 10.5 KiB, free 365.9 MiB)\n",
            "23/04/13 02:49:14 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 6.1 KiB, free 365.9 MiB)\n",
            "23/04/13 02:49:14 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on a1ad50929c8d:32843 (size: 6.1 KiB, free: 366.3 MiB)\n",
            "23/04/13 02:49:14 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1478\n",
            "23/04/13 02:49:14 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[7] at coalesce at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
            "23/04/13 02:49:14 INFO TaskSchedulerImpl: Adding task set 1.0 with 2 tasks resource profile 0\n",
            "23/04/13 02:49:14 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 2) (a1ad50929c8d, executor driver, partition 1, NODE_LOCAL, 4260 bytes) taskResourceAssignments Map()\n",
            "23/04/13 02:49:14 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 3) (a1ad50929c8d, executor driver, partition 0, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()\n",
            "23/04/13 02:49:14 INFO Executor: Running task 1.0 in stage 1.0 (TID 2)\n",
            "23/04/13 02:49:14 INFO Executor: Running task 0.0 in stage 1.0 (TID 3)\n",
            "23/04/13 02:49:14 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/13 02:49:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 18 ms\n",
            "23/04/13 02:49:14 INFO ShuffleBlockFetcherIterator: Getting 2 (144.0 B) non-empty blocks including 2 (144.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/13 02:49:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 24 ms\n",
            "23/04/13 02:49:14 INFO PythonRunner: Times: total = 42, boot = -797, init = 839, finish = 0\n",
            "23/04/13 02:49:14 INFO Executor: Finished task 0.0 in stage 1.0 (TID 3). 1654 bytes result sent to driver\n",
            "23/04/13 02:49:14 INFO PythonRunner: Times: total = 51, boot = -469, init = 519, finish = 1\n",
            "23/04/13 02:49:14 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 3) in 216 ms on a1ad50929c8d (executor driver) (1/2)\n",
            "23/04/13 02:49:14 INFO Executor: Finished task 1.0 in stage 1.0 (TID 2). 1783 bytes result sent to driver\n",
            "23/04/13 02:49:14 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 2) in 237 ms on a1ad50929c8d (executor driver) (2/2)\n",
            "23/04/13 02:49:14 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
            "23/04/13 02:49:14 INFO DAGScheduler: ShuffleMapStage 1 (coalesce at NativeMethodAccessorImpl.java:0) finished in 0.284 s\n",
            "23/04/13 02:49:14 INFO DAGScheduler: looking for newly runnable stages\n",
            "23/04/13 02:49:14 INFO DAGScheduler: running: Set()\n",
            "23/04/13 02:49:14 INFO DAGScheduler: waiting: Set(ResultStage 2)\n",
            "23/04/13 02:49:14 INFO DAGScheduler: failed: Set()\n",
            "23/04/13 02:49:14 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[13] at saveAsTextFile at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/04/13 02:49:14 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 105.9 KiB, free 365.8 MiB)\n",
            "23/04/13 02:49:14 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 39.6 KiB, free 365.7 MiB)\n",
            "23/04/13 02:49:14 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on a1ad50929c8d:32843 (size: 39.6 KiB, free: 366.2 MiB)\n",
            "23/04/13 02:49:14 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1478\n",
            "23/04/13 02:49:14 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[13] at saveAsTextFile at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/04/13 02:49:14 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0\n",
            "23/04/13 02:49:14 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 4) (a1ad50929c8d, executor driver, partition 0, NODE_LOCAL, 4547 bytes) taskResourceAssignments Map()\n",
            "23/04/13 02:49:14 INFO Executor: Running task 0.0 in stage 2.0 (TID 4)\n",
            "23/04/13 02:49:14 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
            "23/04/13 02:49:14 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "23/04/13 02:49:14 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "23/04/13 02:49:14 INFO ShuffleBlockFetcherIterator: Getting 1 (72.0 B) non-empty blocks including 1 (72.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/13 02:49:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 10 ms\n",
            "23/04/13 02:49:14 INFO PythonRunner: Times: total = 44, boot = -228, init = 272, finish = 0\n",
            "23/04/13 02:49:14 INFO FileOutputCommitter: Saved output of task 'attempt_20230413024908385675181363433387_0013_m_000000_0' to file:/content/3_days.out/_temporary/0/task_20230413024908385675181363433387_0013_m_000000\n",
            "23/04/13 02:49:14 INFO SparkHadoopMapRedUtil: attempt_20230413024908385675181363433387_0013_m_000000_0: Committed\n",
            "23/04/13 02:49:14 INFO Executor: Finished task 0.0 in stage 2.0 (TID 4). 1952 bytes result sent to driver\n",
            "23/04/13 02:49:14 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 4) in 254 ms on a1ad50929c8d (executor driver) (1/1)\n",
            "23/04/13 02:49:14 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
            "23/04/13 02:49:14 INFO DAGScheduler: ResultStage 2 (runJob at SparkHadoopWriter.scala:83) finished in 0.306 s\n",
            "23/04/13 02:49:14 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "23/04/13 02:49:14 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished\n",
            "23/04/13 02:49:14 INFO DAGScheduler: Job 0 finished: runJob at SparkHadoopWriter.scala:83, took 5.535275 s\n",
            "23/04/13 02:49:14 INFO SparkHadoopWriter: Start to commit write Job job_20230413024908385675181363433387_0013.\n",
            "23/04/13 02:49:14 INFO SparkHadoopWriter: Write Job job_20230413024908385675181363433387_0013 committed. Elapsed time: 20 ms.\n",
            "23/04/13 02:49:14 INFO SparkUI: Stopped Spark web UI at http://a1ad50929c8d:4040\n",
            "23/04/13 02:49:14 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "23/04/13 02:49:14 INFO MemoryStore: MemoryStore cleared\n",
            "23/04/13 02:49:14 INFO BlockManager: BlockManager stopped\n",
            "23/04/13 02:49:14 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "23/04/13 02:49:14 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "23/04/13 02:49:14 INFO SparkContext: Successfully stopped SparkContext\n",
            "23/04/13 02:49:15 INFO ShutdownHookManager: Shutdown hook called\n",
            "23/04/13 02:49:15 INFO ShutdownHookManager: Deleting directory /tmp/spark-86603530-5dc7-46a4-989d-dc34481a9ba9/pyspark-5cf8135d-da3e-4f3c-a9d2-1149c3e2827d\n",
            "23/04/13 02:49:15 INFO ShutdownHookManager: Deleting directory /tmp/spark-13da7e4b-2a83-44a4-9fe7-86866c3cfa5a\n",
            "23/04/13 02:49:15 INFO ShutdownHookManager: Deleting directory /tmp/spark-86603530-5dc7-46a4-989d-dc34481a9ba9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('3_days.out/part-00000') as f:\n",
        "  lines = f.readlines()\n",
        "  print(lines[0].strip())"
      ],
      "metadata": {
        "id": "3NJGhpiIftXw",
        "outputId": "1ae2c64d-34d3-43b7-e39d-6e3192d86011",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('Sun', 6294)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2h8y_YU2__D2",
        "outputId": "76547d45-02c7-483b-a784-31773116dd6d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def test3(lines):\n",
        "    if lines[0].strip() == \"('Sun', 6294)\":\n",
        "        print(\"passed\")\n",
        "    else:\n",
        "        assert False\n",
        "        \n",
        "# test locall execution results\n",
        "with open('3_days.out/part-00000') as f:\n",
        "  lines = f.readlines()\n",
        "  test3(lines)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "passed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "EMR Tests"
      ],
      "metadata": {
        "id": "11h-7fx3yb9I"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9CjkFJcWydoF"
      },
      "source": [
        "# upload script to S3. This assumes that your bucket name is vandy-bigdata. if not then change the  paths here.\n",
        "s3.upload_file(Filename='3_days.py', Bucket='far-bigdata-spark', Key='hw6/3_days.py')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# replae with your EMR cluster ID\n",
        "CLUSTER_ID = 'j-BP8EOODICRFL'\n",
        "\n",
        "def submit_job(app_name, pyfile_uri):\n",
        "    emr = session.client('emr')\n",
        "    emr.add_job_flow_steps(JobFlowId=CLUSTER_ID, Steps=[{\n",
        "        'Name': app_name,\n",
        "        'ActionOnFailure': 'CANCEL_AND_WAIT',\n",
        "        'HadoopJarStep': {\n",
        "            'Args': ['spark-submit',\n",
        "                     '--master', 'yarn',\n",
        "                     '--deploy-mode', 'cluster',\n",
        "                     pyfile_uri],\n",
        "            'Jar': 'command-runner.jar'\n",
        "        }}])"
      ],
      "metadata": {
        "id": "JZm4IjqlydoF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "butU0z3tydoG"
      },
      "source": [
        "# submit spark job to emr. Make all the necessary changes to the path\n",
        "submit_job(app_name='3_days', pyfile_uri='s3://far-bigdata-spark/hw6/3_days.py')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1574cd81-814c-420b-eb90-45ca7f5322d7",
        "id": "3bRi6rylydoG"
      },
      "source": [
        "# test EMR execution results. Once again, make sure that S3 paths are consistent.\n",
        "output_key = \"hw6/3_days.out/part-00000\"\n",
        "lines = s3.get_object(Bucket='far-bigdata-spark', Key=output_key)['Body'].read().decode().splitlines()\n",
        "test3(lines)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "passed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zdiDy-gu__D3"
      },
      "source": [
        "## Job 4. Join the batting and salaries data for Barry Bonds per year."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-MIbbmfk__D3",
        "outputId": "bd3a7b1a-be5a-4caa-f948-43bed163b5d5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%file 4_join.py\n",
        "\n",
        "'''\n",
        "TODO:\n",
        "Join the batting and salaries data for Barry Bonds per year.\n",
        "\n",
        "The output should be the combined CSV string of batting and salaries data (one per year).\n",
        "\n",
        "Final output format:\n",
        "E.g:\n",
        "('join', 'bondsba01,2006,1,SFN,NL,130,367,74,99,23,0,26,77,3,0,115,51,38,10,0,1,92006,SFN,NL,bondsba01,19331470')\n",
        "\n",
        "Schema:\n",
        "Salaries: yearID\tteamID\tlgID\tplayerID\tsalary\n",
        "Batting: playerID\tyearID\tstint\tteamID\tlgID\tG\tAB\tR\tH\t2B\t3B\tHR\tRBI\tSB\tCS\tBB\tSO\n",
        "\n",
        "Hints: \n",
        "Use split to split the CSV lines (e.g. s = line.split(','))\n",
        "Both files are read as text file stream. Use the length of the lines to determine which is which.\n",
        "'''\n",
        "\n",
        "from pyspark import SparkContext, SparkConf\n",
        "\n",
        "def parse_salaries(line):\n",
        "    fields = line.split(',')\n",
        "    return (fields[3], fields)\n",
        "\n",
        "def parse_batting(line):\n",
        "    try:\n",
        "        fields = line.split(',')\n",
        "        return (fields[0], fields)\n",
        "    except IndexError:\n",
        "        return None\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    conf = SparkConf().setAppName('4_join').set('spark.hadoop.validateOutputSpecs', False)\n",
        "    sc = SparkContext(appName=\"4_join\")\n",
        "\n",
        "    try:\n",
        "        # Read both files separately\n",
        "        salaries_data = sc.textFile('s3://far-bigdata-spark/datasets/Salaries.csv')\n",
        "        batting_data = sc.textFile('s3://far-bigdata-spark/datasets/Batting.csv')\n",
        "\n",
        "        # Filter Barry Bonds' records and parse data\n",
        "        salaries_data = salaries_data.filter(lambda line: len(line) > 0 and 'bondsba01' in line).map(parse_salaries)\n",
        "        batting_data = batting_data.filter(lambda line: len(line) > 0 and 'bondsba01' in line).map(parse_batting).filter(lambda x: x is not None)\n",
        "\n",
        "        # Join data by playerID\n",
        "        joined_data = batting_data.join(salaries_data)\n",
        "\n",
        "        # Format the output\n",
        "        formatted_output = joined_data.map(lambda x: ('join', ','.join(x[1][0][:24] + x[1][1])))\n",
        "\n",
        "        # Save the output\n",
        "        formatted_output.repartition(1).saveAsTextFile(\"s3://far-bigdata-spark/hw6/4_join.out\")\n",
        "\n",
        "    finally:\n",
        "        sc.stop()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting 4_join.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n5_a0GAy__D3",
        "outputId": "ada662c6-d3f6-4cfe-f2a7-eb15b99b277a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# execute locally\n",
        "!spark-submit --packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.4.7 4_join.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ":: loading settings :: url = jar:file:/content/spark-3.2.3-bin-hadoop3.2/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
            "Ivy Default Cache set to: /root/.ivy2/cache\n",
            "The jars for the packages stored in: /root/.ivy2/jars\n",
            "org.apache.spark#spark-streaming-kafka-0-8_2.11 added as a dependency\n",
            ":: resolving dependencies :: org.apache.spark#spark-submit-parent-335a4ff8-275c-4b47-bd3d-bddb5d575e3e;1.0\n",
            "\tconfs: [default]\n",
            "\tfound org.apache.spark#spark-streaming-kafka-0-8_2.11;2.4.7 in central\n",
            "\tfound org.apache.kafka#kafka_2.11;0.8.2.1 in central\n",
            "\tfound org.scala-lang.modules#scala-xml_2.11;1.0.2 in central\n",
            "\tfound com.yammer.metrics#metrics-core;2.2.0 in central\n",
            "\tfound org.slf4j#slf4j-api;1.7.16 in central\n",
            "\tfound org.scala-lang.modules#scala-parser-combinators_2.11;1.1.0 in central\n",
            "\tfound com.101tec#zkclient;0.3 in central\n",
            "\tfound log4j#log4j;1.2.17 in central\n",
            "\tfound org.apache.kafka#kafka-clients;0.8.2.1 in central\n",
            "\tfound net.jpountz.lz4#lz4;1.2.0 in central\n",
            "\tfound org.xerial.snappy#snappy-java;1.1.7.5 in central\n",
            "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
            ":: resolution report :: resolve 845ms :: artifacts dl 21ms\n",
            "\t:: modules in use:\n",
            "\tcom.101tec#zkclient;0.3 from central in [default]\n",
            "\tcom.yammer.metrics#metrics-core;2.2.0 from central in [default]\n",
            "\tlog4j#log4j;1.2.17 from central in [default]\n",
            "\tnet.jpountz.lz4#lz4;1.2.0 from central in [default]\n",
            "\torg.apache.kafka#kafka-clients;0.8.2.1 from central in [default]\n",
            "\torg.apache.kafka#kafka_2.11;0.8.2.1 from central in [default]\n",
            "\torg.apache.spark#spark-streaming-kafka-0-8_2.11;2.4.7 from central in [default]\n",
            "\torg.scala-lang.modules#scala-parser-combinators_2.11;1.1.0 from central in [default]\n",
            "\torg.scala-lang.modules#scala-xml_2.11;1.0.2 from central in [default]\n",
            "\torg.slf4j#slf4j-api;1.7.16 from central in [default]\n",
            "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
            "\torg.xerial.snappy#snappy-java;1.1.7.5 from central in [default]\n",
            "\t---------------------------------------------------------------------\n",
            "\t|                  |            modules            ||   artifacts   |\n",
            "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
            "\t---------------------------------------------------------------------\n",
            "\t|      default     |   12  |   0   |   0   |   0   ||   12  |   0   |\n",
            "\t---------------------------------------------------------------------\n",
            ":: retrieving :: org.apache.spark#spark-submit-parent-335a4ff8-275c-4b47-bd3d-bddb5d575e3e\n",
            "\tconfs: [default]\n",
            "\t0 artifacts copied, 12 already retrieved (0kB/21ms)\n",
            "23/04/13 03:46:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
            "23/04/13 03:46:14 INFO SparkContext: Running Spark version 3.2.3\n",
            "23/04/13 03:46:14 INFO ResourceUtils: ==============================================================\n",
            "23/04/13 03:46:14 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "23/04/13 03:46:14 INFO ResourceUtils: ==============================================================\n",
            "23/04/13 03:46:14 INFO SparkContext: Submitted application: 4_join\n",
            "23/04/13 03:46:14 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "23/04/13 03:46:14 INFO ResourceProfile: Limiting resource is cpu\n",
            "23/04/13 03:46:14 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "23/04/13 03:46:14 INFO SecurityManager: Changing view acls to: root\n",
            "23/04/13 03:46:14 INFO SecurityManager: Changing modify acls to: root\n",
            "23/04/13 03:46:14 INFO SecurityManager: Changing view acls groups to: \n",
            "23/04/13 03:46:14 INFO SecurityManager: Changing modify acls groups to: \n",
            "23/04/13 03:46:14 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\n",
            "23/04/13 03:46:14 INFO Utils: Successfully started service 'sparkDriver' on port 36213.\n",
            "23/04/13 03:46:14 INFO SparkEnv: Registering MapOutputTracker\n",
            "23/04/13 03:46:14 INFO SparkEnv: Registering BlockManagerMaster\n",
            "23/04/13 03:46:14 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "23/04/13 03:46:14 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "23/04/13 03:46:14 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "23/04/13 03:46:15 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-4fe5db76-fb80-4985-ab50-0ae7a79fc0ce\n",
            "23/04/13 03:46:15 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB\n",
            "23/04/13 03:46:15 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "23/04/13 03:46:15 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
            "23/04/13 03:46:15 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://a1ad50929c8d:4040\n",
            "23/04/13 03:46:15 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar at spark://a1ad50929c8d:36213/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1681357574081\n",
            "23/04/13 03:46:15 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar at spark://a1ad50929c8d:36213/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1681357574081\n",
            "23/04/13 03:46:15 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar at spark://a1ad50929c8d:36213/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1681357574081\n",
            "23/04/13 03:46:15 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar at spark://a1ad50929c8d:36213/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1681357574081\n",
            "23/04/13 03:46:15 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar at spark://a1ad50929c8d:36213/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1681357574081\n",
            "23/04/13 03:46:15 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar at spark://a1ad50929c8d:36213/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1681357574081\n",
            "23/04/13 03:46:15 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar at spark://a1ad50929c8d:36213/jars/com.101tec_zkclient-0.3.jar with timestamp 1681357574081\n",
            "23/04/13 03:46:15 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar at spark://a1ad50929c8d:36213/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1681357574081\n",
            "23/04/13 03:46:15 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar at spark://a1ad50929c8d:36213/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1681357574081\n",
            "23/04/13 03:46:15 INFO SparkContext: Added JAR file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar at spark://a1ad50929c8d:36213/jars/log4j_log4j-1.2.17.jar with timestamp 1681357574081\n",
            "23/04/13 03:46:15 INFO SparkContext: Added JAR file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar at spark://a1ad50929c8d:36213/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1681357574081\n",
            "23/04/13 03:46:15 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar at spark://a1ad50929c8d:36213/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1681357574081\n",
            "23/04/13 03:46:15 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar at file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1681357574081\n",
            "23/04/13 03:46:15 INFO Utils: Copying /root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar to /tmp/spark-842cf94c-5b53-4b29-9fef-6b431c2cfba7/userFiles-813d30bf-aa01-4a10-91ac-790d27ef3c76/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar\n",
            "23/04/13 03:46:15 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar at file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1681357574081\n",
            "23/04/13 03:46:15 INFO Utils: Copying /root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar to /tmp/spark-842cf94c-5b53-4b29-9fef-6b431c2cfba7/userFiles-813d30bf-aa01-4a10-91ac-790d27ef3c76/org.apache.kafka_kafka_2.11-0.8.2.1.jar\n",
            "23/04/13 03:46:15 INFO SparkContext: Added file file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar at file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1681357574081\n",
            "23/04/13 03:46:15 INFO Utils: Copying /root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-842cf94c-5b53-4b29-9fef-6b431c2cfba7/userFiles-813d30bf-aa01-4a10-91ac-790d27ef3c76/org.spark-project.spark_unused-1.0.0.jar\n",
            "23/04/13 03:46:15 INFO SparkContext: Added file file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar at file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1681357574081\n",
            "23/04/13 03:46:15 INFO Utils: Copying /root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar to /tmp/spark-842cf94c-5b53-4b29-9fef-6b431c2cfba7/userFiles-813d30bf-aa01-4a10-91ac-790d27ef3c76/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar\n",
            "23/04/13 03:46:15 INFO SparkContext: Added file file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar at file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1681357574081\n",
            "23/04/13 03:46:15 INFO Utils: Copying /root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar to /tmp/spark-842cf94c-5b53-4b29-9fef-6b431c2cfba7/userFiles-813d30bf-aa01-4a10-91ac-790d27ef3c76/com.yammer.metrics_metrics-core-2.2.0.jar\n",
            "23/04/13 03:46:15 INFO SparkContext: Added file file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar at file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1681357574081\n",
            "23/04/13 03:46:15 INFO Utils: Copying /root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar to /tmp/spark-842cf94c-5b53-4b29-9fef-6b431c2cfba7/userFiles-813d30bf-aa01-4a10-91ac-790d27ef3c76/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar\n",
            "23/04/13 03:46:15 INFO SparkContext: Added file file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar at file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar with timestamp 1681357574081\n",
            "23/04/13 03:46:15 INFO Utils: Copying /root/.ivy2/jars/com.101tec_zkclient-0.3.jar to /tmp/spark-842cf94c-5b53-4b29-9fef-6b431c2cfba7/userFiles-813d30bf-aa01-4a10-91ac-790d27ef3c76/com.101tec_zkclient-0.3.jar\n",
            "23/04/13 03:46:15 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar at file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1681357574081\n",
            "23/04/13 03:46:15 INFO Utils: Copying /root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar to /tmp/spark-842cf94c-5b53-4b29-9fef-6b431c2cfba7/userFiles-813d30bf-aa01-4a10-91ac-790d27ef3c76/org.apache.kafka_kafka-clients-0.8.2.1.jar\n",
            "23/04/13 03:46:15 INFO SparkContext: Added file file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar at file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1681357574081\n",
            "23/04/13 03:46:15 INFO Utils: Copying /root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar to /tmp/spark-842cf94c-5b53-4b29-9fef-6b431c2cfba7/userFiles-813d30bf-aa01-4a10-91ac-790d27ef3c76/org.slf4j_slf4j-api-1.7.16.jar\n",
            "23/04/13 03:46:15 INFO SparkContext: Added file file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar at file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar with timestamp 1681357574081\n",
            "23/04/13 03:46:15 INFO Utils: Copying /root/.ivy2/jars/log4j_log4j-1.2.17.jar to /tmp/spark-842cf94c-5b53-4b29-9fef-6b431c2cfba7/userFiles-813d30bf-aa01-4a10-91ac-790d27ef3c76/log4j_log4j-1.2.17.jar\n",
            "23/04/13 03:46:15 INFO SparkContext: Added file file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar at file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1681357574081\n",
            "23/04/13 03:46:15 INFO Utils: Copying /root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar to /tmp/spark-842cf94c-5b53-4b29-9fef-6b431c2cfba7/userFiles-813d30bf-aa01-4a10-91ac-790d27ef3c76/net.jpountz.lz4_lz4-1.2.0.jar\n",
            "23/04/13 03:46:15 INFO SparkContext: Added file file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar at file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1681357574081\n",
            "23/04/13 03:46:15 INFO Utils: Copying /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar to /tmp/spark-842cf94c-5b53-4b29-9fef-6b431c2cfba7/userFiles-813d30bf-aa01-4a10-91ac-790d27ef3c76/org.xerial.snappy_snappy-java-1.1.7.5.jar\n",
            "23/04/13 03:46:16 INFO Executor: Starting executor ID driver on host a1ad50929c8d\n",
            "23/04/13 03:46:16 INFO Executor: Fetching file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar with timestamp 1681357574081\n",
            "23/04/13 03:46:16 INFO Utils: /root/.ivy2/jars/log4j_log4j-1.2.17.jar has been previously copied to /tmp/spark-842cf94c-5b53-4b29-9fef-6b431c2cfba7/userFiles-813d30bf-aa01-4a10-91ac-790d27ef3c76/log4j_log4j-1.2.17.jar\n",
            "23/04/13 03:46:16 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1681357574081\n",
            "23/04/13 03:46:16 INFO Utils: /root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar has been previously copied to /tmp/spark-842cf94c-5b53-4b29-9fef-6b431c2cfba7/userFiles-813d30bf-aa01-4a10-91ac-790d27ef3c76/org.apache.kafka_kafka-clients-0.8.2.1.jar\n",
            "23/04/13 03:46:16 INFO Executor: Fetching file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1681357574081\n",
            "23/04/13 03:46:16 INFO Utils: /root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar has been previously copied to /tmp/spark-842cf94c-5b53-4b29-9fef-6b431c2cfba7/userFiles-813d30bf-aa01-4a10-91ac-790d27ef3c76/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar\n",
            "23/04/13 03:46:16 INFO Executor: Fetching file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1681357574081\n",
            "23/04/13 03:46:16 INFO Utils: /root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar has been previously copied to /tmp/spark-842cf94c-5b53-4b29-9fef-6b431c2cfba7/userFiles-813d30bf-aa01-4a10-91ac-790d27ef3c76/org.slf4j_slf4j-api-1.7.16.jar\n",
            "23/04/13 03:46:16 INFO Executor: Fetching file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1681357574081\n",
            "23/04/13 03:46:16 INFO Utils: /root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar has been previously copied to /tmp/spark-842cf94c-5b53-4b29-9fef-6b431c2cfba7/userFiles-813d30bf-aa01-4a10-91ac-790d27ef3c76/net.jpountz.lz4_lz4-1.2.0.jar\n",
            "23/04/13 03:46:16 INFO Executor: Fetching file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1681357574081\n",
            "23/04/13 03:46:16 INFO Utils: /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar has been previously copied to /tmp/spark-842cf94c-5b53-4b29-9fef-6b431c2cfba7/userFiles-813d30bf-aa01-4a10-91ac-790d27ef3c76/org.xerial.snappy_snappy-java-1.1.7.5.jar\n",
            "23/04/13 03:46:16 INFO Executor: Fetching file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1681357574081\n",
            "23/04/13 03:46:16 INFO Utils: /root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar has been previously copied to /tmp/spark-842cf94c-5b53-4b29-9fef-6b431c2cfba7/userFiles-813d30bf-aa01-4a10-91ac-790d27ef3c76/org.spark-project.spark_unused-1.0.0.jar\n",
            "23/04/13 03:46:16 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1681357574081\n",
            "23/04/13 03:46:16 INFO Utils: /root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar has been previously copied to /tmp/spark-842cf94c-5b53-4b29-9fef-6b431c2cfba7/userFiles-813d30bf-aa01-4a10-91ac-790d27ef3c76/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar\n",
            "23/04/13 03:46:16 INFO Executor: Fetching file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1681357574081\n",
            "23/04/13 03:46:16 INFO Utils: /root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar has been previously copied to /tmp/spark-842cf94c-5b53-4b29-9fef-6b431c2cfba7/userFiles-813d30bf-aa01-4a10-91ac-790d27ef3c76/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar\n",
            "23/04/13 03:46:16 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1681357574081\n",
            "23/04/13 03:46:16 INFO Utils: /root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar has been previously copied to /tmp/spark-842cf94c-5b53-4b29-9fef-6b431c2cfba7/userFiles-813d30bf-aa01-4a10-91ac-790d27ef3c76/org.apache.kafka_kafka_2.11-0.8.2.1.jar\n",
            "23/04/13 03:46:16 INFO Executor: Fetching file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar with timestamp 1681357574081\n",
            "23/04/13 03:46:16 INFO Utils: /root/.ivy2/jars/com.101tec_zkclient-0.3.jar has been previously copied to /tmp/spark-842cf94c-5b53-4b29-9fef-6b431c2cfba7/userFiles-813d30bf-aa01-4a10-91ac-790d27ef3c76/com.101tec_zkclient-0.3.jar\n",
            "23/04/13 03:46:16 INFO Executor: Fetching file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1681357574081\n",
            "23/04/13 03:46:16 INFO Utils: /root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar has been previously copied to /tmp/spark-842cf94c-5b53-4b29-9fef-6b431c2cfba7/userFiles-813d30bf-aa01-4a10-91ac-790d27ef3c76/com.yammer.metrics_metrics-core-2.2.0.jar\n",
            "23/04/13 03:46:16 INFO Executor: Fetching spark://a1ad50929c8d:36213/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1681357574081\n",
            "23/04/13 03:46:16 INFO TransportClientFactory: Successfully created connection to a1ad50929c8d/172.28.0.12:36213 after 74 ms (0 ms spent in bootstraps)\n",
            "23/04/13 03:46:16 INFO Utils: Fetching spark://a1ad50929c8d:36213/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar to /tmp/spark-842cf94c-5b53-4b29-9fef-6b431c2cfba7/userFiles-813d30bf-aa01-4a10-91ac-790d27ef3c76/fetchFileTemp5962411399113287448.tmp\n",
            "23/04/13 03:46:16 INFO Utils: /tmp/spark-842cf94c-5b53-4b29-9fef-6b431c2cfba7/userFiles-813d30bf-aa01-4a10-91ac-790d27ef3c76/fetchFileTemp5962411399113287448.tmp has been previously copied to /tmp/spark-842cf94c-5b53-4b29-9fef-6b431c2cfba7/userFiles-813d30bf-aa01-4a10-91ac-790d27ef3c76/org.apache.kafka_kafka_2.11-0.8.2.1.jar\n",
            "23/04/13 03:46:16 INFO Executor: Adding file:/tmp/spark-842cf94c-5b53-4b29-9fef-6b431c2cfba7/userFiles-813d30bf-aa01-4a10-91ac-790d27ef3c76/org.apache.kafka_kafka_2.11-0.8.2.1.jar to class loader\n",
            "23/04/13 03:46:16 INFO Executor: Fetching spark://a1ad50929c8d:36213/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1681357574081\n",
            "23/04/13 03:46:16 INFO Utils: Fetching spark://a1ad50929c8d:36213/jars/com.yammer.metrics_metrics-core-2.2.0.jar to /tmp/spark-842cf94c-5b53-4b29-9fef-6b431c2cfba7/userFiles-813d30bf-aa01-4a10-91ac-790d27ef3c76/fetchFileTemp6666671925221871081.tmp\n",
            "23/04/13 03:46:16 INFO Utils: /tmp/spark-842cf94c-5b53-4b29-9fef-6b431c2cfba7/userFiles-813d30bf-aa01-4a10-91ac-790d27ef3c76/fetchFileTemp6666671925221871081.tmp has been previously copied to /tmp/spark-842cf94c-5b53-4b29-9fef-6b431c2cfba7/userFiles-813d30bf-aa01-4a10-91ac-790d27ef3c76/com.yammer.metrics_metrics-core-2.2.0.jar\n",
            "23/04/13 03:46:16 INFO Executor: Adding file:/tmp/spark-842cf94c-5b53-4b29-9fef-6b431c2cfba7/userFiles-813d30bf-aa01-4a10-91ac-790d27ef3c76/com.yammer.metrics_metrics-core-2.2.0.jar to class loader\n",
            "23/04/13 03:46:16 INFO Executor: Fetching spark://a1ad50929c8d:36213/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1681357574081\n",
            "23/04/13 03:46:16 INFO Utils: Fetching spark://a1ad50929c8d:36213/jars/net.jpountz.lz4_lz4-1.2.0.jar to /tmp/spark-842cf94c-5b53-4b29-9fef-6b431c2cfba7/userFiles-813d30bf-aa01-4a10-91ac-790d27ef3c76/fetchFileTemp7305235113103031116.tmp\n",
            "23/04/13 03:46:16 INFO Utils: /tmp/spark-842cf94c-5b53-4b29-9fef-6b431c2cfba7/userFiles-813d30bf-aa01-4a10-91ac-790d27ef3c76/fetchFileTemp7305235113103031116.tmp has been previously copied to /tmp/spark-842cf94c-5b53-4b29-9fef-6b431c2cfba7/userFiles-813d30bf-aa01-4a10-91ac-790d27ef3c76/net.jpountz.lz4_lz4-1.2.0.jar\n",
            "23/04/13 03:46:16 INFO Executor: Adding file:/tmp/spark-842cf94c-5b53-4b29-9fef-6b431c2cfba7/userFiles-813d30bf-aa01-4a10-91ac-790d27ef3c76/net.jpountz.lz4_lz4-1.2.0.jar to class loader\n",
            "23/04/13 03:46:16 INFO Executor: Fetching spark://a1ad50929c8d:36213/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1681357574081\n",
            "23/04/13 03:46:16 INFO Utils: Fetching spark://a1ad50929c8d:36213/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar to /tmp/spark-842cf94c-5b53-4b29-9fef-6b431c2cfba7/userFiles-813d30bf-aa01-4a10-91ac-790d27ef3c76/fetchFileTemp8980016138782461853.tmp\n",
            "23/04/13 03:46:16 INFO Utils: /tmp/spark-842cf94c-5b53-4b29-9fef-6b431c2cfba7/userFiles-813d30bf-aa01-4a10-91ac-790d27ef3c76/fetchFileTemp8980016138782461853.tmp has been previously copied to /tmp/spark-842cf94c-5b53-4b29-9fef-6b431c2cfba7/userFiles-813d30bf-aa01-4a10-91ac-790d27ef3c76/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar\n",
            "23/04/13 03:46:16 INFO Executor: Adding file:/tmp/spark-842cf94c-5b53-4b29-9fef-6b431c2cfba7/userFiles-813d30bf-aa01-4a10-91ac-790d27ef3c76/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar to class loader\n",
            "23/04/13 03:46:16 INFO Executor: Fetching spark://a1ad50929c8d:36213/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1681357574081\n",
            "23/04/13 03:46:16 INFO Utils: Fetching spark://a1ad50929c8d:36213/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar to /tmp/spark-842cf94c-5b53-4b29-9fef-6b431c2cfba7/userFiles-813d30bf-aa01-4a10-91ac-790d27ef3c76/fetchFileTemp3777057394737241584.tmp\n",
            "23/04/13 03:46:16 INFO Utils: /tmp/spark-842cf94c-5b53-4b29-9fef-6b431c2cfba7/userFiles-813d30bf-aa01-4a10-91ac-790d27ef3c76/fetchFileTemp3777057394737241584.tmp has been previously copied to /tmp/spark-842cf94c-5b53-4b29-9fef-6b431c2cfba7/userFiles-813d30bf-aa01-4a10-91ac-790d27ef3c76/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar\n",
            "23/04/13 03:46:16 INFO Executor: Adding file:/tmp/spark-842cf94c-5b53-4b29-9fef-6b431c2cfba7/userFiles-813d30bf-aa01-4a10-91ac-790d27ef3c76/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar to class loader\n",
            "23/04/13 03:46:16 INFO Executor: Fetching spark://a1ad50929c8d:36213/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1681357574081\n",
            "23/04/13 03:46:16 INFO Utils: Fetching spark://a1ad50929c8d:36213/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-842cf94c-5b53-4b29-9fef-6b431c2cfba7/userFiles-813d30bf-aa01-4a10-91ac-790d27ef3c76/fetchFileTemp2493113053082860604.tmp\n",
            "23/04/13 03:46:16 INFO Utils: /tmp/spark-842cf94c-5b53-4b29-9fef-6b431c2cfba7/userFiles-813d30bf-aa01-4a10-91ac-790d27ef3c76/fetchFileTemp2493113053082860604.tmp has been previously copied to /tmp/spark-842cf94c-5b53-4b29-9fef-6b431c2cfba7/userFiles-813d30bf-aa01-4a10-91ac-790d27ef3c76/org.spark-project.spark_unused-1.0.0.jar\n",
            "23/04/13 03:46:16 INFO Executor: Adding file:/tmp/spark-842cf94c-5b53-4b29-9fef-6b431c2cfba7/userFiles-813d30bf-aa01-4a10-91ac-790d27ef3c76/org.spark-project.spark_unused-1.0.0.jar to class loader\n",
            "23/04/13 03:46:16 INFO Executor: Fetching spark://a1ad50929c8d:36213/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1681357574081\n",
            "23/04/13 03:46:16 INFO Utils: Fetching spark://a1ad50929c8d:36213/jars/org.slf4j_slf4j-api-1.7.16.jar to /tmp/spark-842cf94c-5b53-4b29-9fef-6b431c2cfba7/userFiles-813d30bf-aa01-4a10-91ac-790d27ef3c76/fetchFileTemp4605585052595041919.tmp\n",
            "23/04/13 03:46:16 INFO Utils: /tmp/spark-842cf94c-5b53-4b29-9fef-6b431c2cfba7/userFiles-813d30bf-aa01-4a10-91ac-790d27ef3c76/fetchFileTemp4605585052595041919.tmp has been previously copied to /tmp/spark-842cf94c-5b53-4b29-9fef-6b431c2cfba7/userFiles-813d30bf-aa01-4a10-91ac-790d27ef3c76/org.slf4j_slf4j-api-1.7.16.jar\n",
            "23/04/13 03:46:16 INFO Executor: Adding file:/tmp/spark-842cf94c-5b53-4b29-9fef-6b431c2cfba7/userFiles-813d30bf-aa01-4a10-91ac-790d27ef3c76/org.slf4j_slf4j-api-1.7.16.jar to class loader\n",
            "23/04/13 03:46:16 INFO Executor: Fetching spark://a1ad50929c8d:36213/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1681357574081\n",
            "23/04/13 03:46:16 INFO Utils: Fetching spark://a1ad50929c8d:36213/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar to /tmp/spark-842cf94c-5b53-4b29-9fef-6b431c2cfba7/userFiles-813d30bf-aa01-4a10-91ac-790d27ef3c76/fetchFileTemp6210576864053259341.tmp\n",
            "23/04/13 03:46:16 INFO Utils: /tmp/spark-842cf94c-5b53-4b29-9fef-6b431c2cfba7/userFiles-813d30bf-aa01-4a10-91ac-790d27ef3c76/fetchFileTemp6210576864053259341.tmp has been previously copied to /tmp/spark-842cf94c-5b53-4b29-9fef-6b431c2cfba7/userFiles-813d30bf-aa01-4a10-91ac-790d27ef3c76/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar\n",
            "23/04/13 03:46:16 INFO Executor: Adding file:/tmp/spark-842cf94c-5b53-4b29-9fef-6b431c2cfba7/userFiles-813d30bf-aa01-4a10-91ac-790d27ef3c76/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar to class loader\n",
            "23/04/13 03:46:16 INFO Executor: Fetching spark://a1ad50929c8d:36213/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1681357574081\n",
            "23/04/13 03:46:16 INFO Utils: Fetching spark://a1ad50929c8d:36213/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar to /tmp/spark-842cf94c-5b53-4b29-9fef-6b431c2cfba7/userFiles-813d30bf-aa01-4a10-91ac-790d27ef3c76/fetchFileTemp7298175869724414271.tmp\n",
            "23/04/13 03:46:16 INFO Utils: /tmp/spark-842cf94c-5b53-4b29-9fef-6b431c2cfba7/userFiles-813d30bf-aa01-4a10-91ac-790d27ef3c76/fetchFileTemp7298175869724414271.tmp has been previously copied to /tmp/spark-842cf94c-5b53-4b29-9fef-6b431c2cfba7/userFiles-813d30bf-aa01-4a10-91ac-790d27ef3c76/org.xerial.snappy_snappy-java-1.1.7.5.jar\n",
            "23/04/13 03:46:16 INFO Executor: Adding file:/tmp/spark-842cf94c-5b53-4b29-9fef-6b431c2cfba7/userFiles-813d30bf-aa01-4a10-91ac-790d27ef3c76/org.xerial.snappy_snappy-java-1.1.7.5.jar to class loader\n",
            "23/04/13 03:46:16 INFO Executor: Fetching spark://a1ad50929c8d:36213/jars/com.101tec_zkclient-0.3.jar with timestamp 1681357574081\n",
            "23/04/13 03:46:16 INFO Utils: Fetching spark://a1ad50929c8d:36213/jars/com.101tec_zkclient-0.3.jar to /tmp/spark-842cf94c-5b53-4b29-9fef-6b431c2cfba7/userFiles-813d30bf-aa01-4a10-91ac-790d27ef3c76/fetchFileTemp7700662113658847452.tmp\n",
            "23/04/13 03:46:16 INFO Utils: /tmp/spark-842cf94c-5b53-4b29-9fef-6b431c2cfba7/userFiles-813d30bf-aa01-4a10-91ac-790d27ef3c76/fetchFileTemp7700662113658847452.tmp has been previously copied to /tmp/spark-842cf94c-5b53-4b29-9fef-6b431c2cfba7/userFiles-813d30bf-aa01-4a10-91ac-790d27ef3c76/com.101tec_zkclient-0.3.jar\n",
            "23/04/13 03:46:16 INFO Executor: Adding file:/tmp/spark-842cf94c-5b53-4b29-9fef-6b431c2cfba7/userFiles-813d30bf-aa01-4a10-91ac-790d27ef3c76/com.101tec_zkclient-0.3.jar to class loader\n",
            "23/04/13 03:46:16 INFO Executor: Fetching spark://a1ad50929c8d:36213/jars/log4j_log4j-1.2.17.jar with timestamp 1681357574081\n",
            "23/04/13 03:46:16 INFO Utils: Fetching spark://a1ad50929c8d:36213/jars/log4j_log4j-1.2.17.jar to /tmp/spark-842cf94c-5b53-4b29-9fef-6b431c2cfba7/userFiles-813d30bf-aa01-4a10-91ac-790d27ef3c76/fetchFileTemp6167145068193537574.tmp\n",
            "23/04/13 03:46:16 INFO Utils: /tmp/spark-842cf94c-5b53-4b29-9fef-6b431c2cfba7/userFiles-813d30bf-aa01-4a10-91ac-790d27ef3c76/fetchFileTemp6167145068193537574.tmp has been previously copied to /tmp/spark-842cf94c-5b53-4b29-9fef-6b431c2cfba7/userFiles-813d30bf-aa01-4a10-91ac-790d27ef3c76/log4j_log4j-1.2.17.jar\n",
            "23/04/13 03:46:16 INFO Executor: Adding file:/tmp/spark-842cf94c-5b53-4b29-9fef-6b431c2cfba7/userFiles-813d30bf-aa01-4a10-91ac-790d27ef3c76/log4j_log4j-1.2.17.jar to class loader\n",
            "23/04/13 03:46:16 INFO Executor: Fetching spark://a1ad50929c8d:36213/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1681357574081\n",
            "23/04/13 03:46:16 INFO Utils: Fetching spark://a1ad50929c8d:36213/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar to /tmp/spark-842cf94c-5b53-4b29-9fef-6b431c2cfba7/userFiles-813d30bf-aa01-4a10-91ac-790d27ef3c76/fetchFileTemp2092228416606404695.tmp\n",
            "23/04/13 03:46:16 INFO Utils: /tmp/spark-842cf94c-5b53-4b29-9fef-6b431c2cfba7/userFiles-813d30bf-aa01-4a10-91ac-790d27ef3c76/fetchFileTemp2092228416606404695.tmp has been previously copied to /tmp/spark-842cf94c-5b53-4b29-9fef-6b431c2cfba7/userFiles-813d30bf-aa01-4a10-91ac-790d27ef3c76/org.apache.kafka_kafka-clients-0.8.2.1.jar\n",
            "23/04/13 03:46:16 INFO Executor: Adding file:/tmp/spark-842cf94c-5b53-4b29-9fef-6b431c2cfba7/userFiles-813d30bf-aa01-4a10-91ac-790d27ef3c76/org.apache.kafka_kafka-clients-0.8.2.1.jar to class loader\n",
            "23/04/13 03:46:16 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42713.\n",
            "23/04/13 03:46:16 INFO NettyBlockTransferService: Server created on a1ad50929c8d:42713\n",
            "23/04/13 03:46:16 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "23/04/13 03:46:16 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, a1ad50929c8d, 42713, None)\n",
            "23/04/13 03:46:16 INFO BlockManagerMasterEndpoint: Registering block manager a1ad50929c8d:42713 with 366.3 MiB RAM, BlockManagerId(driver, a1ad50929c8d, 42713, None)\n",
            "23/04/13 03:46:16 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, a1ad50929c8d, 42713, None)\n",
            "23/04/13 03:46:16 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, a1ad50929c8d, 42713, None)\n",
            "23/04/13 03:46:18 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 354.3 KiB, free 366.0 MiB)\n",
            "23/04/13 03:46:18 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 32.0 KiB, free 365.9 MiB)\n",
            "23/04/13 03:46:18 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on a1ad50929c8d:42713 (size: 32.0 KiB, free: 366.3 MiB)\n",
            "23/04/13 03:46:18 INFO SparkContext: Created broadcast 0 from textFile at NativeMethodAccessorImpl.java:0\n",
            "23/04/13 03:46:19 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 354.3 KiB, free 365.6 MiB)\n",
            "23/04/13 03:46:19 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 32.0 KiB, free 365.5 MiB)\n",
            "23/04/13 03:46:19 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on a1ad50929c8d:42713 (size: 32.0 KiB, free: 366.2 MiB)\n",
            "23/04/13 03:46:19 INFO SparkContext: Created broadcast 1 from textFile at NativeMethodAccessorImpl.java:0\n",
            "23/04/13 03:46:19 INFO FileInputFormat: Total input files to process : 1\n",
            "23/04/13 03:46:19 INFO FileInputFormat: Total input files to process : 1\n",
            "23/04/13 03:46:19 INFO deprecation: mapred.output.dir is deprecated. Instead, use mapreduce.output.fileoutputformat.outputdir\n",
            "23/04/13 03:46:19 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
            "23/04/13 03:46:19 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "23/04/13 03:46:19 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "23/04/13 03:46:20 INFO SparkContext: Starting job: runJob at SparkHadoopWriter.scala:83\n",
            "23/04/13 03:46:20 INFO DAGScheduler: Registering RDD 8 (join at /content/4_join.py:48) as input to shuffle 1\n",
            "23/04/13 03:46:20 INFO DAGScheduler: Registering RDD 12 (coalesce at NativeMethodAccessorImpl.java:0) as input to shuffle 0\n",
            "23/04/13 03:46:20 INFO DAGScheduler: Got job 0 (runJob at SparkHadoopWriter.scala:83) with 1 output partitions\n",
            "23/04/13 03:46:20 INFO DAGScheduler: Final stage: ResultStage 2 (runJob at SparkHadoopWriter.scala:83)\n",
            "23/04/13 03:46:20 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 1)\n",
            "23/04/13 03:46:20 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 1)\n",
            "23/04/13 03:46:20 INFO DAGScheduler: Submitting ShuffleMapStage 0 (PairwiseRDD[8] at join at /content/4_join.py:48), which has no missing parents\n",
            "23/04/13 03:46:20 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 19.0 KiB, free 365.5 MiB)\n",
            "23/04/13 03:46:21 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 8.7 KiB, free 365.5 MiB)\n",
            "23/04/13 03:46:21 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on a1ad50929c8d:42713 (size: 8.7 KiB, free: 366.2 MiB)\n",
            "23/04/13 03:46:21 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1478\n",
            "23/04/13 03:46:21 INFO DAGScheduler: Submitting 4 missing tasks from ShuffleMapStage 0 (PairwiseRDD[8] at join at /content/4_join.py:48) (first 15 tasks are for partitions Vector(0, 1, 2, 3))\n",
            "23/04/13 03:46:21 INFO TaskSchedulerImpl: Adding task set 0.0 with 4 tasks resource profile 0\n",
            "23/04/13 03:46:21 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (a1ad50929c8d, executor driver, partition 0, PROCESS_LOCAL, 4592 bytes) taskResourceAssignments Map()\n",
            "23/04/13 03:46:21 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1) (a1ad50929c8d, executor driver, partition 1, PROCESS_LOCAL, 4592 bytes) taskResourceAssignments Map()\n",
            "23/04/13 03:46:21 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)\n",
            "23/04/13 03:46:21 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
            "23/04/13 03:46:21 INFO HadoopRDD: Input split: file:/content/Batting.csv:0+3244373\n",
            "23/04/13 03:46:21 INFO HadoopRDD: Input split: file:/content/Batting.csv:3244373+3244374\n",
            "23/04/13 03:46:22 INFO PythonRunner: Times: total = 1015, boot = 623, init = 69, finish = 323\n",
            "23/04/13 03:46:22 INFO PythonRunner: Times: total = 394, boot = 30, init = 363, finish = 1\n",
            "23/04/13 03:46:22 INFO PythonRunner: Times: total = 1107, boot = 622, init = 58, finish = 427\n",
            "23/04/13 03:46:22 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 1528 bytes result sent to driver\n",
            "23/04/13 03:46:22 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2) (a1ad50929c8d, executor driver, partition 2, PROCESS_LOCAL, 4593 bytes) taskResourceAssignments Map()\n",
            "23/04/13 03:46:22 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)\n",
            "23/04/13 03:46:23 INFO HadoopRDD: Input split: file:/content/Salaries.csv:0+350012\n",
            "23/04/13 03:46:23 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 1845 ms on a1ad50929c8d (executor driver) (1/4)\n",
            "23/04/13 03:46:23 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 51953\n",
            "23/04/13 03:46:23 INFO PythonRunner: Times: total = 79, boot = -139, init = 162, finish = 56\n",
            "23/04/13 03:46:23 INFO PythonRunner: Times: total = 473, boot = 46, init = 425, finish = 2\n",
            "23/04/13 03:46:23 INFO PythonRunner: Times: total = 73, boot = -122, init = 194, finish = 1\n",
            "23/04/13 03:46:23 INFO Executor: Finished task 2.0 in stage 0.0 (TID 2). 1571 bytes result sent to driver\n",
            "23/04/13 03:46:23 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1614 bytes result sent to driver\n",
            "23/04/13 03:46:23 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3) (a1ad50929c8d, executor driver, partition 3, PROCESS_LOCAL, 4593 bytes) taskResourceAssignments Map()\n",
            "23/04/13 03:46:23 INFO Executor: Running task 3.0 in stage 0.0 (TID 3)\n",
            "23/04/13 03:46:23 INFO HadoopRDD: Input split: file:/content/Salaries.csv:350012+350012\n",
            "23/04/13 03:46:23 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 522 ms on a1ad50929c8d (executor driver) (2/4)\n",
            "23/04/13 03:46:23 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 2400 ms on a1ad50929c8d (executor driver) (3/4)\n",
            "23/04/13 03:46:23 INFO PythonRunner: Times: total = 84, boot = -538, init = 545, finish = 77\n",
            "23/04/13 03:46:23 INFO PythonRunner: Times: total = 71, boot = -354, init = 424, finish = 1\n",
            "23/04/13 03:46:23 INFO Executor: Finished task 3.0 in stage 0.0 (TID 3). 1571 bytes result sent to driver\n",
            "23/04/13 03:46:23 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 174 ms on a1ad50929c8d (executor driver) (4/4)\n",
            "23/04/13 03:46:23 INFO DAGScheduler: ShuffleMapStage 0 (join at /content/4_join.py:48) finished in 3.307 s\n",
            "23/04/13 03:46:23 INFO DAGScheduler: looking for newly runnable stages\n",
            "23/04/13 03:46:23 INFO DAGScheduler: running: Set()\n",
            "23/04/13 03:46:23 INFO DAGScheduler: waiting: Set(ShuffleMapStage 1, ResultStage 2)\n",
            "23/04/13 03:46:23 INFO DAGScheduler: failed: Set()\n",
            "23/04/13 03:46:23 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
            "23/04/13 03:46:23 INFO DAGScheduler: Submitting ShuffleMapStage 1 (MapPartitionsRDD[12] at coalesce at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/04/13 03:46:23 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 12.9 KiB, free 365.5 MiB)\n",
            "23/04/13 03:46:23 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 7.2 KiB, free 365.5 MiB)\n",
            "23/04/13 03:46:23 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on a1ad50929c8d:42713 (size: 7.2 KiB, free: 366.2 MiB)\n",
            "23/04/13 03:46:23 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1478\n",
            "23/04/13 03:46:23 INFO DAGScheduler: Submitting 4 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[12] at coalesce at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))\n",
            "23/04/13 03:46:23 INFO TaskSchedulerImpl: Adding task set 1.0 with 4 tasks resource profile 0\n",
            "23/04/13 03:46:23 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 4) (a1ad50929c8d, executor driver, partition 2, NODE_LOCAL, 4260 bytes) taskResourceAssignments Map()\n",
            "23/04/13 03:46:23 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 5) (a1ad50929c8d, executor driver, partition 0, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()\n",
            "23/04/13 03:46:23 INFO Executor: Running task 2.0 in stage 1.0 (TID 4)\n",
            "23/04/13 03:46:23 INFO Executor: Running task 0.0 in stage 1.0 (TID 5)\n",
            "23/04/13 03:46:23 INFO ShuffleBlockFetcherIterator: Getting 3 (2.2 KiB) non-empty blocks including 3 (2.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/13 03:46:23 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/13 03:46:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 29 ms\n",
            "23/04/13 03:46:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 27 ms\n",
            "23/04/13 03:46:23 INFO PythonRunner: Times: total = 46, boot = -908, init = 951, finish = 3\n",
            "23/04/13 03:46:23 INFO PythonRunner: Times: total = 63, boot = -742, init = 805, finish = 0\n",
            "23/04/13 03:46:23 INFO Executor: Finished task 0.0 in stage 1.0 (TID 5). 1654 bytes result sent to driver\n",
            "23/04/13 03:46:23 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 6) (a1ad50929c8d, executor driver, partition 1, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()\n",
            "23/04/13 03:46:23 INFO Executor: Running task 1.0 in stage 1.0 (TID 6)\n",
            "23/04/13 03:46:23 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 5) in 240 ms on a1ad50929c8d (executor driver) (1/4)\n",
            "23/04/13 03:46:23 INFO Executor: Finished task 2.0 in stage 1.0 (TID 4). 1783 bytes result sent to driver\n",
            "23/04/13 03:46:23 INFO TaskSetManager: Starting task 3.0 in stage 1.0 (TID 7) (a1ad50929c8d, executor driver, partition 3, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()\n",
            "23/04/13 03:46:23 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/13 03:46:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
            "23/04/13 03:46:23 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 4) in 262 ms on a1ad50929c8d (executor driver) (2/4)\n",
            "23/04/13 03:46:23 INFO Executor: Running task 3.0 in stage 1.0 (TID 7)\n",
            "23/04/13 03:46:24 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/13 03:46:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms\n",
            "23/04/13 03:46:24 INFO PythonRunner: Times: total = 59, boot = -357, init = 415, finish = 1\n",
            "23/04/13 03:46:24 INFO Executor: Finished task 1.0 in stage 1.0 (TID 6). 1654 bytes result sent to driver\n",
            "23/04/13 03:46:24 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 6) in 85 ms on a1ad50929c8d (executor driver) (3/4)\n",
            "23/04/13 03:46:24 INFO PythonRunner: Times: total = 44, boot = -361, init = 404, finish = 1\n",
            "23/04/13 03:46:24 INFO Executor: Finished task 3.0 in stage 1.0 (TID 7). 1654 bytes result sent to driver\n",
            "23/04/13 03:46:24 INFO TaskSetManager: Finished task 3.0 in stage 1.0 (TID 7) in 90 ms on a1ad50929c8d (executor driver) (4/4)\n",
            "23/04/13 03:46:24 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
            "23/04/13 03:46:24 INFO DAGScheduler: ShuffleMapStage 1 (coalesce at NativeMethodAccessorImpl.java:0) finished in 0.383 s\n",
            "23/04/13 03:46:24 INFO DAGScheduler: looking for newly runnable stages\n",
            "23/04/13 03:46:24 INFO DAGScheduler: running: Set()\n",
            "23/04/13 03:46:24 INFO DAGScheduler: waiting: Set(ResultStage 2)\n",
            "23/04/13 03:46:24 INFO DAGScheduler: failed: Set()\n",
            "23/04/13 03:46:24 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[18] at saveAsTextFile at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/04/13 03:46:24 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 105.9 KiB, free 365.4 MiB)\n",
            "23/04/13 03:46:24 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 39.6 KiB, free 365.4 MiB)\n",
            "23/04/13 03:46:24 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on a1ad50929c8d:42713 (size: 39.6 KiB, free: 366.2 MiB)\n",
            "23/04/13 03:46:24 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1478\n",
            "23/04/13 03:46:24 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[18] at saveAsTextFile at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/04/13 03:46:24 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0\n",
            "23/04/13 03:46:24 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 8) (a1ad50929c8d, executor driver, partition 0, NODE_LOCAL, 4547 bytes) taskResourceAssignments Map()\n",
            "23/04/13 03:46:24 INFO Executor: Running task 0.0 in stage 2.0 (TID 8)\n",
            "23/04/13 03:46:24 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
            "23/04/13 03:46:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "23/04/13 03:46:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "23/04/13 03:46:24 INFO ShuffleBlockFetcherIterator: Getting 1 (7.6 KiB) non-empty blocks including 1 (7.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/13 03:46:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 17 ms\n",
            "23/04/13 03:46:24 INFO PythonRunner: Times: total = 45, boot = -314, init = 357, finish = 2\n",
            "23/04/13 03:46:24 INFO FileOutputCommitter: Saved output of task 'attempt_202304130346197311079459327240357_0018_m_000000_0' to file:/content/4_join.out/_temporary/0/task_202304130346197311079459327240357_0018_m_000000\n",
            "23/04/13 03:46:24 INFO SparkHadoopMapRedUtil: attempt_202304130346197311079459327240357_0018_m_000000_0: Committed\n",
            "23/04/13 03:46:24 INFO Executor: Finished task 0.0 in stage 2.0 (TID 8). 1952 bytes result sent to driver\n",
            "23/04/13 03:46:24 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 8) in 295 ms on a1ad50929c8d (executor driver) (1/1)\n",
            "23/04/13 03:46:24 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
            "23/04/13 03:46:24 INFO DAGScheduler: ResultStage 2 (runJob at SparkHadoopWriter.scala:83) finished in 0.347 s\n",
            "23/04/13 03:46:24 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "23/04/13 03:46:24 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished\n",
            "23/04/13 03:46:24 INFO DAGScheduler: Job 0 finished: runJob at SparkHadoopWriter.scala:83, took 4.296399 s\n",
            "23/04/13 03:46:24 INFO SparkHadoopWriter: Start to commit write Job job_202304130346197311079459327240357_0018.\n",
            "23/04/13 03:46:24 INFO SparkHadoopWriter: Write Job job_202304130346197311079459327240357_0018 committed. Elapsed time: 19 ms.\n",
            "23/04/13 03:46:24 INFO SparkUI: Stopped Spark web UI at http://a1ad50929c8d:4040\n",
            "23/04/13 03:46:24 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "23/04/13 03:46:24 INFO MemoryStore: MemoryStore cleared\n",
            "23/04/13 03:46:24 INFO BlockManager: BlockManager stopped\n",
            "23/04/13 03:46:24 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "23/04/13 03:46:24 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "23/04/13 03:46:24 INFO SparkContext: Successfully stopped SparkContext\n",
            "23/04/13 03:46:25 INFO ShutdownHookManager: Shutdown hook called\n",
            "23/04/13 03:46:25 INFO ShutdownHookManager: Deleting directory /tmp/spark-842cf94c-5b53-4b29-9fef-6b431c2cfba7\n",
            "23/04/13 03:46:25 INFO ShutdownHookManager: Deleting directory /tmp/spark-842cf94c-5b53-4b29-9fef-6b431c2cfba7/pyspark-64c4bab9-aeb1-477c-9273-39c6c3425b25\n",
            "23/04/13 03:46:25 INFO ShutdownHookManager: Deleting directory /tmp/spark-837d548e-3c0b-4013-8a56-253b9d13fd08\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XpkDNQCb__D3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c06a1cc5-40f9-44b9-d69f-033ef7840020"
      },
      "source": [
        "# test locally\n",
        "results = [('join', 'bondsba01,1991,1,PIT,NL,153,153,510,95,149,28,5,25,116,43,13,107,73,25,4,0,13,8,153,1991,PIT,NL,bondsba01,2300000'),\n",
        "('join', 'bondsba01,1993,1,SFN,NL,159,159,539,129,181,38,4,46,123,29,12,126,79,43,2,0,7,11,159,1993,SFN,NL,bondsba01,4516666'),\n",
        "('join', 'bondsba01,2002,1,SFN,NL,143,143,403,117,149,31,2,46,110,9,2,198,47,68,9,0,2,4,143,2002,SFN,NL,bondsba01,15000000'),\n",
        "('join', 'bondsba01,2004,1,SFN,NL,147,147,373,129,135,27,3,45,101,6,1,232,41,120,9,0,3,5,147,2004,SFN,NL,bondsba01,18000000'),\n",
        "('join', 'bondsba01,1986,1,PIT,NL,113,113,413,72,92,26,3,16,48,36,7,65,102,2,2,2,2,4,113,1986,PIT,NL,bondsba01,60000'),\n",
        "('join', 'bondsba01,1996,1,SFN,NL,158,158,517,122,159,27,3,42,129,40,7,151,76,30,1,0,6,11,158,1996,SFN,NL,bondsba01,8416667'),\n",
        "('join', 'bondsba01,1997,1,SFN,NL,159,159,532,123,155,26,5,40,101,37,8,145,87,34,8,0,5,13,159,1997,SFN,NL,bondsba01,8666667'),\n",
        "('join', 'bondsba01,1999,1,SFN,NL,102,102,355,91,93,20,2,34,83,15,2,73,62,9,3,0,3,6,102,1999,SFN,NL,bondsba01,9381057'),\n",
        "('join', 'bondsba01,1990,1,PIT,NL,151,151,519,104,156,32,3,33,114,52,13,93,83,15,3,0,6,8,151,1990,PIT,NL,bondsba01,850000'),\n",
        "('join', 'bondsba01,1994,1,SFN,NL,112,112,391,89,122,18,1,37,81,29,9,74,43,18,6,0,3,3,112,1994,SFN,NL,bondsba01,5166666'),\n",
        "('join', 'bondsba01,1995,1,SFN,NL,144,144,506,109,149,30,7,33,104,31,10,120,83,22,5,0,4,12,144,1995,SFN,NL,bondsba01,8166666'),\n",
        "('join', 'bondsba01,2003,1,SFN,NL,130,130,390,111,133,22,1,45,90,7,0,148,58,61,10,0,2,7,130,2003,SFN,NL,bondsba01,15500000'),\n",
        "('join', 'bondsba01,2007,1,SFN,NL,126,126,340,75,94,14,0,28,66,5,0,132,54,43,3,0,2,13,126,2007,SFN,NL,bondsba01,15533970'),\n",
        "('join', 'bondsba01,1987,1,PIT,NL,150,150,551,99,144,34,9,25,59,32,10,54,88,3,3,0,3,4,150,1987,PIT,NL,bondsba01,100000'),\n",
        "('join', 'bondsba01,1988,1,PIT,NL,144,144,538,97,152,30,5,24,58,17,11,72,82,14,2,0,2,3,144,1988,PIT,NL,bondsba01,220000'),\n",
        "('join', 'bondsba01,1989,1,PIT,NL,159,159,580,96,144,34,6,19,58,32,10,93,93,22,1,1,4,9,159,1989,PIT,NL,bondsba01,360000'),\n",
        "('join', 'bondsba01,1992,1,PIT,NL,140,140,473,109,147,36,5,34,103,39,8,127,69,32,5,0,7,9,140,1992,PIT,NL,bondsba01,4800000'),\n",
        "('join', 'bondsba01,1998,1,SFN,NL,156,156,552,120,167,44,7,37,122,28,12,130,92,29,8,1,6,15,156,1998,SFN,NL,bondsba01,8916667'),\n",
        "('join', 'bondsba01,2000,1,SFN,NL,143,143,480,129,147,28,4,49,106,11,3,117,77,22,3,0,7,6,143,2000,SFN,NL,bondsba01,10658826'),\n",
        "('join', 'bondsba01,2001,1,SFN,NL,153,153,476,129,156,32,2,73,137,13,3,177,93,35,9,0,2,5,153,2001,SFN,NL,bondsba01,10300000'),\n",
        "('join', 'bondsba01,2005,1,SFN,NL,14,14,42,8,12,1,0,5,10,0,0,9,6,3,0,0,1,0,14,2005,SFN,NL,bondsba01,22000000'),\n",
        "('join', 'bondsba01,2006,1,SFN,NL,130,130,367,74,99,23,0,26,77,3,0,115,51,38,10,0,1,9,130,2006,SFN,NL,bondsba01,19331470')]\n",
        "def test4(lines):\n",
        "  global results\n",
        "  results = [str(x) for x in results]\n",
        "  find_lines = 0\n",
        "  for  line in lines:\n",
        "    if line.strip() in results:\n",
        "        find_lines += 1\n",
        "  if find_lines != 22:\n",
        "      assert False\n",
        "  print('test passed')\n",
        "with open('4_join.out/part-00000') as f:\n",
        "    lines = f.readlines()\n",
        "    test4(lines)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test passed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "EMR"
      ],
      "metadata": {
        "id": "BKJT7nQwzww9"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGssEqiGzyEE"
      },
      "source": [
        "# upload script to S3. This assumes that your bucket name is vandy-bigdata. if not then change the  paths here.\n",
        "s3.upload_file(Filename='4_join.py', Bucket='far-bigdata-spark', Key='hw6/4_join.py')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# replae with your EMR cluster ID\n",
        "CLUSTER_ID = 'j-BP8EOODICRFL'\n",
        "\n",
        "def submit_job(app_name, pyfile_uri):\n",
        "    emr = session.client('emr')\n",
        "    emr.add_job_flow_steps(JobFlowId=CLUSTER_ID, Steps=[{\n",
        "        'Name': app_name,\n",
        "        'ActionOnFailure': 'CANCEL_AND_WAIT',\n",
        "        'HadoopJarStep': {\n",
        "            'Args': ['spark-submit',\n",
        "                     '--master', 'yarn',\n",
        "                     '--deploy-mode', 'cluster',\n",
        "                     pyfile_uri],\n",
        "            'Jar': 'command-runner.jar'\n",
        "        }}])"
      ],
      "metadata": {
        "id": "rnlUowmizyEF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7bAt1R5zyEF"
      },
      "source": [
        "# submit spark job to emr. Make all the necessary changes to the path\n",
        "submit_job(app_name='4_join', pyfile_uri='s3://far-bigdata-spark/hw6/4_join.py')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "534e96d8-c9e6-4f36-e0e5-44fb05e87980",
        "id": "n-Qaqh6qzyEF"
      },
      "source": [
        "# test EMR execution results. Once again, make sure that S3 paths are consistent.\n",
        "output_key = \"hw6/4_join.out/part-00000\"\n",
        "lines = s3.get_object(Bucket='far-bigdata-spark', Key=output_key)['Body'].read().decode().splitlines()\n",
        "test4(lines)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test passed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3 Execute the same scripts on EMR.\n",
        "\n",
        "* Make sure that you have created an EMR cluster using the instructions in the main readme.\n",
        "* upload the main data to s3"
      ],
      "metadata": {
        "id": "Ed2gw0o387bf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup AWS Credentials and fill them here. Make sure you do not save this information back to github"
      ],
      "metadata": {
        "id": "dyeTsQLV-B-u"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bMEsJXTU__Dx"
      },
      "source": [
        "# Please fill your aws credential information here\n",
        "credentials = {\n",
        "    'region_name': 'xxx',\n",
        "    'aws_access_key_id': 'xxx',\n",
        "    'aws_secret_access_key': 'xxx',\n",
        "    'aws_session_token': 'xxx'\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yRJjVwtO__Dx"
      },
      "source": [
        "!pip install boto3\n",
        "import boto3, json\n",
        "\n",
        "session = boto3.session.Session(**credentials)\n",
        "s3 = session.client('s3')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Upload Data to S3"
      ],
      "metadata": {
        "id": "Mss_SpU89_eo"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8MzsIwij__Dy"
      },
      "source": [
        "# upload tweets dataset to S3, please replace the bucket name and object keys with yours\n",
        "s3.upload_file(Filename='nashville-tweets-2019-01-28', Bucket='far-bigdata-spark', Key='nashville-tweets-2019-01-28')\n",
        "s3.upload_file(Filename='Batting.csv', Bucket='far-bigdata-spark', Key='Batting.csv')\n",
        "s3.upload_file(Filename='Salaries.csv', Bucket='far-bigdata-spark', Key='Salaries.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lljwHyOy__D2"
      },
      "source": [
        "# replae with your EMR cluster ID\n",
        "CLUSTER_ID = 'j-xxxx'\n",
        "\n",
        "def submit_job(app_name, pyfile_uri):\n",
        "    emr = session.client('emr')\n",
        "    emr.add_job_flow_steps(JobFlowId=CLUSTER_ID, Steps=[{\n",
        "        'Name': app_name,\n",
        "        'ActionOnFailure': 'CANCEL_AND_WAIT',\n",
        "        'HadoopJarStep': {\n",
        "            'Args': ['spark-submit',\n",
        "                     '--master', 'yarn',\n",
        "                     '--deploy-mode', 'cluster',\n",
        "                     pyfile_uri],\n",
        "            'Jar': 'command-runner.jar'\n",
        "        }}])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pCzOGzTj__D1"
      },
      "source": [
        "# upload script to S3\n",
        "s3.upload_file(Filename='2_group.py', Bucket='vandy-bigdata', Key='hw6/2_group.py')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eoqkfcEn__D1"
      },
      "source": [
        "# submit spark job to emr\n",
        "submit_job(app_name='2_group', pyfile_uri='s3://vandy-bigdata/hw6/2_group.py')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gm7iAdlL__D1"
      },
      "source": [
        "# test emr execution results\n",
        "output_key = \"hw6/2_group.out/part-00000\"\n",
        "lines = s3.get_object(Bucket='vandy-bigdata', Key=output_key)['Body'].read().decode().splitlines()\n",
        "test2(lines)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4VUV5ZIp__D2"
      },
      "source": [
        "# upload script to S3 - Make sure that the S3 bucket name is changed to your own bucket\n",
        "s3.upload_file(Filename='3_days.py', Bucket='vandy-bigdata', Key='hw6/3_days.py')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YFWrjexF__D2"
      },
      "source": [
        "# submit spark job to emr\n",
        "submit_job(app_name='3_days', pyfile_uri='s3://vandy-bigdata/hw6/3_days.py')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V1eFnxXZ__D3"
      },
      "source": [
        "# test emr execution results\n",
        "output_key = \"hw6/outputs/3_days/part-00000\"\n",
        "lines = s3.get_object(Bucket='vandy-bigdata', Key=output_key)['Body'].read().decode().splitlines()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OkQiNiXW__D4"
      },
      "source": [
        "# submit spark job to emr\n",
        "submit_job(app_name='4_join', pyfile_uri='s3://vandy-bigdata/hw6/4_join.py')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VqyNRexw__D4"
      },
      "source": [
        "# test emr execution results\n",
        "output_key = \"hw6/4_join.out/part-00000\"\n",
        "lines = s3.get_object(Bucket='vandy-bigdata', Key=output_key)['Body'].read().decode().splitlines()\n",
        "test4(lines)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}