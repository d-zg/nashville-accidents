{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vu-topics-in-big-data-2023/Team05/blob/main/spark_jobs_mark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "rRZIkSqT__Dv"
      },
      "outputs": [],
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_BcbpaYeKNlG",
        "outputId": "196ef8d1-43af-4376-a9ff-cb7950788fbb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 294140\n",
            "drwxr-xr-x  1 root root      4096 Apr 27 13:35 sample_data\n",
            "drwxr-xr-x 13 1000 1000      4096 Apr  9 21:17 spark-3.2.4-bin-hadoop3.2\n",
            "-rw-r--r--  1 root root 301183180 Apr  9 21:35 spark-3.2.4-bin-hadoop3.2.tgz\n"
          ]
        }
      ],
      "source": [
        "#install spark. we are using the one that uses hadoop as the underlying scheduler.\n",
        "!wget -q https://downloads.apache.org/spark/spark-3.2.4/spark-3.2.4-bin-hadoop3.2.tgz\n",
        "!tar xf  spark-3.2.4-bin-hadoop3.2.tgz\n",
        "!ls -l\n",
        "\n",
        "#Provides findspark.init() to make pyspark importable as a regular library.\n",
        "os.environ[\"SPARK_HOME\"] = \"spark-3.2.4-bin-hadoop3.2\"\n",
        "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.4.7 pyspark-shell'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xB4TPZH16jmR",
        "outputId": "d32458ea-09a1-45bc-b3b2-09e23d1f8cca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q findspark pyspark\n",
        "import findspark\n",
        "findspark.init()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKheJ7h0AiQW"
      },
      "source": [
        "## Step 1: Upload the three files included in the data folder to colab. \n",
        "\n",
        "Upload the zip version and then use the !unzip command on the shell to unzip them"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jc95zzPvzJJT",
        "outputId": "26b2ae3c-403a-4f8f-d12b-66ac0873cff4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cke5YdqizMy4"
      },
      "outputs": [],
      "source": [
        "#!cat /content/drive/MyDrive/Big-Data-Final/nfd_incidents_xd_seg.parquet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DNqilzg_zJwG"
      },
      "outputs": [],
      "source": [
        "drive.flush_and_unmount()\n",
        "print('All changes made in this colab session should now be visible in Drive.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1prQTAn7Mbu"
      },
      "source": [
        "# Write Spark Code Locally and test the Code and Save it to your repository"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pz7dMIVJ__Dy"
      },
      "source": [
        "# Step 2. Complete Spark Jobs Below Locally. \n",
        "\n",
        "Once they work you can submit them to EMR"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Average response time + variance by SegID"
      ],
      "metadata": {
        "id": "0pqBG_yqIPj4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"read_parquet_file\").getOrCreate()\n",
        "\n",
        "df = spark.read.parquet(\"/content/drive/MyDrive/Big-Data-Final/nfd_incidents_xd_seg.parquet\")\n"
      ],
      "metadata": {
        "id": "gmB_ijDxIQUV"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.show(5))"
      ],
      "metadata": {
        "id": "_GCTQzqjIf8H",
        "outputId": "ef4fe4d0-b808-4825-9622-3321399d7c7c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+-----------+------------+-------------+--------------------+--------------------+-----------------+-----------+--------------+--------------------+-----------+------------------+-------------+\n",
            "|         ID_Original|   latitude|   longitude|emdCardNumber|            time_utc|          time_local|response_time_sec|day_of_week|weekend_or_not|            geometry|Incident_ID|       Dist_to_Seg|      XDSegID|\n",
            "+--------------------+-----------+------------+-------------+--------------------+--------------------+-----------------+-----------+--------------+--------------------+-----------+------------------+-------------+\n",
            "|ObjectId(59d3a819...|36.03722849|-86.78324314|         29B5|2017-01-01 07:59:...|2017-01-01 01:59:...|            268.0|          6|             1|POINT (-86.783243...|         10|13.550370382347305|1.524393684E9|\n",
            "|ObjectId(59d3a819...|36.03741402|-86.78657189|        29D2P| 2017-01-01 08:30:55| 2017-01-01 02:30:55|            512.0|          6|             1|POINT (-86.786571...|         11|3.6639521286614225|1.524356434E9|\n",
            "|ObjectId(59d3a81a...|36.21766267|-86.80869908|        29D1H| 2017-01-01 09:35:10| 2017-01-01 03:35:10|            271.0|          6|             1|POINT (-86.808699...|         12|              null|         null|\n",
            "|ObjectId(59d3a81a...|36.05779026|-86.73972836|        29D8V|2017-01-01 10:29:...|2017-01-01 04:29:...|            366.0|          6|             1|POINT (-86.739728...|         13|              null|         null|\n",
            "|ObjectId(59d3a81a...|36.03922079|-86.60278517|        29A2V| 2017-01-01 11:09:13| 2017-01-01 05:09:13|            447.0|          6|             1|POINT (-86.602785...|         14| 3.171781614079594| 4.49620819E8|\n",
            "+--------------------+-----------+------------+-------------+--------------------+--------------------+-----------------+-----------+--------------+--------------------+-----------+------------------+-------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#HERE WE HAVE TO CREATE THE CENSUS TRACT PER ROW"
      ],
      "metadata": {
        "id": "TCXUXGDiIxA2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import avg\n",
        "\n",
        "# Read the input file as a Spark dataframe\n",
        "# df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"input.csv\")\n",
        "\n",
        "# Group the dataframe by XDSegID and compute the average response time\n",
        "result_df = df.groupBy(\"XDSegID\").agg(avg(\"response_time_sec\").alias(\"AvgResponseTime\"))\n",
        "\n",
        "# Write the result to a CSV file\n",
        "result_df.write.format(\"csv\").option(\"header\", \"true\").mode(\"overwrite\").save(\"output.csv\")"
      ],
      "metadata": {
        "id": "yMJDi2KHI1nY"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result_df.show()"
      ],
      "metadata": {
        "id": "TeFR1DbzJJW-",
        "outputId": "faaba0c3-882e-40e3-d9bb-0af0b821f26f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+------------------+\n",
            "|      XDSegID|   AvgResponseTime|\n",
            "+-------------+------------------+\n",
            "|1.524470198E9|             349.2|\n",
            "| 1.52446673E9|             404.0|\n",
            "| 4.41552617E8| 297.6666666666667|\n",
            "|  4.5042845E8|             582.5|\n",
            "|1.524609417E9|             407.6|\n",
            "|1.524475884E9|             337.0|\n",
            "| 4.29334846E8|            490.25|\n",
            "|1.524322123E9|             154.5|\n",
            "| 4.49620825E8|             553.0|\n",
            "|1.524554221E9|             410.0|\n",
            "| 4.49621869E8|             798.0|\n",
            "| 4.49629818E8|             148.5|\n",
            "| 1.56811863E8|             405.0|\n",
            "|1.524576615E9|             496.0|\n",
            "| 4.29334039E8|             449.6|\n",
            "| 1.56551371E8|             256.0|\n",
            "| 1.56121138E8|             310.8|\n",
            "|1.524303919E9|           242.625|\n",
            "|1.524378121E9|476.14285714285717|\n",
            "|1.524474299E9|             449.5|\n",
            "+-------------+------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, variance\n",
        "\n",
        "# group by XDSegID and calculate variance of ResponseTime\n",
        "df_agg = df.groupBy(\"XDSegID\").agg(variance(col(\"response_time_sec\")).alias(\"Variance\"))\n",
        "\n",
        "# show the results\n",
        "df_agg.show()"
      ],
      "metadata": {
        "id": "P8-C7N2iJZc-",
        "outputId": "d93a4cf8-c5da-4998-f66c-9def30f9e3f8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+------------------+\n",
            "|      XDSegID|          Variance|\n",
            "+-------------+------------------+\n",
            "|1.524470198E9|           51834.2|\n",
            "| 1.52446673E9| 68509.06666666667|\n",
            "| 4.41552617E8| 4837.333333333333|\n",
            "|  4.5042845E8|          180600.5|\n",
            "|1.524609417E9|           23415.3|\n",
            "|1.524475884E9|              null|\n",
            "| 4.29334846E8|         124896.25|\n",
            "|1.524322123E9|             364.5|\n",
            "| 4.49620825E8|           18050.0|\n",
            "|1.524554221E9|              null|\n",
            "| 4.49621869E8|              null|\n",
            "| 4.49629818E8|           42340.5|\n",
            "| 1.56811863E8|              null|\n",
            "|1.524576615E9|              null|\n",
            "| 4.29334039E8|11660.300000000001|\n",
            "| 1.56551371E8|            2584.0|\n",
            "| 1.56121138E8|           20254.7|\n",
            "|1.524303919E9|14160.916666666664|\n",
            "|1.524378121E9|18207.476190476194|\n",
            "|1.524474299E9|           19012.5|\n",
            "+-------------+------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxUib2yt__Dy"
      },
      "source": [
        "## Job 1. Count the number of tweets.\n",
        "\n",
        "I have almost completed this for you. You still have to do the reduce and add - look into the wordcount example. But then use this as the template to finish the rest."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YDL78heZ__Dz",
        "outputId": "5fd64d5f-c334-4935-a9fe-edc70318d889"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting 1_count.py\n"
          ]
        }
      ],
      "source": [
        "%%file 1_count.py\n",
        "'''\n",
        "TODO:\n",
        "Count the number of tweets.\n",
        "Parse tweets with json.loads -- note how the tweets are huge JSON blobs.\n",
        "Ignore tweets that error on load.\n",
        "'''\n",
        "\n",
        "from pyspark import SparkContext, SparkConf\n",
        "import json\n",
        "from operator import add\n",
        "#import other things as required\n",
        "\n",
        "#create spark context. This is very important. Do this similarly for the other parts\n",
        "# Note to read a file directly from s3 into an rdd you may have to do something like this\n",
        "\n",
        "def checkjson(entry):\n",
        "  try:\n",
        "    json.loads(entry)\n",
        "    #if load succeeded. We use correct as the key\n",
        "    return \"correct\", 1\n",
        "  except:\n",
        "    #there was an error in loading. We use incorrect as the key\n",
        "    return \"incorrect\", 1\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  # replace this line with the s3 pass when testing over EMR\n",
        "  conf = SparkConf().setAppName('1_count').set('spark.hadoop.validateOutputSpecs', False)\n",
        "  sc = SparkContext(conf=conf).getOrCreate()\n",
        "\n",
        "  try:\n",
        "    #@todo: fix the path as required\n",
        "    tweets=sc.textFile('./nashville-tweets-2019-01-28')\n",
        "    # review the page rank example for how to use the map operation\n",
        "    # review word count for reduce and add\n",
        "    # see how we use map to parse each row\n",
        "    counts = tweets.map(lambda tweet: checkjson(tweet))\n",
        "    print(counts.take(2))\n",
        "\n",
        "    # @todo: create an output rdd that uses reduce by key and add operator to add up the correct entries and incorrect entries\n",
        "    total = counts.map(lambda x: (x, 1)).reduceByKey(lambda a, b: a + b)\n",
        "\n",
        "    # @todo: the s3 version will have to save it to correct s3 path\n",
        "    total.repartition(1).saveAsTextFile(\"1_count.out\")\n",
        "\n",
        "  finally:\n",
        "    # very important: stop the context. Otherwise you may get an error that context is still alive. if you are on colab just restart the runtime if you face problem\n",
        "    #finally is used to make sure the context is stopped even with errors\n",
        "    sc.stop()\n",
        "  \n",
        "\n",
        " \n",
        "  \n",
        "  pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PH2TFBJp7q0O"
      },
      "source": [
        "### Test local Execution Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L3y2A34y__Dz",
        "outputId": "5523396e-f45b-4ce5-8f40-8191e4a2d85d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ":: loading settings :: url = jar:file:/content/spark-3.2.3-bin-hadoop3.2/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
            "Ivy Default Cache set to: /root/.ivy2/cache\n",
            "The jars for the packages stored in: /root/.ivy2/jars\n",
            "org.apache.spark#spark-streaming-kafka-0-8_2.11 added as a dependency\n",
            ":: resolving dependencies :: org.apache.spark#spark-submit-parent-e64045ba-d7b8-4a38-982e-785ae9af6a5c;1.0\n",
            "\tconfs: [default]\n",
            "\tfound org.apache.spark#spark-streaming-kafka-0-8_2.11;2.4.7 in central\n",
            "\tfound org.apache.kafka#kafka_2.11;0.8.2.1 in central\n",
            "\tfound org.scala-lang.modules#scala-xml_2.11;1.0.2 in central\n",
            "\tfound com.yammer.metrics#metrics-core;2.2.0 in central\n",
            "\tfound org.slf4j#slf4j-api;1.7.16 in central\n",
            "\tfound org.scala-lang.modules#scala-parser-combinators_2.11;1.1.0 in central\n",
            "\tfound com.101tec#zkclient;0.3 in central\n",
            "\tfound log4j#log4j;1.2.17 in central\n",
            "\tfound org.apache.kafka#kafka-clients;0.8.2.1 in central\n",
            "\tfound net.jpountz.lz4#lz4;1.2.0 in central\n",
            "\tfound org.xerial.snappy#snappy-java;1.1.7.5 in central\n",
            "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
            "downloading https://repo1.maven.org/maven2/org/apache/spark/spark-streaming-kafka-0-8_2.11/2.4.7/spark-streaming-kafka-0-8_2.11-2.4.7.jar ...\n",
            "\t[SUCCESSFUL ] org.apache.spark#spark-streaming-kafka-0-8_2.11;2.4.7!spark-streaming-kafka-0-8_2.11.jar (35ms)\n",
            "downloading https://repo1.maven.org/maven2/org/apache/kafka/kafka_2.11/0.8.2.1/kafka_2.11-0.8.2.1.jar ...\n",
            "\t[SUCCESSFUL ] org.apache.kafka#kafka_2.11;0.8.2.1!kafka_2.11.jar (172ms)\n",
            "downloading https://repo1.maven.org/maven2/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar ...\n",
            "\t[SUCCESSFUL ] org.spark-project.spark#unused;1.0.0!unused.jar (8ms)\n",
            "downloading https://repo1.maven.org/maven2/org/scala-lang/modules/scala-xml_2.11/1.0.2/scala-xml_2.11-1.0.2.jar ...\n",
            "\t[SUCCESSFUL ] org.scala-lang.modules#scala-xml_2.11;1.0.2!scala-xml_2.11.jar(bundle) (36ms)\n",
            "downloading https://repo1.maven.org/maven2/com/yammer/metrics/metrics-core/2.2.0/metrics-core-2.2.0.jar ...\n",
            "\t[SUCCESSFUL ] com.yammer.metrics#metrics-core;2.2.0!metrics-core.jar (12ms)\n",
            "downloading https://repo1.maven.org/maven2/org/scala-lang/modules/scala-parser-combinators_2.11/1.1.0/scala-parser-combinators_2.11-1.1.0.jar ...\n",
            "\t[SUCCESSFUL ] org.scala-lang.modules#scala-parser-combinators_2.11;1.1.0!scala-parser-combinators_2.11.jar(bundle) (28ms)\n",
            "downloading https://repo1.maven.org/maven2/com/101tec/zkclient/0.3/zkclient-0.3.jar ...\n",
            "\t[SUCCESSFUL ] com.101tec#zkclient;0.3!zkclient.jar (8ms)\n",
            "downloading https://repo1.maven.org/maven2/org/apache/kafka/kafka-clients/0.8.2.1/kafka-clients-0.8.2.1.jar ...\n",
            "\t[SUCCESSFUL ] org.apache.kafka#kafka-clients;0.8.2.1!kafka-clients.jar (25ms)\n",
            "downloading https://repo1.maven.org/maven2/org/slf4j/slf4j-api/1.7.16/slf4j-api-1.7.16.jar ...\n",
            "\t[SUCCESSFUL ] org.slf4j#slf4j-api;1.7.16!slf4j-api.jar (9ms)\n",
            "downloading https://repo1.maven.org/maven2/log4j/log4j/1.2.17/log4j-1.2.17.jar ...\n",
            "\t[SUCCESSFUL ] log4j#log4j;1.2.17!log4j.jar(bundle) (69ms)\n",
            "downloading https://repo1.maven.org/maven2/net/jpountz/lz4/lz4/1.2.0/lz4-1.2.0.jar ...\n",
            "\t[SUCCESSFUL ] net.jpountz.lz4#lz4;1.2.0!lz4.jar (14ms)\n",
            "downloading https://repo1.maven.org/maven2/org/xerial/snappy/snappy-java/1.1.7.5/snappy-java-1.1.7.5.jar ...\n",
            "\t[SUCCESSFUL ] org.xerial.snappy#snappy-java;1.1.7.5!snappy-java.jar(bundle) (64ms)\n",
            ":: resolution report :: resolve 5038ms :: artifacts dl 499ms\n",
            "\t:: modules in use:\n",
            "\tcom.101tec#zkclient;0.3 from central in [default]\n",
            "\tcom.yammer.metrics#metrics-core;2.2.0 from central in [default]\n",
            "\tlog4j#log4j;1.2.17 from central in [default]\n",
            "\tnet.jpountz.lz4#lz4;1.2.0 from central in [default]\n",
            "\torg.apache.kafka#kafka-clients;0.8.2.1 from central in [default]\n",
            "\torg.apache.kafka#kafka_2.11;0.8.2.1 from central in [default]\n",
            "\torg.apache.spark#spark-streaming-kafka-0-8_2.11;2.4.7 from central in [default]\n",
            "\torg.scala-lang.modules#scala-parser-combinators_2.11;1.1.0 from central in [default]\n",
            "\torg.scala-lang.modules#scala-xml_2.11;1.0.2 from central in [default]\n",
            "\torg.slf4j#slf4j-api;1.7.16 from central in [default]\n",
            "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
            "\torg.xerial.snappy#snappy-java;1.1.7.5 from central in [default]\n",
            "\t---------------------------------------------------------------------\n",
            "\t|                  |            modules            ||   artifacts   |\n",
            "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
            "\t---------------------------------------------------------------------\n",
            "\t|      default     |   12  |   12  |   12  |   0   ||   12  |   12  |\n",
            "\t---------------------------------------------------------------------\n",
            ":: retrieving :: org.apache.spark#spark-submit-parent-e64045ba-d7b8-4a38-982e-785ae9af6a5c\n",
            "\tconfs: [default]\n",
            "\t12 artifacts copied, 0 already retrieved (8282kB/59ms)\n",
            "23/04/12 23:13:00 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
            "23/04/12 23:13:01 INFO SparkContext: Running Spark version 3.2.3\n",
            "23/04/12 23:13:01 INFO ResourceUtils: ==============================================================\n",
            "23/04/12 23:13:01 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "23/04/12 23:13:01 INFO ResourceUtils: ==============================================================\n",
            "23/04/12 23:13:01 INFO SparkContext: Submitted application: 1_count\n",
            "23/04/12 23:13:01 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "23/04/12 23:13:01 INFO ResourceProfile: Limiting resource is cpu\n",
            "23/04/12 23:13:01 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "23/04/12 23:13:01 INFO SecurityManager: Changing view acls to: root\n",
            "23/04/12 23:13:01 INFO SecurityManager: Changing modify acls to: root\n",
            "23/04/12 23:13:01 INFO SecurityManager: Changing view acls groups to: \n",
            "23/04/12 23:13:01 INFO SecurityManager: Changing modify acls groups to: \n",
            "23/04/12 23:13:01 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\n",
            "23/04/12 23:13:02 INFO Utils: Successfully started service 'sparkDriver' on port 38227.\n",
            "23/04/12 23:13:02 INFO SparkEnv: Registering MapOutputTracker\n",
            "23/04/12 23:13:02 INFO SparkEnv: Registering BlockManagerMaster\n",
            "23/04/12 23:13:02 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "23/04/12 23:13:02 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "23/04/12 23:13:02 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "23/04/12 23:13:02 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-d9c75dac-c79b-4d50-86be-4fa1c8542524\n",
            "23/04/12 23:13:02 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB\n",
            "23/04/12 23:13:02 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "23/04/12 23:13:03 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
            "23/04/12 23:13:03 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://8c2d110fc661:4040\n",
            "23/04/12 23:13:03 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar at spark://8c2d110fc661:38227/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1681341181635\n",
            "23/04/12 23:13:03 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar at spark://8c2d110fc661:38227/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1681341181635\n",
            "23/04/12 23:13:03 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar at spark://8c2d110fc661:38227/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1681341181635\n",
            "23/04/12 23:13:03 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar at spark://8c2d110fc661:38227/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1681341181635\n",
            "23/04/12 23:13:03 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar at spark://8c2d110fc661:38227/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1681341181635\n",
            "23/04/12 23:13:03 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar at spark://8c2d110fc661:38227/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1681341181635\n",
            "23/04/12 23:13:03 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar at spark://8c2d110fc661:38227/jars/com.101tec_zkclient-0.3.jar with timestamp 1681341181635\n",
            "23/04/12 23:13:03 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar at spark://8c2d110fc661:38227/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1681341181635\n",
            "23/04/12 23:13:03 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar at spark://8c2d110fc661:38227/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1681341181635\n",
            "23/04/12 23:13:03 INFO SparkContext: Added JAR file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar at spark://8c2d110fc661:38227/jars/log4j_log4j-1.2.17.jar with timestamp 1681341181635\n",
            "23/04/12 23:13:03 INFO SparkContext: Added JAR file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar at spark://8c2d110fc661:38227/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1681341181635\n",
            "23/04/12 23:13:03 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar at spark://8c2d110fc661:38227/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1681341181635\n",
            "23/04/12 23:13:03 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar at file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1681341181635\n",
            "23/04/12 23:13:03 INFO Utils: Copying /root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar to /tmp/spark-5a1e6bf1-b7eb-4038-bb34-8002c9c6dbfc/userFiles-b55df87f-e987-40fb-b645-f55fcc1f8b7f/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar\n",
            "23/04/12 23:13:03 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar at file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1681341181635\n",
            "23/04/12 23:13:03 INFO Utils: Copying /root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar to /tmp/spark-5a1e6bf1-b7eb-4038-bb34-8002c9c6dbfc/userFiles-b55df87f-e987-40fb-b645-f55fcc1f8b7f/org.apache.kafka_kafka_2.11-0.8.2.1.jar\n",
            "23/04/12 23:13:03 INFO SparkContext: Added file file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar at file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1681341181635\n",
            "23/04/12 23:13:03 INFO Utils: Copying /root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-5a1e6bf1-b7eb-4038-bb34-8002c9c6dbfc/userFiles-b55df87f-e987-40fb-b645-f55fcc1f8b7f/org.spark-project.spark_unused-1.0.0.jar\n",
            "23/04/12 23:13:03 INFO SparkContext: Added file file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar at file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1681341181635\n",
            "23/04/12 23:13:03 INFO Utils: Copying /root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar to /tmp/spark-5a1e6bf1-b7eb-4038-bb34-8002c9c6dbfc/userFiles-b55df87f-e987-40fb-b645-f55fcc1f8b7f/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar\n",
            "23/04/12 23:13:03 INFO SparkContext: Added file file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar at file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1681341181635\n",
            "23/04/12 23:13:03 INFO Utils: Copying /root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar to /tmp/spark-5a1e6bf1-b7eb-4038-bb34-8002c9c6dbfc/userFiles-b55df87f-e987-40fb-b645-f55fcc1f8b7f/com.yammer.metrics_metrics-core-2.2.0.jar\n",
            "23/04/12 23:13:03 INFO SparkContext: Added file file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar at file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1681341181635\n",
            "23/04/12 23:13:03 INFO Utils: Copying /root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar to /tmp/spark-5a1e6bf1-b7eb-4038-bb34-8002c9c6dbfc/userFiles-b55df87f-e987-40fb-b645-f55fcc1f8b7f/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar\n",
            "23/04/12 23:13:03 INFO SparkContext: Added file file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar at file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar with timestamp 1681341181635\n",
            "23/04/12 23:13:03 INFO Utils: Copying /root/.ivy2/jars/com.101tec_zkclient-0.3.jar to /tmp/spark-5a1e6bf1-b7eb-4038-bb34-8002c9c6dbfc/userFiles-b55df87f-e987-40fb-b645-f55fcc1f8b7f/com.101tec_zkclient-0.3.jar\n",
            "23/04/12 23:13:03 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar at file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1681341181635\n",
            "23/04/12 23:13:03 INFO Utils: Copying /root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar to /tmp/spark-5a1e6bf1-b7eb-4038-bb34-8002c9c6dbfc/userFiles-b55df87f-e987-40fb-b645-f55fcc1f8b7f/org.apache.kafka_kafka-clients-0.8.2.1.jar\n",
            "23/04/12 23:13:03 INFO SparkContext: Added file file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar at file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1681341181635\n",
            "23/04/12 23:13:03 INFO Utils: Copying /root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar to /tmp/spark-5a1e6bf1-b7eb-4038-bb34-8002c9c6dbfc/userFiles-b55df87f-e987-40fb-b645-f55fcc1f8b7f/org.slf4j_slf4j-api-1.7.16.jar\n",
            "23/04/12 23:13:03 INFO SparkContext: Added file file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar at file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar with timestamp 1681341181635\n",
            "23/04/12 23:13:03 INFO Utils: Copying /root/.ivy2/jars/log4j_log4j-1.2.17.jar to /tmp/spark-5a1e6bf1-b7eb-4038-bb34-8002c9c6dbfc/userFiles-b55df87f-e987-40fb-b645-f55fcc1f8b7f/log4j_log4j-1.2.17.jar\n",
            "23/04/12 23:13:03 INFO SparkContext: Added file file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar at file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1681341181635\n",
            "23/04/12 23:13:03 INFO Utils: Copying /root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar to /tmp/spark-5a1e6bf1-b7eb-4038-bb34-8002c9c6dbfc/userFiles-b55df87f-e987-40fb-b645-f55fcc1f8b7f/net.jpountz.lz4_lz4-1.2.0.jar\n",
            "23/04/12 23:13:03 INFO SparkContext: Added file file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar at file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1681341181635\n",
            "23/04/12 23:13:03 INFO Utils: Copying /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar to /tmp/spark-5a1e6bf1-b7eb-4038-bb34-8002c9c6dbfc/userFiles-b55df87f-e987-40fb-b645-f55fcc1f8b7f/org.xerial.snappy_snappy-java-1.1.7.5.jar\n",
            "23/04/12 23:13:04 INFO Executor: Starting executor ID driver on host 8c2d110fc661\n",
            "23/04/12 23:13:04 INFO Executor: Fetching file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar with timestamp 1681341181635\n",
            "23/04/12 23:13:04 INFO Utils: /root/.ivy2/jars/log4j_log4j-1.2.17.jar has been previously copied to /tmp/spark-5a1e6bf1-b7eb-4038-bb34-8002c9c6dbfc/userFiles-b55df87f-e987-40fb-b645-f55fcc1f8b7f/log4j_log4j-1.2.17.jar\n",
            "23/04/12 23:13:04 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1681341181635\n",
            "23/04/12 23:13:04 INFO Utils: /root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar has been previously copied to /tmp/spark-5a1e6bf1-b7eb-4038-bb34-8002c9c6dbfc/userFiles-b55df87f-e987-40fb-b645-f55fcc1f8b7f/org.apache.kafka_kafka-clients-0.8.2.1.jar\n",
            "23/04/12 23:13:04 INFO Executor: Fetching file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1681341181635\n",
            "23/04/12 23:13:04 INFO Utils: /root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar has been previously copied to /tmp/spark-5a1e6bf1-b7eb-4038-bb34-8002c9c6dbfc/userFiles-b55df87f-e987-40fb-b645-f55fcc1f8b7f/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar\n",
            "23/04/12 23:13:04 INFO Executor: Fetching file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1681341181635\n",
            "23/04/12 23:13:04 INFO Utils: /root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar has been previously copied to /tmp/spark-5a1e6bf1-b7eb-4038-bb34-8002c9c6dbfc/userFiles-b55df87f-e987-40fb-b645-f55fcc1f8b7f/org.slf4j_slf4j-api-1.7.16.jar\n",
            "23/04/12 23:13:04 INFO Executor: Fetching file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1681341181635\n",
            "23/04/12 23:13:04 INFO Utils: /root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar has been previously copied to /tmp/spark-5a1e6bf1-b7eb-4038-bb34-8002c9c6dbfc/userFiles-b55df87f-e987-40fb-b645-f55fcc1f8b7f/net.jpountz.lz4_lz4-1.2.0.jar\n",
            "23/04/12 23:13:04 INFO Executor: Fetching file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1681341181635\n",
            "23/04/12 23:13:04 INFO Utils: /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar has been previously copied to /tmp/spark-5a1e6bf1-b7eb-4038-bb34-8002c9c6dbfc/userFiles-b55df87f-e987-40fb-b645-f55fcc1f8b7f/org.xerial.snappy_snappy-java-1.1.7.5.jar\n",
            "23/04/12 23:13:04 INFO Executor: Fetching file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1681341181635\n",
            "23/04/12 23:13:04 INFO Utils: /root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar has been previously copied to /tmp/spark-5a1e6bf1-b7eb-4038-bb34-8002c9c6dbfc/userFiles-b55df87f-e987-40fb-b645-f55fcc1f8b7f/org.spark-project.spark_unused-1.0.0.jar\n",
            "23/04/12 23:13:04 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1681341181635\n",
            "23/04/12 23:13:04 INFO Utils: /root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar has been previously copied to /tmp/spark-5a1e6bf1-b7eb-4038-bb34-8002c9c6dbfc/userFiles-b55df87f-e987-40fb-b645-f55fcc1f8b7f/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar\n",
            "23/04/12 23:13:04 INFO Executor: Fetching file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1681341181635\n",
            "23/04/12 23:13:04 INFO Utils: /root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar has been previously copied to /tmp/spark-5a1e6bf1-b7eb-4038-bb34-8002c9c6dbfc/userFiles-b55df87f-e987-40fb-b645-f55fcc1f8b7f/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar\n",
            "23/04/12 23:13:04 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1681341181635\n",
            "23/04/12 23:13:04 INFO Utils: /root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar has been previously copied to /tmp/spark-5a1e6bf1-b7eb-4038-bb34-8002c9c6dbfc/userFiles-b55df87f-e987-40fb-b645-f55fcc1f8b7f/org.apache.kafka_kafka_2.11-0.8.2.1.jar\n",
            "23/04/12 23:13:04 INFO Executor: Fetching file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar with timestamp 1681341181635\n",
            "23/04/12 23:13:04 INFO Utils: /root/.ivy2/jars/com.101tec_zkclient-0.3.jar has been previously copied to /tmp/spark-5a1e6bf1-b7eb-4038-bb34-8002c9c6dbfc/userFiles-b55df87f-e987-40fb-b645-f55fcc1f8b7f/com.101tec_zkclient-0.3.jar\n",
            "23/04/12 23:13:04 INFO Executor: Fetching file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1681341181635\n",
            "23/04/12 23:13:04 INFO Utils: /root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar has been previously copied to /tmp/spark-5a1e6bf1-b7eb-4038-bb34-8002c9c6dbfc/userFiles-b55df87f-e987-40fb-b645-f55fcc1f8b7f/com.yammer.metrics_metrics-core-2.2.0.jar\n",
            "23/04/12 23:13:04 INFO Executor: Fetching spark://8c2d110fc661:38227/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1681341181635\n",
            "23/04/12 23:13:04 INFO TransportClientFactory: Successfully created connection to 8c2d110fc661/172.28.0.12:38227 after 63 ms (0 ms spent in bootstraps)\n",
            "23/04/12 23:13:04 INFO Utils: Fetching spark://8c2d110fc661:38227/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar to /tmp/spark-5a1e6bf1-b7eb-4038-bb34-8002c9c6dbfc/userFiles-b55df87f-e987-40fb-b645-f55fcc1f8b7f/fetchFileTemp9101984983856158522.tmp\n",
            "23/04/12 23:13:04 INFO Utils: /tmp/spark-5a1e6bf1-b7eb-4038-bb34-8002c9c6dbfc/userFiles-b55df87f-e987-40fb-b645-f55fcc1f8b7f/fetchFileTemp9101984983856158522.tmp has been previously copied to /tmp/spark-5a1e6bf1-b7eb-4038-bb34-8002c9c6dbfc/userFiles-b55df87f-e987-40fb-b645-f55fcc1f8b7f/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar\n",
            "23/04/12 23:13:04 INFO Executor: Adding file:/tmp/spark-5a1e6bf1-b7eb-4038-bb34-8002c9c6dbfc/userFiles-b55df87f-e987-40fb-b645-f55fcc1f8b7f/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar to class loader\n",
            "23/04/12 23:13:04 INFO Executor: Fetching spark://8c2d110fc661:38227/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1681341181635\n",
            "23/04/12 23:13:04 INFO Utils: Fetching spark://8c2d110fc661:38227/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar to /tmp/spark-5a1e6bf1-b7eb-4038-bb34-8002c9c6dbfc/userFiles-b55df87f-e987-40fb-b645-f55fcc1f8b7f/fetchFileTemp2249917270334771510.tmp\n",
            "23/04/12 23:13:04 INFO Utils: /tmp/spark-5a1e6bf1-b7eb-4038-bb34-8002c9c6dbfc/userFiles-b55df87f-e987-40fb-b645-f55fcc1f8b7f/fetchFileTemp2249917270334771510.tmp has been previously copied to /tmp/spark-5a1e6bf1-b7eb-4038-bb34-8002c9c6dbfc/userFiles-b55df87f-e987-40fb-b645-f55fcc1f8b7f/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar\n",
            "23/04/12 23:13:04 INFO Executor: Adding file:/tmp/spark-5a1e6bf1-b7eb-4038-bb34-8002c9c6dbfc/userFiles-b55df87f-e987-40fb-b645-f55fcc1f8b7f/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar to class loader\n",
            "23/04/12 23:13:04 INFO Executor: Fetching spark://8c2d110fc661:38227/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1681341181635\n",
            "23/04/12 23:13:04 INFO Utils: Fetching spark://8c2d110fc661:38227/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar to /tmp/spark-5a1e6bf1-b7eb-4038-bb34-8002c9c6dbfc/userFiles-b55df87f-e987-40fb-b645-f55fcc1f8b7f/fetchFileTemp4412217182379422083.tmp\n",
            "23/04/12 23:13:04 INFO Utils: /tmp/spark-5a1e6bf1-b7eb-4038-bb34-8002c9c6dbfc/userFiles-b55df87f-e987-40fb-b645-f55fcc1f8b7f/fetchFileTemp4412217182379422083.tmp has been previously copied to /tmp/spark-5a1e6bf1-b7eb-4038-bb34-8002c9c6dbfc/userFiles-b55df87f-e987-40fb-b645-f55fcc1f8b7f/org.apache.kafka_kafka_2.11-0.8.2.1.jar\n",
            "23/04/12 23:13:04 INFO Executor: Adding file:/tmp/spark-5a1e6bf1-b7eb-4038-bb34-8002c9c6dbfc/userFiles-b55df87f-e987-40fb-b645-f55fcc1f8b7f/org.apache.kafka_kafka_2.11-0.8.2.1.jar to class loader\n",
            "23/04/12 23:13:04 INFO Executor: Fetching spark://8c2d110fc661:38227/jars/log4j_log4j-1.2.17.jar with timestamp 1681341181635\n",
            "23/04/12 23:13:04 INFO Utils: Fetching spark://8c2d110fc661:38227/jars/log4j_log4j-1.2.17.jar to /tmp/spark-5a1e6bf1-b7eb-4038-bb34-8002c9c6dbfc/userFiles-b55df87f-e987-40fb-b645-f55fcc1f8b7f/fetchFileTemp3443471934367985898.tmp\n",
            "23/04/12 23:13:04 INFO Utils: /tmp/spark-5a1e6bf1-b7eb-4038-bb34-8002c9c6dbfc/userFiles-b55df87f-e987-40fb-b645-f55fcc1f8b7f/fetchFileTemp3443471934367985898.tmp has been previously copied to /tmp/spark-5a1e6bf1-b7eb-4038-bb34-8002c9c6dbfc/userFiles-b55df87f-e987-40fb-b645-f55fcc1f8b7f/log4j_log4j-1.2.17.jar\n",
            "23/04/12 23:13:04 INFO Executor: Adding file:/tmp/spark-5a1e6bf1-b7eb-4038-bb34-8002c9c6dbfc/userFiles-b55df87f-e987-40fb-b645-f55fcc1f8b7f/log4j_log4j-1.2.17.jar to class loader\n",
            "23/04/12 23:13:04 INFO Executor: Fetching spark://8c2d110fc661:38227/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1681341181635\n",
            "23/04/12 23:13:04 INFO Utils: Fetching spark://8c2d110fc661:38227/jars/com.yammer.metrics_metrics-core-2.2.0.jar to /tmp/spark-5a1e6bf1-b7eb-4038-bb34-8002c9c6dbfc/userFiles-b55df87f-e987-40fb-b645-f55fcc1f8b7f/fetchFileTemp8639169588820116744.tmp\n",
            "23/04/12 23:13:04 INFO Utils: /tmp/spark-5a1e6bf1-b7eb-4038-bb34-8002c9c6dbfc/userFiles-b55df87f-e987-40fb-b645-f55fcc1f8b7f/fetchFileTemp8639169588820116744.tmp has been previously copied to /tmp/spark-5a1e6bf1-b7eb-4038-bb34-8002c9c6dbfc/userFiles-b55df87f-e987-40fb-b645-f55fcc1f8b7f/com.yammer.metrics_metrics-core-2.2.0.jar\n",
            "23/04/12 23:13:04 INFO Executor: Adding file:/tmp/spark-5a1e6bf1-b7eb-4038-bb34-8002c9c6dbfc/userFiles-b55df87f-e987-40fb-b645-f55fcc1f8b7f/com.yammer.metrics_metrics-core-2.2.0.jar to class loader\n",
            "23/04/12 23:13:04 INFO Executor: Fetching spark://8c2d110fc661:38227/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1681341181635\n",
            "23/04/12 23:13:04 INFO Utils: Fetching spark://8c2d110fc661:38227/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar to /tmp/spark-5a1e6bf1-b7eb-4038-bb34-8002c9c6dbfc/userFiles-b55df87f-e987-40fb-b645-f55fcc1f8b7f/fetchFileTemp8976140438710729010.tmp\n",
            "23/04/12 23:13:04 INFO Utils: /tmp/spark-5a1e6bf1-b7eb-4038-bb34-8002c9c6dbfc/userFiles-b55df87f-e987-40fb-b645-f55fcc1f8b7f/fetchFileTemp8976140438710729010.tmp has been previously copied to /tmp/spark-5a1e6bf1-b7eb-4038-bb34-8002c9c6dbfc/userFiles-b55df87f-e987-40fb-b645-f55fcc1f8b7f/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar\n",
            "23/04/12 23:13:04 INFO Executor: Adding file:/tmp/spark-5a1e6bf1-b7eb-4038-bb34-8002c9c6dbfc/userFiles-b55df87f-e987-40fb-b645-f55fcc1f8b7f/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar to class loader\n",
            "23/04/12 23:13:04 INFO Executor: Fetching spark://8c2d110fc661:38227/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1681341181635\n",
            "23/04/12 23:13:04 INFO Utils: Fetching spark://8c2d110fc661:38227/jars/org.slf4j_slf4j-api-1.7.16.jar to /tmp/spark-5a1e6bf1-b7eb-4038-bb34-8002c9c6dbfc/userFiles-b55df87f-e987-40fb-b645-f55fcc1f8b7f/fetchFileTemp6321753388122523345.tmp\n",
            "23/04/12 23:13:04 INFO Utils: /tmp/spark-5a1e6bf1-b7eb-4038-bb34-8002c9c6dbfc/userFiles-b55df87f-e987-40fb-b645-f55fcc1f8b7f/fetchFileTemp6321753388122523345.tmp has been previously copied to /tmp/spark-5a1e6bf1-b7eb-4038-bb34-8002c9c6dbfc/userFiles-b55df87f-e987-40fb-b645-f55fcc1f8b7f/org.slf4j_slf4j-api-1.7.16.jar\n",
            "23/04/12 23:13:04 INFO Executor: Adding file:/tmp/spark-5a1e6bf1-b7eb-4038-bb34-8002c9c6dbfc/userFiles-b55df87f-e987-40fb-b645-f55fcc1f8b7f/org.slf4j_slf4j-api-1.7.16.jar to class loader\n",
            "23/04/12 23:13:04 INFO Executor: Fetching spark://8c2d110fc661:38227/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1681341181635\n",
            "23/04/12 23:13:04 INFO Utils: Fetching spark://8c2d110fc661:38227/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-5a1e6bf1-b7eb-4038-bb34-8002c9c6dbfc/userFiles-b55df87f-e987-40fb-b645-f55fcc1f8b7f/fetchFileTemp4663794016909893454.tmp\n",
            "23/04/12 23:13:04 INFO Utils: /tmp/spark-5a1e6bf1-b7eb-4038-bb34-8002c9c6dbfc/userFiles-b55df87f-e987-40fb-b645-f55fcc1f8b7f/fetchFileTemp4663794016909893454.tmp has been previously copied to /tmp/spark-5a1e6bf1-b7eb-4038-bb34-8002c9c6dbfc/userFiles-b55df87f-e987-40fb-b645-f55fcc1f8b7f/org.spark-project.spark_unused-1.0.0.jar\n",
            "23/04/12 23:13:04 INFO Executor: Adding file:/tmp/spark-5a1e6bf1-b7eb-4038-bb34-8002c9c6dbfc/userFiles-b55df87f-e987-40fb-b645-f55fcc1f8b7f/org.spark-project.spark_unused-1.0.0.jar to class loader\n",
            "23/04/12 23:13:04 INFO Executor: Fetching spark://8c2d110fc661:38227/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1681341181635\n",
            "23/04/12 23:13:04 INFO Utils: Fetching spark://8c2d110fc661:38227/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar to /tmp/spark-5a1e6bf1-b7eb-4038-bb34-8002c9c6dbfc/userFiles-b55df87f-e987-40fb-b645-f55fcc1f8b7f/fetchFileTemp3675839553899709337.tmp\n",
            "23/04/12 23:13:04 INFO Utils: /tmp/spark-5a1e6bf1-b7eb-4038-bb34-8002c9c6dbfc/userFiles-b55df87f-e987-40fb-b645-f55fcc1f8b7f/fetchFileTemp3675839553899709337.tmp has been previously copied to /tmp/spark-5a1e6bf1-b7eb-4038-bb34-8002c9c6dbfc/userFiles-b55df87f-e987-40fb-b645-f55fcc1f8b7f/org.apache.kafka_kafka-clients-0.8.2.1.jar\n",
            "23/04/12 23:13:04 INFO Executor: Adding file:/tmp/spark-5a1e6bf1-b7eb-4038-bb34-8002c9c6dbfc/userFiles-b55df87f-e987-40fb-b645-f55fcc1f8b7f/org.apache.kafka_kafka-clients-0.8.2.1.jar to class loader\n",
            "23/04/12 23:13:04 INFO Executor: Fetching spark://8c2d110fc661:38227/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1681341181635\n",
            "23/04/12 23:13:04 INFO Utils: Fetching spark://8c2d110fc661:38227/jars/net.jpountz.lz4_lz4-1.2.0.jar to /tmp/spark-5a1e6bf1-b7eb-4038-bb34-8002c9c6dbfc/userFiles-b55df87f-e987-40fb-b645-f55fcc1f8b7f/fetchFileTemp1288014476607573864.tmp\n",
            "23/04/12 23:13:04 INFO Utils: /tmp/spark-5a1e6bf1-b7eb-4038-bb34-8002c9c6dbfc/userFiles-b55df87f-e987-40fb-b645-f55fcc1f8b7f/fetchFileTemp1288014476607573864.tmp has been previously copied to /tmp/spark-5a1e6bf1-b7eb-4038-bb34-8002c9c6dbfc/userFiles-b55df87f-e987-40fb-b645-f55fcc1f8b7f/net.jpountz.lz4_lz4-1.2.0.jar\n",
            "23/04/12 23:13:04 INFO Executor: Adding file:/tmp/spark-5a1e6bf1-b7eb-4038-bb34-8002c9c6dbfc/userFiles-b55df87f-e987-40fb-b645-f55fcc1f8b7f/net.jpountz.lz4_lz4-1.2.0.jar to class loader\n",
            "23/04/12 23:13:04 INFO Executor: Fetching spark://8c2d110fc661:38227/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1681341181635\n",
            "23/04/12 23:13:04 INFO Utils: Fetching spark://8c2d110fc661:38227/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar to /tmp/spark-5a1e6bf1-b7eb-4038-bb34-8002c9c6dbfc/userFiles-b55df87f-e987-40fb-b645-f55fcc1f8b7f/fetchFileTemp5869411888292655644.tmp\n",
            "23/04/12 23:13:04 INFO Utils: /tmp/spark-5a1e6bf1-b7eb-4038-bb34-8002c9c6dbfc/userFiles-b55df87f-e987-40fb-b645-f55fcc1f8b7f/fetchFileTemp5869411888292655644.tmp has been previously copied to /tmp/spark-5a1e6bf1-b7eb-4038-bb34-8002c9c6dbfc/userFiles-b55df87f-e987-40fb-b645-f55fcc1f8b7f/org.xerial.snappy_snappy-java-1.1.7.5.jar\n",
            "23/04/12 23:13:04 INFO Executor: Adding file:/tmp/spark-5a1e6bf1-b7eb-4038-bb34-8002c9c6dbfc/userFiles-b55df87f-e987-40fb-b645-f55fcc1f8b7f/org.xerial.snappy_snappy-java-1.1.7.5.jar to class loader\n",
            "23/04/12 23:13:04 INFO Executor: Fetching spark://8c2d110fc661:38227/jars/com.101tec_zkclient-0.3.jar with timestamp 1681341181635\n",
            "23/04/12 23:13:04 INFO Utils: Fetching spark://8c2d110fc661:38227/jars/com.101tec_zkclient-0.3.jar to /tmp/spark-5a1e6bf1-b7eb-4038-bb34-8002c9c6dbfc/userFiles-b55df87f-e987-40fb-b645-f55fcc1f8b7f/fetchFileTemp4501878137787804240.tmp\n",
            "23/04/12 23:13:04 INFO Utils: /tmp/spark-5a1e6bf1-b7eb-4038-bb34-8002c9c6dbfc/userFiles-b55df87f-e987-40fb-b645-f55fcc1f8b7f/fetchFileTemp4501878137787804240.tmp has been previously copied to /tmp/spark-5a1e6bf1-b7eb-4038-bb34-8002c9c6dbfc/userFiles-b55df87f-e987-40fb-b645-f55fcc1f8b7f/com.101tec_zkclient-0.3.jar\n",
            "23/04/12 23:13:04 INFO Executor: Adding file:/tmp/spark-5a1e6bf1-b7eb-4038-bb34-8002c9c6dbfc/userFiles-b55df87f-e987-40fb-b645-f55fcc1f8b7f/com.101tec_zkclient-0.3.jar to class loader\n",
            "23/04/12 23:13:04 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 41875.\n",
            "23/04/12 23:13:04 INFO NettyBlockTransferService: Server created on 8c2d110fc661:41875\n",
            "23/04/12 23:13:04 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "23/04/12 23:13:04 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8c2d110fc661, 41875, None)\n",
            "23/04/12 23:13:04 INFO BlockManagerMasterEndpoint: Registering block manager 8c2d110fc661:41875 with 366.3 MiB RAM, BlockManagerId(driver, 8c2d110fc661, 41875, None)\n",
            "23/04/12 23:13:04 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8c2d110fc661, 41875, None)\n",
            "23/04/12 23:13:04 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 8c2d110fc661, 41875, None)\n",
            "23/04/12 23:13:05 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 354.3 KiB, free 366.0 MiB)\n",
            "23/04/12 23:13:05 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 32.0 KiB, free 365.9 MiB)\n",
            "23/04/12 23:13:05 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 8c2d110fc661:41875 (size: 32.0 KiB, free: 366.3 MiB)\n",
            "23/04/12 23:13:05 INFO SparkContext: Created broadcast 0 from textFile at NativeMethodAccessorImpl.java:0\n",
            "23/04/12 23:13:06 INFO FileInputFormat: Total input files to process : 1\n",
            "23/04/12 23:13:06 INFO SparkContext: Starting job: runJob at PythonRDD.scala:166\n",
            "23/04/12 23:13:06 INFO DAGScheduler: Got job 0 (runJob at PythonRDD.scala:166) with 1 output partitions\n",
            "23/04/12 23:13:06 INFO DAGScheduler: Final stage: ResultStage 0 (runJob at PythonRDD.scala:166)\n",
            "23/04/12 23:13:06 INFO DAGScheduler: Parents of final stage: List()\n",
            "23/04/12 23:13:06 INFO DAGScheduler: Missing parents: List()\n",
            "23/04/12 23:13:06 INFO DAGScheduler: Submitting ResultStage 0 (PythonRDD[2] at RDD at PythonRDD.scala:53), which has no missing parents\n",
            "23/04/12 23:13:06 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 8.9 KiB, free 365.9 MiB)\n",
            "23/04/12 23:13:06 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.5 KiB, free 365.9 MiB)\n",
            "23/04/12 23:13:06 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 8c2d110fc661:41875 (size: 5.5 KiB, free: 366.3 MiB)\n",
            "23/04/12 23:13:06 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1478\n",
            "23/04/12 23:13:06 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (PythonRDD[2] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(0))\n",
            "23/04/12 23:13:06 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
            "23/04/12 23:13:07 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (8c2d110fc661, executor driver, partition 0, PROCESS_LOCAL, 4510 bytes) taskResourceAssignments Map()\n",
            "23/04/12 23:13:07 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
            "23/04/12 23:13:08 INFO HadoopRDD: Input split: file:/content/nashville-tweets-2019-01-28:0+12519625\n",
            "23/04/12 23:13:09 INFO PythonRunner: Times: total = 1030, boot = 988, init = 41, finish = 1\n",
            "23/04/12 23:13:09 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1448 bytes result sent to driver\n",
            "23/04/12 23:13:09 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 2851 ms on 8c2d110fc661 (executor driver) (1/1)\n",
            "23/04/12 23:13:09 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
            "23/04/12 23:13:09 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 42767\n",
            "23/04/12 23:13:09 INFO DAGScheduler: ResultStage 0 (runJob at PythonRDD.scala:166) finished in 3.379 s\n",
            "23/04/12 23:13:09 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "23/04/12 23:13:09 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
            "23/04/12 23:13:09 INFO DAGScheduler: Job 0 finished: runJob at PythonRDD.scala:166, took 3.586245 s\n",
            "[('correct', 1), ('correct', 1)]\n",
            "23/04/12 23:13:10 INFO deprecation: mapred.output.dir is deprecated. Instead, use mapreduce.output.fileoutputformat.outputdir\n",
            "23/04/12 23:13:10 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
            "23/04/12 23:13:10 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "23/04/12 23:13:10 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "23/04/12 23:13:10 INFO SparkContext: Starting job: runJob at SparkHadoopWriter.scala:83\n",
            "23/04/12 23:13:10 INFO DAGScheduler: Registering RDD 4 (reduceByKey at /content/1_count.py:41) as input to shuffle 1\n",
            "23/04/12 23:13:10 INFO DAGScheduler: Registering RDD 8 (coalesce at NativeMethodAccessorImpl.java:0) as input to shuffle 0\n",
            "23/04/12 23:13:10 INFO DAGScheduler: Got job 1 (runJob at SparkHadoopWriter.scala:83) with 1 output partitions\n",
            "23/04/12 23:13:10 INFO DAGScheduler: Final stage: ResultStage 3 (runJob at SparkHadoopWriter.scala:83)\n",
            "23/04/12 23:13:10 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)\n",
            "23/04/12 23:13:10 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 2)\n",
            "23/04/12 23:13:10 INFO DAGScheduler: Submitting ShuffleMapStage 1 (PairwiseRDD[4] at reduceByKey at /content/1_count.py:41), which has no missing parents\n",
            "23/04/12 23:13:10 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 12.6 KiB, free 365.9 MiB)\n",
            "23/04/12 23:13:10 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 365.9 MiB)\n",
            "23/04/12 23:13:10 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 8c2d110fc661:41875 (size: 7.7 KiB, free: 366.3 MiB)\n",
            "23/04/12 23:13:10 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1478\n",
            "23/04/12 23:13:10 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 1 (PairwiseRDD[4] at reduceByKey at /content/1_count.py:41) (first 15 tasks are for partitions Vector(0, 1))\n",
            "23/04/12 23:13:10 INFO TaskSchedulerImpl: Adding task set 1.0 with 2 tasks resource profile 0\n",
            "23/04/12 23:13:10 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (8c2d110fc661, executor driver, partition 0, PROCESS_LOCAL, 4499 bytes) taskResourceAssignments Map()\n",
            "23/04/12 23:13:10 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 2) (8c2d110fc661, executor driver, partition 1, PROCESS_LOCAL, 4499 bytes) taskResourceAssignments Map()\n",
            "23/04/12 23:13:10 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)\n",
            "23/04/12 23:13:10 INFO Executor: Running task 1.0 in stage 1.0 (TID 2)\n",
            "23/04/12 23:13:10 INFO HadoopRDD: Input split: file:/content/nashville-tweets-2019-01-28:0+12519625\n",
            "23/04/12 23:13:10 INFO HadoopRDD: Input split: file:/content/nashville-tweets-2019-01-28:12519625+12519626\n",
            "23/04/12 23:13:10 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 8c2d110fc661:41875 in memory (size: 5.5 KiB, free: 366.3 MiB)\n",
            "23/04/12 23:13:12 INFO PythonRunner: Times: total = 847, boot = 11, init = 19, finish = 817\n",
            "23/04/12 23:13:12 INFO PythonRunner: Times: total = 982, boot = 29, init = 36, finish = 917\n",
            "23/04/12 23:13:12 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1612 bytes result sent to driver\n",
            "23/04/12 23:13:12 INFO Executor: Finished task 1.0 in stage 1.0 (TID 2). 1655 bytes result sent to driver\n",
            "23/04/12 23:13:12 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 1555 ms on 8c2d110fc661 (executor driver) (1/2)\n",
            "23/04/12 23:13:12 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 2) in 1558 ms on 8c2d110fc661 (executor driver) (2/2)\n",
            "23/04/12 23:13:12 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
            "23/04/12 23:13:12 INFO DAGScheduler: ShuffleMapStage 1 (reduceByKey at /content/1_count.py:41) finished in 1.610 s\n",
            "23/04/12 23:13:12 INFO DAGScheduler: looking for newly runnable stages\n",
            "23/04/12 23:13:12 INFO DAGScheduler: running: Set()\n",
            "23/04/12 23:13:12 INFO DAGScheduler: waiting: Set(ShuffleMapStage 2, ResultStage 3)\n",
            "23/04/12 23:13:12 INFO DAGScheduler: failed: Set()\n",
            "23/04/12 23:13:12 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[8] at coalesce at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/04/12 23:13:12 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 10.6 KiB, free 365.9 MiB)\n",
            "23/04/12 23:13:12 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 6.2 KiB, free 365.9 MiB)\n",
            "23/04/12 23:13:12 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 8c2d110fc661:41875 (size: 6.2 KiB, free: 366.3 MiB)\n",
            "23/04/12 23:13:12 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1478\n",
            "23/04/12 23:13:12 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[8] at coalesce at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
            "23/04/12 23:13:12 INFO TaskSchedulerImpl: Adding task set 2.0 with 2 tasks resource profile 0\n",
            "23/04/12 23:13:12 INFO TaskSetManager: Starting task 1.0 in stage 2.0 (TID 3) (8c2d110fc661, executor driver, partition 1, NODE_LOCAL, 4260 bytes) taskResourceAssignments Map()\n",
            "23/04/12 23:13:12 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 4) (8c2d110fc661, executor driver, partition 0, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()\n",
            "23/04/12 23:13:12 INFO Executor: Running task 1.0 in stage 2.0 (TID 3)\n",
            "23/04/12 23:13:12 INFO Executor: Running task 0.0 in stage 2.0 (TID 4)\n",
            "23/04/12 23:13:12 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/12 23:13:12 INFO ShuffleBlockFetcherIterator: Getting 2 (160.0 B) non-empty blocks including 2 (160.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/12 23:13:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 19 ms\n",
            "23/04/12 23:13:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 21 ms\n",
            "23/04/12 23:13:12 INFO PythonRunner: Times: total = 47, boot = -748, init = 795, finish = 0\n",
            "23/04/12 23:13:12 INFO Executor: Finished task 0.0 in stage 2.0 (TID 4). 1654 bytes result sent to driver\n",
            "23/04/12 23:13:12 INFO PythonRunner: Times: total = 57, boot = -645, init = 701, finish = 1\n",
            "23/04/12 23:13:12 INFO Executor: Finished task 1.0 in stage 2.0 (TID 3). 1783 bytes result sent to driver\n",
            "23/04/12 23:13:12 INFO TaskSetManager: Finished task 1.0 in stage 2.0 (TID 3) in 178 ms on 8c2d110fc661 (executor driver) (1/2)\n",
            "23/04/12 23:13:12 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 4) in 184 ms on 8c2d110fc661 (executor driver) (2/2)\n",
            "23/04/12 23:13:12 INFO DAGScheduler: ShuffleMapStage 2 (coalesce at NativeMethodAccessorImpl.java:0) finished in 0.229 s\n",
            "23/04/12 23:13:12 INFO DAGScheduler: looking for newly runnable stages\n",
            "23/04/12 23:13:12 INFO DAGScheduler: running: Set()\n",
            "23/04/12 23:13:12 INFO DAGScheduler: waiting: Set(ResultStage 3)\n",
            "23/04/12 23:13:12 INFO DAGScheduler: failed: Set()\n",
            "23/04/12 23:13:12 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[14] at saveAsTextFile at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/04/12 23:13:12 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
            "23/04/12 23:13:12 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 105.9 KiB, free 365.8 MiB)\n",
            "23/04/12 23:13:12 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 39.6 KiB, free 365.7 MiB)\n",
            "23/04/12 23:13:12 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 8c2d110fc661:41875 (size: 39.6 KiB, free: 366.2 MiB)\n",
            "23/04/12 23:13:12 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1478\n",
            "23/04/12 23:13:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[14] at saveAsTextFile at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/04/12 23:13:12 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0\n",
            "23/04/12 23:13:12 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 5) (8c2d110fc661, executor driver, partition 0, NODE_LOCAL, 4547 bytes) taskResourceAssignments Map()\n",
            "23/04/12 23:13:12 INFO Executor: Running task 0.0 in stage 3.0 (TID 5)\n",
            "23/04/12 23:13:12 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
            "23/04/12 23:13:12 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "23/04/12 23:13:12 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "23/04/12 23:13:12 INFO ShuffleBlockFetcherIterator: Getting 1 (80.0 B) non-empty blocks including 1 (80.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/12 23:13:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
            "23/04/12 23:13:12 INFO PythonRunner: Times: total = 42, boot = -167, init = 209, finish = 0\n",
            "23/04/12 23:13:12 INFO FileOutputCommitter: Saved output of task 'attempt_202304122313102186825092691624496_0014_m_000000_0' to file:/content/1_count.out/_temporary/0/task_202304122313102186825092691624496_0014_m_000000\n",
            "23/04/12 23:13:12 INFO SparkHadoopMapRedUtil: attempt_202304122313102186825092691624496_0014_m_000000_0: Committed\n",
            "23/04/12 23:13:12 INFO Executor: Finished task 0.0 in stage 3.0 (TID 5). 1952 bytes result sent to driver\n",
            "23/04/12 23:13:12 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 5) in 214 ms on 8c2d110fc661 (executor driver) (1/1)\n",
            "23/04/12 23:13:12 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
            "23/04/12 23:13:12 INFO DAGScheduler: ResultStage 3 (runJob at SparkHadoopWriter.scala:83) finished in 0.253 s\n",
            "23/04/12 23:13:12 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "23/04/12 23:13:12 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished\n",
            "23/04/12 23:13:12 INFO DAGScheduler: Job 1 finished: runJob at SparkHadoopWriter.scala:83, took 2.163436 s\n",
            "23/04/12 23:13:12 INFO SparkHadoopWriter: Start to commit write Job job_202304122313102186825092691624496_0014.\n",
            "23/04/12 23:13:12 INFO SparkHadoopWriter: Write Job job_202304122313102186825092691624496_0014 committed. Elapsed time: 18 ms.\n",
            "23/04/12 23:13:12 INFO SparkUI: Stopped Spark web UI at http://8c2d110fc661:4040\n",
            "23/04/12 23:13:12 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "23/04/12 23:13:12 INFO MemoryStore: MemoryStore cleared\n",
            "23/04/12 23:13:12 INFO BlockManager: BlockManager stopped\n",
            "23/04/12 23:13:12 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "23/04/12 23:13:12 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "23/04/12 23:13:12 INFO SparkContext: Successfully stopped SparkContext\n",
            "23/04/12 23:13:13 INFO ShutdownHookManager: Shutdown hook called\n",
            "23/04/12 23:13:13 INFO ShutdownHookManager: Deleting directory /tmp/spark-5a1e6bf1-b7eb-4038-bb34-8002c9c6dbfc\n",
            "23/04/12 23:13:13 INFO ShutdownHookManager: Deleting directory /tmp/spark-5a1e6bf1-b7eb-4038-bb34-8002c9c6dbfc/pyspark-e0370c84-3992-40af-a6a4-86c8f108b9ba\n",
            "23/04/12 23:13:13 INFO ShutdownHookManager: Deleting directory /tmp/spark-3d41712e-08b5-42a5-9269-ca6900025702\n"
          ]
        }
      ],
      "source": [
        "# execute locally and ensure everything works. If it works you should get the 1_count.out/part-00000 file. \n",
        "!spark-submit --packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.4.7 1_count.py\n",
        "# note the cell magic command %%file 1_count.py is used to create a local copy of the content of cell as a file 1_count.py on colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TrkWukH-__Dz"
      },
      "outputs": [],
      "source": [
        "def test1(lines):\n",
        "  if '6294' in lines[0] and 'correct' in lines[0]:\n",
        "    print(\"passed\")\n",
        "  else:\n",
        "    print(\"failed\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BYMwjqpsn2Le",
        "outputId": "e08cdbd4-b8a0-41ce-de73-12f2f5c783c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[\"(('correct', 1), 6294)\\n\"]\n",
            "passed\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# test local execution results\n",
        "with open('1_count.out/part-00000') as f:\n",
        "  lines = f.readlines()\n",
        "  print(lines)\n",
        "  test1(lines)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jy9ZZT06__Dy"
      },
      "source": [
        "### Please save the output of each job as a single text file into your S3 bucket.\n",
        "\n",
        "Hint:\n",
        "\n",
        "1. You may call the **saveAsTextFile** function to populate the output file. \n",
        "2. Note spark may generate multiple output files due to partitioning, you can use the **repartition** or **coalesce** function to merge them to a single one.\n",
        "\n",
        "**You need to replace all s3 uri shown in below cells with yours.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3BPCSIi0__D0"
      },
      "source": [
        "## Job 2. Count the screen name with the most tweets and its counts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tvw5wFdq__D0",
        "outputId": "3d646bfa-09ff-4291-d62e-75064f5583ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing 2_group.py\n"
          ]
        }
      ],
      "source": [
        "%%file 2_group.py\n",
        "\n",
        "'''\n",
        "TODO:\n",
        "Count the screen name with the most tweets and its counts.\n",
        "\n",
        "See http://mike.teczno.com/notes/streaming-data-from-twitter.html for parsing info.\n",
        "Get the screen name by accessing tweet['user']['screen_name']\n",
        "\n",
        "Output:\n",
        "number_of_most_tweets    username\n",
        "\n",
        "'''\n",
        "\n",
        "\n",
        "from pyspark import SparkContext, SparkConf\n",
        "import json\n",
        "from operator import add\n",
        "\n",
        "def checkjson(entry):\n",
        "  try:\n",
        "    tweet = json.loads(entry)\n",
        "    screen_name = tweet['user']['screen_name']\n",
        "    return screen_name, 1\n",
        "  except:\n",
        "    pass\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  #conf = SparkConf().setAppName('1_count').set('spark.hadoop.validateOutputSpecs', False)\n",
        "  #sc = SparkContext(conf=conf).getOrCreate()\n",
        "  sc = SparkContext(appName=\"2_group\")\n",
        "\n",
        "\n",
        "  try:\n",
        "    tweets=sc.textFile('s3://vandy-bigdata-2/nashville-tweets-2019-01-28')\n",
        "    counts = tweets.map(lambda tweet: checkjson(tweet))\n",
        "    print(counts.take(2))\n",
        "\n",
        "    total = counts.map(lambda x: (x)).reduceByKey(lambda a, b: a + b)\n",
        "    max_tuple = total.max(key=lambda x: x[1])\n",
        "    print(max_tuple)\n",
        "    sc.parallelize([max_tuple]).saveAsTextFile(\"s3://vandy-bigdata-2/hw6/2_group.out\")\n",
        "\n",
        "  finally:\n",
        "    sc.stop()\n",
        "  \n",
        "\n",
        " \n",
        "  \n",
        "  pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xe2ZqP18Uso"
      },
      "source": [
        "### Execute and test locally"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tH6TK-Nk__D0",
        "outputId": "5b97e22f-47c6-460f-e228-13deb6c27de2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ":: loading settings :: url = jar:file:/content/spark-3.2.3-bin-hadoop3.2/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
            "Ivy Default Cache set to: /root/.ivy2/cache\n",
            "The jars for the packages stored in: /root/.ivy2/jars\n",
            "org.apache.spark#spark-streaming-kafka-0-8_2.11 added as a dependency\n",
            ":: resolving dependencies :: org.apache.spark#spark-submit-parent-f5912cf3-e1a4-4ea8-bd05-38c8033db672;1.0\n",
            "\tconfs: [default]\n",
            "\tfound org.apache.spark#spark-streaming-kafka-0-8_2.11;2.4.7 in central\n",
            "\tfound org.apache.kafka#kafka_2.11;0.8.2.1 in central\n",
            "\tfound org.scala-lang.modules#scala-xml_2.11;1.0.2 in central\n",
            "\tfound com.yammer.metrics#metrics-core;2.2.0 in central\n",
            "\tfound org.slf4j#slf4j-api;1.7.16 in central\n",
            "\tfound org.scala-lang.modules#scala-parser-combinators_2.11;1.1.0 in central\n",
            "\tfound com.101tec#zkclient;0.3 in central\n",
            "\tfound log4j#log4j;1.2.17 in central\n",
            "\tfound org.apache.kafka#kafka-clients;0.8.2.1 in central\n",
            "\tfound net.jpountz.lz4#lz4;1.2.0 in central\n",
            "\tfound org.xerial.snappy#snappy-java;1.1.7.5 in central\n",
            "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
            ":: resolution report :: resolve 1507ms :: artifacts dl 18ms\n",
            "\t:: modules in use:\n",
            "\tcom.101tec#zkclient;0.3 from central in [default]\n",
            "\tcom.yammer.metrics#metrics-core;2.2.0 from central in [default]\n",
            "\tlog4j#log4j;1.2.17 from central in [default]\n",
            "\tnet.jpountz.lz4#lz4;1.2.0 from central in [default]\n",
            "\torg.apache.kafka#kafka-clients;0.8.2.1 from central in [default]\n",
            "\torg.apache.kafka#kafka_2.11;0.8.2.1 from central in [default]\n",
            "\torg.apache.spark#spark-streaming-kafka-0-8_2.11;2.4.7 from central in [default]\n",
            "\torg.scala-lang.modules#scala-parser-combinators_2.11;1.1.0 from central in [default]\n",
            "\torg.scala-lang.modules#scala-xml_2.11;1.0.2 from central in [default]\n",
            "\torg.slf4j#slf4j-api;1.7.16 from central in [default]\n",
            "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
            "\torg.xerial.snappy#snappy-java;1.1.7.5 from central in [default]\n",
            "\t---------------------------------------------------------------------\n",
            "\t|                  |            modules            ||   artifacts   |\n",
            "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
            "\t---------------------------------------------------------------------\n",
            "\t|      default     |   12  |   0   |   0   |   0   ||   12  |   0   |\n",
            "\t---------------------------------------------------------------------\n",
            ":: retrieving :: org.apache.spark#spark-submit-parent-f5912cf3-e1a4-4ea8-bd05-38c8033db672\n",
            "\tconfs: [default]\n",
            "\t0 artifacts copied, 12 already retrieved (0kB/25ms)\n",
            "23/04/10 03:10:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
            "23/04/10 03:10:17 INFO SparkContext: Running Spark version 3.2.3\n",
            "23/04/10 03:10:17 INFO ResourceUtils: ==============================================================\n",
            "23/04/10 03:10:17 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "23/04/10 03:10:17 INFO ResourceUtils: ==============================================================\n",
            "23/04/10 03:10:17 INFO SparkContext: Submitted application: 1_count\n",
            "23/04/10 03:10:17 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "23/04/10 03:10:17 INFO ResourceProfile: Limiting resource is cpu\n",
            "23/04/10 03:10:17 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "23/04/10 03:10:17 INFO SecurityManager: Changing view acls to: root\n",
            "23/04/10 03:10:17 INFO SecurityManager: Changing modify acls to: root\n",
            "23/04/10 03:10:17 INFO SecurityManager: Changing view acls groups to: \n",
            "23/04/10 03:10:17 INFO SecurityManager: Changing modify acls groups to: \n",
            "23/04/10 03:10:17 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\n",
            "23/04/10 03:10:18 INFO Utils: Successfully started service 'sparkDriver' on port 43005.\n",
            "23/04/10 03:10:18 INFO SparkEnv: Registering MapOutputTracker\n",
            "23/04/10 03:10:18 INFO SparkEnv: Registering BlockManagerMaster\n",
            "23/04/10 03:10:18 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "23/04/10 03:10:18 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "23/04/10 03:10:18 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "23/04/10 03:10:18 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-387c554d-72c4-4a55-a347-5d50fcc93655\n",
            "23/04/10 03:10:18 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB\n",
            "23/04/10 03:10:18 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "23/04/10 03:10:19 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
            "23/04/10 03:10:19 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://e77b3fc47c45:4040\n",
            "23/04/10 03:10:19 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar at spark://e77b3fc47c45:43005/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1681096217749\n",
            "23/04/10 03:10:19 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar at spark://e77b3fc47c45:43005/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1681096217749\n",
            "23/04/10 03:10:19 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar at spark://e77b3fc47c45:43005/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1681096217749\n",
            "23/04/10 03:10:19 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar at spark://e77b3fc47c45:43005/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1681096217749\n",
            "23/04/10 03:10:19 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar at spark://e77b3fc47c45:43005/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1681096217749\n",
            "23/04/10 03:10:19 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar at spark://e77b3fc47c45:43005/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1681096217749\n",
            "23/04/10 03:10:19 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar at spark://e77b3fc47c45:43005/jars/com.101tec_zkclient-0.3.jar with timestamp 1681096217749\n",
            "23/04/10 03:10:19 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar at spark://e77b3fc47c45:43005/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1681096217749\n",
            "23/04/10 03:10:19 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar at spark://e77b3fc47c45:43005/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1681096217749\n",
            "23/04/10 03:10:19 INFO SparkContext: Added JAR file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar at spark://e77b3fc47c45:43005/jars/log4j_log4j-1.2.17.jar with timestamp 1681096217749\n",
            "23/04/10 03:10:19 INFO SparkContext: Added JAR file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar at spark://e77b3fc47c45:43005/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1681096217749\n",
            "23/04/10 03:10:19 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar at spark://e77b3fc47c45:43005/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1681096217749\n",
            "23/04/10 03:10:19 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar at file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1681096217749\n",
            "23/04/10 03:10:19 INFO Utils: Copying /root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar to /tmp/spark-12b0f44e-d31b-48b2-a540-92fc65be8a7c/userFiles-ebdca2e3-14f5-4d81-ae13-0aded0310a4b/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar\n",
            "23/04/10 03:10:19 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar at file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1681096217749\n",
            "23/04/10 03:10:19 INFO Utils: Copying /root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar to /tmp/spark-12b0f44e-d31b-48b2-a540-92fc65be8a7c/userFiles-ebdca2e3-14f5-4d81-ae13-0aded0310a4b/org.apache.kafka_kafka_2.11-0.8.2.1.jar\n",
            "23/04/10 03:10:19 INFO SparkContext: Added file file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar at file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1681096217749\n",
            "23/04/10 03:10:19 INFO Utils: Copying /root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-12b0f44e-d31b-48b2-a540-92fc65be8a7c/userFiles-ebdca2e3-14f5-4d81-ae13-0aded0310a4b/org.spark-project.spark_unused-1.0.0.jar\n",
            "23/04/10 03:10:19 INFO SparkContext: Added file file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar at file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1681096217749\n",
            "23/04/10 03:10:19 INFO Utils: Copying /root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar to /tmp/spark-12b0f44e-d31b-48b2-a540-92fc65be8a7c/userFiles-ebdca2e3-14f5-4d81-ae13-0aded0310a4b/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar\n",
            "23/04/10 03:10:19 INFO SparkContext: Added file file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar at file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1681096217749\n",
            "23/04/10 03:10:19 INFO Utils: Copying /root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar to /tmp/spark-12b0f44e-d31b-48b2-a540-92fc65be8a7c/userFiles-ebdca2e3-14f5-4d81-ae13-0aded0310a4b/com.yammer.metrics_metrics-core-2.2.0.jar\n",
            "23/04/10 03:10:19 INFO SparkContext: Added file file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar at file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1681096217749\n",
            "23/04/10 03:10:19 INFO Utils: Copying /root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar to /tmp/spark-12b0f44e-d31b-48b2-a540-92fc65be8a7c/userFiles-ebdca2e3-14f5-4d81-ae13-0aded0310a4b/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar\n",
            "23/04/10 03:10:19 INFO SparkContext: Added file file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar at file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar with timestamp 1681096217749\n",
            "23/04/10 03:10:19 INFO Utils: Copying /root/.ivy2/jars/com.101tec_zkclient-0.3.jar to /tmp/spark-12b0f44e-d31b-48b2-a540-92fc65be8a7c/userFiles-ebdca2e3-14f5-4d81-ae13-0aded0310a4b/com.101tec_zkclient-0.3.jar\n",
            "23/04/10 03:10:19 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar at file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1681096217749\n",
            "23/04/10 03:10:19 INFO Utils: Copying /root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar to /tmp/spark-12b0f44e-d31b-48b2-a540-92fc65be8a7c/userFiles-ebdca2e3-14f5-4d81-ae13-0aded0310a4b/org.apache.kafka_kafka-clients-0.8.2.1.jar\n",
            "23/04/10 03:10:19 INFO SparkContext: Added file file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar at file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1681096217749\n",
            "23/04/10 03:10:19 INFO Utils: Copying /root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar to /tmp/spark-12b0f44e-d31b-48b2-a540-92fc65be8a7c/userFiles-ebdca2e3-14f5-4d81-ae13-0aded0310a4b/org.slf4j_slf4j-api-1.7.16.jar\n",
            "23/04/10 03:10:19 INFO SparkContext: Added file file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar at file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar with timestamp 1681096217749\n",
            "23/04/10 03:10:19 INFO Utils: Copying /root/.ivy2/jars/log4j_log4j-1.2.17.jar to /tmp/spark-12b0f44e-d31b-48b2-a540-92fc65be8a7c/userFiles-ebdca2e3-14f5-4d81-ae13-0aded0310a4b/log4j_log4j-1.2.17.jar\n",
            "23/04/10 03:10:19 INFO SparkContext: Added file file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar at file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1681096217749\n",
            "23/04/10 03:10:19 INFO Utils: Copying /root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar to /tmp/spark-12b0f44e-d31b-48b2-a540-92fc65be8a7c/userFiles-ebdca2e3-14f5-4d81-ae13-0aded0310a4b/net.jpountz.lz4_lz4-1.2.0.jar\n",
            "23/04/10 03:10:19 INFO SparkContext: Added file file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar at file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1681096217749\n",
            "23/04/10 03:10:19 INFO Utils: Copying /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar to /tmp/spark-12b0f44e-d31b-48b2-a540-92fc65be8a7c/userFiles-ebdca2e3-14f5-4d81-ae13-0aded0310a4b/org.xerial.snappy_snappy-java-1.1.7.5.jar\n",
            "23/04/10 03:10:19 INFO Executor: Starting executor ID driver on host e77b3fc47c45\n",
            "23/04/10 03:10:19 INFO Executor: Fetching file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar with timestamp 1681096217749\n",
            "23/04/10 03:10:19 INFO Utils: /root/.ivy2/jars/log4j_log4j-1.2.17.jar has been previously copied to /tmp/spark-12b0f44e-d31b-48b2-a540-92fc65be8a7c/userFiles-ebdca2e3-14f5-4d81-ae13-0aded0310a4b/log4j_log4j-1.2.17.jar\n",
            "23/04/10 03:10:19 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1681096217749\n",
            "23/04/10 03:10:19 INFO Utils: /root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar has been previously copied to /tmp/spark-12b0f44e-d31b-48b2-a540-92fc65be8a7c/userFiles-ebdca2e3-14f5-4d81-ae13-0aded0310a4b/org.apache.kafka_kafka-clients-0.8.2.1.jar\n",
            "23/04/10 03:10:19 INFO Executor: Fetching file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1681096217749\n",
            "23/04/10 03:10:19 INFO Utils: /root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar has been previously copied to /tmp/spark-12b0f44e-d31b-48b2-a540-92fc65be8a7c/userFiles-ebdca2e3-14f5-4d81-ae13-0aded0310a4b/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar\n",
            "23/04/10 03:10:19 INFO Executor: Fetching file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1681096217749\n",
            "23/04/10 03:10:19 INFO Utils: /root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar has been previously copied to /tmp/spark-12b0f44e-d31b-48b2-a540-92fc65be8a7c/userFiles-ebdca2e3-14f5-4d81-ae13-0aded0310a4b/org.slf4j_slf4j-api-1.7.16.jar\n",
            "23/04/10 03:10:19 INFO Executor: Fetching file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1681096217749\n",
            "23/04/10 03:10:19 INFO Utils: /root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar has been previously copied to /tmp/spark-12b0f44e-d31b-48b2-a540-92fc65be8a7c/userFiles-ebdca2e3-14f5-4d81-ae13-0aded0310a4b/net.jpountz.lz4_lz4-1.2.0.jar\n",
            "23/04/10 03:10:19 INFO Executor: Fetching file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1681096217749\n",
            "23/04/10 03:10:19 INFO Utils: /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar has been previously copied to /tmp/spark-12b0f44e-d31b-48b2-a540-92fc65be8a7c/userFiles-ebdca2e3-14f5-4d81-ae13-0aded0310a4b/org.xerial.snappy_snappy-java-1.1.7.5.jar\n",
            "23/04/10 03:10:19 INFO Executor: Fetching file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1681096217749\n",
            "23/04/10 03:10:19 INFO Utils: /root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar has been previously copied to /tmp/spark-12b0f44e-d31b-48b2-a540-92fc65be8a7c/userFiles-ebdca2e3-14f5-4d81-ae13-0aded0310a4b/org.spark-project.spark_unused-1.0.0.jar\n",
            "23/04/10 03:10:19 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1681096217749\n",
            "23/04/10 03:10:19 INFO Utils: /root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar has been previously copied to /tmp/spark-12b0f44e-d31b-48b2-a540-92fc65be8a7c/userFiles-ebdca2e3-14f5-4d81-ae13-0aded0310a4b/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar\n",
            "23/04/10 03:10:19 INFO Executor: Fetching file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1681096217749\n",
            "23/04/10 03:10:19 INFO Utils: /root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar has been previously copied to /tmp/spark-12b0f44e-d31b-48b2-a540-92fc65be8a7c/userFiles-ebdca2e3-14f5-4d81-ae13-0aded0310a4b/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar\n",
            "23/04/10 03:10:19 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1681096217749\n",
            "23/04/10 03:10:19 INFO Utils: /root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar has been previously copied to /tmp/spark-12b0f44e-d31b-48b2-a540-92fc65be8a7c/userFiles-ebdca2e3-14f5-4d81-ae13-0aded0310a4b/org.apache.kafka_kafka_2.11-0.8.2.1.jar\n",
            "23/04/10 03:10:19 INFO Executor: Fetching file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar with timestamp 1681096217749\n",
            "23/04/10 03:10:19 INFO Utils: /root/.ivy2/jars/com.101tec_zkclient-0.3.jar has been previously copied to /tmp/spark-12b0f44e-d31b-48b2-a540-92fc65be8a7c/userFiles-ebdca2e3-14f5-4d81-ae13-0aded0310a4b/com.101tec_zkclient-0.3.jar\n",
            "23/04/10 03:10:19 INFO Executor: Fetching file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1681096217749\n",
            "23/04/10 03:10:19 INFO Utils: /root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar has been previously copied to /tmp/spark-12b0f44e-d31b-48b2-a540-92fc65be8a7c/userFiles-ebdca2e3-14f5-4d81-ae13-0aded0310a4b/com.yammer.metrics_metrics-core-2.2.0.jar\n",
            "23/04/10 03:10:19 INFO Executor: Fetching spark://e77b3fc47c45:43005/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1681096217749\n",
            "23/04/10 03:10:19 INFO TransportClientFactory: Successfully created connection to e77b3fc47c45/172.28.0.12:43005 after 58 ms (0 ms spent in bootstraps)\n",
            "23/04/10 03:10:19 INFO Utils: Fetching spark://e77b3fc47c45:43005/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar to /tmp/spark-12b0f44e-d31b-48b2-a540-92fc65be8a7c/userFiles-ebdca2e3-14f5-4d81-ae13-0aded0310a4b/fetchFileTemp7102877649691876810.tmp\n",
            "23/04/10 03:10:19 INFO Utils: /tmp/spark-12b0f44e-d31b-48b2-a540-92fc65be8a7c/userFiles-ebdca2e3-14f5-4d81-ae13-0aded0310a4b/fetchFileTemp7102877649691876810.tmp has been previously copied to /tmp/spark-12b0f44e-d31b-48b2-a540-92fc65be8a7c/userFiles-ebdca2e3-14f5-4d81-ae13-0aded0310a4b/org.apache.kafka_kafka-clients-0.8.2.1.jar\n",
            "23/04/10 03:10:19 INFO Executor: Adding file:/tmp/spark-12b0f44e-d31b-48b2-a540-92fc65be8a7c/userFiles-ebdca2e3-14f5-4d81-ae13-0aded0310a4b/org.apache.kafka_kafka-clients-0.8.2.1.jar to class loader\n",
            "23/04/10 03:10:19 INFO Executor: Fetching spark://e77b3fc47c45:43005/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1681096217749\n",
            "23/04/10 03:10:19 INFO Utils: Fetching spark://e77b3fc47c45:43005/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar to /tmp/spark-12b0f44e-d31b-48b2-a540-92fc65be8a7c/userFiles-ebdca2e3-14f5-4d81-ae13-0aded0310a4b/fetchFileTemp1775824629672992605.tmp\n",
            "23/04/10 03:10:20 INFO Utils: /tmp/spark-12b0f44e-d31b-48b2-a540-92fc65be8a7c/userFiles-ebdca2e3-14f5-4d81-ae13-0aded0310a4b/fetchFileTemp1775824629672992605.tmp has been previously copied to /tmp/spark-12b0f44e-d31b-48b2-a540-92fc65be8a7c/userFiles-ebdca2e3-14f5-4d81-ae13-0aded0310a4b/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar\n",
            "23/04/10 03:10:20 INFO Executor: Adding file:/tmp/spark-12b0f44e-d31b-48b2-a540-92fc65be8a7c/userFiles-ebdca2e3-14f5-4d81-ae13-0aded0310a4b/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar to class loader\n",
            "23/04/10 03:10:20 INFO Executor: Fetching spark://e77b3fc47c45:43005/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1681096217749\n",
            "23/04/10 03:10:20 INFO Utils: Fetching spark://e77b3fc47c45:43005/jars/com.yammer.metrics_metrics-core-2.2.0.jar to /tmp/spark-12b0f44e-d31b-48b2-a540-92fc65be8a7c/userFiles-ebdca2e3-14f5-4d81-ae13-0aded0310a4b/fetchFileTemp5566460557792601086.tmp\n",
            "23/04/10 03:10:20 INFO Utils: /tmp/spark-12b0f44e-d31b-48b2-a540-92fc65be8a7c/userFiles-ebdca2e3-14f5-4d81-ae13-0aded0310a4b/fetchFileTemp5566460557792601086.tmp has been previously copied to /tmp/spark-12b0f44e-d31b-48b2-a540-92fc65be8a7c/userFiles-ebdca2e3-14f5-4d81-ae13-0aded0310a4b/com.yammer.metrics_metrics-core-2.2.0.jar\n",
            "23/04/10 03:10:20 INFO Executor: Adding file:/tmp/spark-12b0f44e-d31b-48b2-a540-92fc65be8a7c/userFiles-ebdca2e3-14f5-4d81-ae13-0aded0310a4b/com.yammer.metrics_metrics-core-2.2.0.jar to class loader\n",
            "23/04/10 03:10:20 INFO Executor: Fetching spark://e77b3fc47c45:43005/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1681096217749\n",
            "23/04/10 03:10:20 INFO Utils: Fetching spark://e77b3fc47c45:43005/jars/org.slf4j_slf4j-api-1.7.16.jar to /tmp/spark-12b0f44e-d31b-48b2-a540-92fc65be8a7c/userFiles-ebdca2e3-14f5-4d81-ae13-0aded0310a4b/fetchFileTemp360109160503918848.tmp\n",
            "23/04/10 03:10:20 INFO Utils: /tmp/spark-12b0f44e-d31b-48b2-a540-92fc65be8a7c/userFiles-ebdca2e3-14f5-4d81-ae13-0aded0310a4b/fetchFileTemp360109160503918848.tmp has been previously copied to /tmp/spark-12b0f44e-d31b-48b2-a540-92fc65be8a7c/userFiles-ebdca2e3-14f5-4d81-ae13-0aded0310a4b/org.slf4j_slf4j-api-1.7.16.jar\n",
            "23/04/10 03:10:20 INFO Executor: Adding file:/tmp/spark-12b0f44e-d31b-48b2-a540-92fc65be8a7c/userFiles-ebdca2e3-14f5-4d81-ae13-0aded0310a4b/org.slf4j_slf4j-api-1.7.16.jar to class loader\n",
            "23/04/10 03:10:20 INFO Executor: Fetching spark://e77b3fc47c45:43005/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1681096217749\n",
            "23/04/10 03:10:20 INFO Utils: Fetching spark://e77b3fc47c45:43005/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar to /tmp/spark-12b0f44e-d31b-48b2-a540-92fc65be8a7c/userFiles-ebdca2e3-14f5-4d81-ae13-0aded0310a4b/fetchFileTemp7657407171218489451.tmp\n",
            "23/04/10 03:10:20 INFO Utils: /tmp/spark-12b0f44e-d31b-48b2-a540-92fc65be8a7c/userFiles-ebdca2e3-14f5-4d81-ae13-0aded0310a4b/fetchFileTemp7657407171218489451.tmp has been previously copied to /tmp/spark-12b0f44e-d31b-48b2-a540-92fc65be8a7c/userFiles-ebdca2e3-14f5-4d81-ae13-0aded0310a4b/org.apache.kafka_kafka_2.11-0.8.2.1.jar\n",
            "23/04/10 03:10:20 INFO Executor: Adding file:/tmp/spark-12b0f44e-d31b-48b2-a540-92fc65be8a7c/userFiles-ebdca2e3-14f5-4d81-ae13-0aded0310a4b/org.apache.kafka_kafka_2.11-0.8.2.1.jar to class loader\n",
            "23/04/10 03:10:20 INFO Executor: Fetching spark://e77b3fc47c45:43005/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1681096217749\n",
            "23/04/10 03:10:20 INFO Utils: Fetching spark://e77b3fc47c45:43005/jars/net.jpountz.lz4_lz4-1.2.0.jar to /tmp/spark-12b0f44e-d31b-48b2-a540-92fc65be8a7c/userFiles-ebdca2e3-14f5-4d81-ae13-0aded0310a4b/fetchFileTemp1296487743603112335.tmp\n",
            "23/04/10 03:10:20 INFO Utils: /tmp/spark-12b0f44e-d31b-48b2-a540-92fc65be8a7c/userFiles-ebdca2e3-14f5-4d81-ae13-0aded0310a4b/fetchFileTemp1296487743603112335.tmp has been previously copied to /tmp/spark-12b0f44e-d31b-48b2-a540-92fc65be8a7c/userFiles-ebdca2e3-14f5-4d81-ae13-0aded0310a4b/net.jpountz.lz4_lz4-1.2.0.jar\n",
            "23/04/10 03:10:20 INFO Executor: Adding file:/tmp/spark-12b0f44e-d31b-48b2-a540-92fc65be8a7c/userFiles-ebdca2e3-14f5-4d81-ae13-0aded0310a4b/net.jpountz.lz4_lz4-1.2.0.jar to class loader\n",
            "23/04/10 03:10:20 INFO Executor: Fetching spark://e77b3fc47c45:43005/jars/com.101tec_zkclient-0.3.jar with timestamp 1681096217749\n",
            "23/04/10 03:10:20 INFO Utils: Fetching spark://e77b3fc47c45:43005/jars/com.101tec_zkclient-0.3.jar to /tmp/spark-12b0f44e-d31b-48b2-a540-92fc65be8a7c/userFiles-ebdca2e3-14f5-4d81-ae13-0aded0310a4b/fetchFileTemp1410559581546843249.tmp\n",
            "23/04/10 03:10:20 INFO Utils: /tmp/spark-12b0f44e-d31b-48b2-a540-92fc65be8a7c/userFiles-ebdca2e3-14f5-4d81-ae13-0aded0310a4b/fetchFileTemp1410559581546843249.tmp has been previously copied to /tmp/spark-12b0f44e-d31b-48b2-a540-92fc65be8a7c/userFiles-ebdca2e3-14f5-4d81-ae13-0aded0310a4b/com.101tec_zkclient-0.3.jar\n",
            "23/04/10 03:10:20 INFO Executor: Adding file:/tmp/spark-12b0f44e-d31b-48b2-a540-92fc65be8a7c/userFiles-ebdca2e3-14f5-4d81-ae13-0aded0310a4b/com.101tec_zkclient-0.3.jar to class loader\n",
            "23/04/10 03:10:20 INFO Executor: Fetching spark://e77b3fc47c45:43005/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1681096217749\n",
            "23/04/10 03:10:20 INFO Utils: Fetching spark://e77b3fc47c45:43005/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar to /tmp/spark-12b0f44e-d31b-48b2-a540-92fc65be8a7c/userFiles-ebdca2e3-14f5-4d81-ae13-0aded0310a4b/fetchFileTemp1817708846455861208.tmp\n",
            "23/04/10 03:10:20 INFO Utils: /tmp/spark-12b0f44e-d31b-48b2-a540-92fc65be8a7c/userFiles-ebdca2e3-14f5-4d81-ae13-0aded0310a4b/fetchFileTemp1817708846455861208.tmp has been previously copied to /tmp/spark-12b0f44e-d31b-48b2-a540-92fc65be8a7c/userFiles-ebdca2e3-14f5-4d81-ae13-0aded0310a4b/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar\n",
            "23/04/10 03:10:20 INFO Executor: Adding file:/tmp/spark-12b0f44e-d31b-48b2-a540-92fc65be8a7c/userFiles-ebdca2e3-14f5-4d81-ae13-0aded0310a4b/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar to class loader\n",
            "23/04/10 03:10:20 INFO Executor: Fetching spark://e77b3fc47c45:43005/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1681096217749\n",
            "23/04/10 03:10:20 INFO Utils: Fetching spark://e77b3fc47c45:43005/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar to /tmp/spark-12b0f44e-d31b-48b2-a540-92fc65be8a7c/userFiles-ebdca2e3-14f5-4d81-ae13-0aded0310a4b/fetchFileTemp6529258773345545793.tmp\n",
            "23/04/10 03:10:20 INFO Utils: /tmp/spark-12b0f44e-d31b-48b2-a540-92fc65be8a7c/userFiles-ebdca2e3-14f5-4d81-ae13-0aded0310a4b/fetchFileTemp6529258773345545793.tmp has been previously copied to /tmp/spark-12b0f44e-d31b-48b2-a540-92fc65be8a7c/userFiles-ebdca2e3-14f5-4d81-ae13-0aded0310a4b/org.xerial.snappy_snappy-java-1.1.7.5.jar\n",
            "23/04/10 03:10:20 INFO Executor: Adding file:/tmp/spark-12b0f44e-d31b-48b2-a540-92fc65be8a7c/userFiles-ebdca2e3-14f5-4d81-ae13-0aded0310a4b/org.xerial.snappy_snappy-java-1.1.7.5.jar to class loader\n",
            "23/04/10 03:10:20 INFO Executor: Fetching spark://e77b3fc47c45:43005/jars/log4j_log4j-1.2.17.jar with timestamp 1681096217749\n",
            "23/04/10 03:10:20 INFO Utils: Fetching spark://e77b3fc47c45:43005/jars/log4j_log4j-1.2.17.jar to /tmp/spark-12b0f44e-d31b-48b2-a540-92fc65be8a7c/userFiles-ebdca2e3-14f5-4d81-ae13-0aded0310a4b/fetchFileTemp5250713497614100392.tmp\n",
            "23/04/10 03:10:20 INFO Utils: /tmp/spark-12b0f44e-d31b-48b2-a540-92fc65be8a7c/userFiles-ebdca2e3-14f5-4d81-ae13-0aded0310a4b/fetchFileTemp5250713497614100392.tmp has been previously copied to /tmp/spark-12b0f44e-d31b-48b2-a540-92fc65be8a7c/userFiles-ebdca2e3-14f5-4d81-ae13-0aded0310a4b/log4j_log4j-1.2.17.jar\n",
            "23/04/10 03:10:20 INFO Executor: Adding file:/tmp/spark-12b0f44e-d31b-48b2-a540-92fc65be8a7c/userFiles-ebdca2e3-14f5-4d81-ae13-0aded0310a4b/log4j_log4j-1.2.17.jar to class loader\n",
            "23/04/10 03:10:20 INFO Executor: Fetching spark://e77b3fc47c45:43005/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1681096217749\n",
            "23/04/10 03:10:20 INFO Utils: Fetching spark://e77b3fc47c45:43005/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-12b0f44e-d31b-48b2-a540-92fc65be8a7c/userFiles-ebdca2e3-14f5-4d81-ae13-0aded0310a4b/fetchFileTemp182677384746708001.tmp\n",
            "23/04/10 03:10:20 INFO Utils: /tmp/spark-12b0f44e-d31b-48b2-a540-92fc65be8a7c/userFiles-ebdca2e3-14f5-4d81-ae13-0aded0310a4b/fetchFileTemp182677384746708001.tmp has been previously copied to /tmp/spark-12b0f44e-d31b-48b2-a540-92fc65be8a7c/userFiles-ebdca2e3-14f5-4d81-ae13-0aded0310a4b/org.spark-project.spark_unused-1.0.0.jar\n",
            "23/04/10 03:10:20 INFO Executor: Adding file:/tmp/spark-12b0f44e-d31b-48b2-a540-92fc65be8a7c/userFiles-ebdca2e3-14f5-4d81-ae13-0aded0310a4b/org.spark-project.spark_unused-1.0.0.jar to class loader\n",
            "23/04/10 03:10:20 INFO Executor: Fetching spark://e77b3fc47c45:43005/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1681096217749\n",
            "23/04/10 03:10:20 INFO Utils: Fetching spark://e77b3fc47c45:43005/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar to /tmp/spark-12b0f44e-d31b-48b2-a540-92fc65be8a7c/userFiles-ebdca2e3-14f5-4d81-ae13-0aded0310a4b/fetchFileTemp1503678614670746479.tmp\n",
            "23/04/10 03:10:20 INFO Utils: /tmp/spark-12b0f44e-d31b-48b2-a540-92fc65be8a7c/userFiles-ebdca2e3-14f5-4d81-ae13-0aded0310a4b/fetchFileTemp1503678614670746479.tmp has been previously copied to /tmp/spark-12b0f44e-d31b-48b2-a540-92fc65be8a7c/userFiles-ebdca2e3-14f5-4d81-ae13-0aded0310a4b/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar\n",
            "23/04/10 03:10:20 INFO Executor: Adding file:/tmp/spark-12b0f44e-d31b-48b2-a540-92fc65be8a7c/userFiles-ebdca2e3-14f5-4d81-ae13-0aded0310a4b/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar to class loader\n",
            "23/04/10 03:10:20 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38225.\n",
            "23/04/10 03:10:20 INFO NettyBlockTransferService: Server created on e77b3fc47c45:38225\n",
            "23/04/10 03:10:20 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "23/04/10 03:10:20 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, e77b3fc47c45, 38225, None)\n",
            "23/04/10 03:10:20 INFO BlockManagerMasterEndpoint: Registering block manager e77b3fc47c45:38225 with 366.3 MiB RAM, BlockManagerId(driver, e77b3fc47c45, 38225, None)\n",
            "23/04/10 03:10:20 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, e77b3fc47c45, 38225, None)\n",
            "23/04/10 03:10:20 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, e77b3fc47c45, 38225, None)\n",
            "23/04/10 03:10:21 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 354.3 KiB, free 366.0 MiB)\n",
            "23/04/10 03:10:21 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 32.0 KiB, free 365.9 MiB)\n",
            "23/04/10 03:10:21 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on e77b3fc47c45:38225 (size: 32.0 KiB, free: 366.3 MiB)\n",
            "23/04/10 03:10:21 INFO SparkContext: Created broadcast 0 from textFile at NativeMethodAccessorImpl.java:0\n",
            "23/04/10 03:10:22 INFO FileInputFormat: Total input files to process : 1\n",
            "23/04/10 03:10:22 INFO SparkContext: Starting job: runJob at PythonRDD.scala:166\n",
            "23/04/10 03:10:22 INFO DAGScheduler: Got job 0 (runJob at PythonRDD.scala:166) with 1 output partitions\n",
            "23/04/10 03:10:22 INFO DAGScheduler: Final stage: ResultStage 0 (runJob at PythonRDD.scala:166)\n",
            "23/04/10 03:10:22 INFO DAGScheduler: Parents of final stage: List()\n",
            "23/04/10 03:10:22 INFO DAGScheduler: Missing parents: List()\n",
            "23/04/10 03:10:22 INFO DAGScheduler: Submitting ResultStage 0 (PythonRDD[2] at RDD at PythonRDD.scala:53), which has no missing parents\n",
            "23/04/10 03:10:22 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 8.9 KiB, free 365.9 MiB)\n",
            "23/04/10 03:10:22 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.5 KiB, free 365.9 MiB)\n",
            "23/04/10 03:10:22 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on e77b3fc47c45:38225 (size: 5.5 KiB, free: 366.3 MiB)\n",
            "23/04/10 03:10:22 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1478\n",
            "23/04/10 03:10:22 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (PythonRDD[2] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(0))\n",
            "23/04/10 03:10:22 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
            "23/04/10 03:10:22 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (e77b3fc47c45, executor driver, partition 0, PROCESS_LOCAL, 4510 bytes) taskResourceAssignments Map()\n",
            "23/04/10 03:10:22 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
            "23/04/10 03:10:23 INFO HadoopRDD: Input split: file:/content/nashville-tweets-2019-01-28:0+12519625\n",
            "23/04/10 03:10:24 INFO PythonRunner: Times: total = 835, boot = 801, init = 33, finish = 1\n",
            "23/04/10 03:10:24 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1462 bytes result sent to driver\n",
            "23/04/10 03:10:24 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1716 ms on e77b3fc47c45 (executor driver) (1/1)\n",
            "23/04/10 03:10:24 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
            "23/04/10 03:10:24 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 41753\n",
            "23/04/10 03:10:24 INFO DAGScheduler: ResultStage 0 (runJob at PythonRDD.scala:166) finished in 2.144 s\n",
            "23/04/10 03:10:24 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "23/04/10 03:10:24 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
            "23/04/10 03:10:24 INFO DAGScheduler: Job 0 finished: runJob at PythonRDD.scala:166, took 2.267590 s\n",
            "[('RandallCantrel4', 1), ('LakerMike2416', 1)]\n",
            "23/04/10 03:10:24 INFO SparkContext: Starting job: max at /content/2_group.py:50\n",
            "23/04/10 03:10:24 INFO DAGScheduler: Registering RDD 4 (reduceByKey at /content/2_group.py:49) as input to shuffle 0\n",
            "23/04/10 03:10:24 INFO DAGScheduler: Got job 1 (max at /content/2_group.py:50) with 2 output partitions\n",
            "23/04/10 03:10:24 INFO DAGScheduler: Final stage: ResultStage 2 (max at /content/2_group.py:50)\n",
            "23/04/10 03:10:24 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 1)\n",
            "23/04/10 03:10:24 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 1)\n",
            "23/04/10 03:10:24 INFO DAGScheduler: Submitting ShuffleMapStage 1 (PairwiseRDD[4] at reduceByKey at /content/2_group.py:49), which has no missing parents\n",
            "23/04/10 03:10:24 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 12.6 KiB, free 365.9 MiB)\n",
            "23/04/10 03:10:24 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 365.9 MiB)\n",
            "23/04/10 03:10:24 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on e77b3fc47c45:38225 (size: 7.7 KiB, free: 366.3 MiB)\n",
            "23/04/10 03:10:24 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1478\n",
            "23/04/10 03:10:24 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 1 (PairwiseRDD[4] at reduceByKey at /content/2_group.py:49) (first 15 tasks are for partitions Vector(0, 1))\n",
            "23/04/10 03:10:24 INFO TaskSchedulerImpl: Adding task set 1.0 with 2 tasks resource profile 0\n",
            "23/04/10 03:10:24 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (e77b3fc47c45, executor driver, partition 0, PROCESS_LOCAL, 4499 bytes) taskResourceAssignments Map()\n",
            "23/04/10 03:10:24 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 2) (e77b3fc47c45, executor driver, partition 1, PROCESS_LOCAL, 4499 bytes) taskResourceAssignments Map()\n",
            "23/04/10 03:10:24 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)\n",
            "23/04/10 03:10:24 INFO Executor: Running task 1.0 in stage 1.0 (TID 2)\n",
            "23/04/10 03:10:24 INFO HadoopRDD: Input split: file:/content/nashville-tweets-2019-01-28:12519625+12519626\n",
            "23/04/10 03:10:24 INFO HadoopRDD: Input split: file:/content/nashville-tweets-2019-01-28:0+12519625\n",
            "23/04/10 03:10:26 INFO PythonRunner: Times: total = 1022, boot = 26, init = 12, finish = 984\n",
            "23/04/10 03:10:26 INFO PythonRunner: Times: total = 956, boot = 15, init = 32, finish = 909\n",
            "23/04/10 03:10:26 INFO Executor: Finished task 1.0 in stage 1.0 (TID 2). 1655 bytes result sent to driver\n",
            "23/04/10 03:10:26 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1655 bytes result sent to driver\n",
            "23/04/10 03:10:26 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 1618 ms on e77b3fc47c45 (executor driver) (1/2)\n",
            "23/04/10 03:10:26 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 2) in 1622 ms on e77b3fc47c45 (executor driver) (2/2)\n",
            "23/04/10 03:10:26 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
            "23/04/10 03:10:26 INFO DAGScheduler: ShuffleMapStage 1 (reduceByKey at /content/2_group.py:49) finished in 1.667 s\n",
            "23/04/10 03:10:26 INFO DAGScheduler: looking for newly runnable stages\n",
            "23/04/10 03:10:26 INFO DAGScheduler: running: Set()\n",
            "23/04/10 03:10:26 INFO DAGScheduler: waiting: Set(ResultStage 2)\n",
            "23/04/10 03:10:26 INFO DAGScheduler: failed: Set()\n",
            "23/04/10 03:10:26 INFO DAGScheduler: Submitting ResultStage 2 (PythonRDD[7] at max at /content/2_group.py:50), which has no missing parents\n",
            "23/04/10 03:10:26 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 10.8 KiB, free 365.9 MiB)\n",
            "23/04/10 03:10:26 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 365.9 MiB)\n",
            "23/04/10 03:10:26 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on e77b3fc47c45:38225 (size: 6.3 KiB, free: 366.2 MiB)\n",
            "23/04/10 03:10:26 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1478\n",
            "23/04/10 03:10:26 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 2 (PythonRDD[7] at max at /content/2_group.py:50) (first 15 tasks are for partitions Vector(0, 1))\n",
            "23/04/10 03:10:26 INFO TaskSchedulerImpl: Adding task set 2.0 with 2 tasks resource profile 0\n",
            "23/04/10 03:10:26 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 3) (e77b3fc47c45, executor driver, partition 0, NODE_LOCAL, 4271 bytes) taskResourceAssignments Map()\n",
            "23/04/10 03:10:26 INFO TaskSetManager: Starting task 1.0 in stage 2.0 (TID 4) (e77b3fc47c45, executor driver, partition 1, NODE_LOCAL, 4271 bytes) taskResourceAssignments Map()\n",
            "23/04/10 03:10:26 INFO Executor: Running task 0.0 in stage 2.0 (TID 3)\n",
            "23/04/10 03:10:26 INFO Executor: Running task 1.0 in stage 2.0 (TID 4)\n",
            "23/04/10 03:10:26 INFO ShuffleBlockFetcherIterator: Getting 2 (21.2 KiB) non-empty blocks including 2 (21.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/10 03:10:26 INFO ShuffleBlockFetcherIterator: Getting 2 (21.2 KiB) non-empty blocks including 2 (21.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/10 03:10:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 14 ms\n",
            "23/04/10 03:10:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 25 ms\n",
            "23/04/10 03:10:26 INFO PythonRunner: Times: total = 48, boot = -627, init = 671, finish = 4\n",
            "23/04/10 03:10:26 INFO Executor: Finished task 0.0 in stage 2.0 (TID 3). 1627 bytes result sent to driver\n",
            "23/04/10 03:10:26 INFO PythonRunner: Times: total = 59, boot = -677, init = 732, finish = 4\n",
            "23/04/10 03:10:26 INFO Executor: Finished task 1.0 in stage 2.0 (TID 4). 1623 bytes result sent to driver\n",
            "23/04/10 03:10:26 INFO TaskSetManager: Finished task 1.0 in stage 2.0 (TID 4) in 163 ms on e77b3fc47c45 (executor driver) (1/2)\n",
            "23/04/10 03:10:26 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 3) in 182 ms on e77b3fc47c45 (executor driver) (2/2)\n",
            "23/04/10 03:10:26 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
            "23/04/10 03:10:26 INFO DAGScheduler: ResultStage 2 (max at /content/2_group.py:50) finished in 0.211 s\n",
            "23/04/10 03:10:26 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "23/04/10 03:10:26 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished\n",
            "23/04/10 03:10:26 INFO DAGScheduler: Job 1 finished: max at /content/2_group.py:50, took 1.929906 s\n",
            "('rpsabo', 88)\n",
            "23/04/10 03:10:26 INFO deprecation: mapred.output.dir is deprecated. Instead, use mapreduce.output.fileoutputformat.outputdir\n",
            "23/04/10 03:10:26 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
            "23/04/10 03:10:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "23/04/10 03:10:26 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "23/04/10 03:10:26 INFO SparkContext: Starting job: runJob at SparkHadoopWriter.scala:83\n",
            "23/04/10 03:10:26 INFO DAGScheduler: Got job 2 (runJob at SparkHadoopWriter.scala:83) with 2 output partitions\n",
            "23/04/10 03:10:26 INFO DAGScheduler: Final stage: ResultStage 3 (runJob at SparkHadoopWriter.scala:83)\n",
            "23/04/10 03:10:26 INFO DAGScheduler: Parents of final stage: List()\n",
            "23/04/10 03:10:26 INFO DAGScheduler: Missing parents: List()\n",
            "23/04/10 03:10:26 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[11] at saveAsTextFile at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/04/10 03:10:26 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 103.4 KiB, free 365.8 MiB)\n",
            "23/04/10 03:10:26 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 38.1 KiB, free 365.7 MiB)\n",
            "23/04/10 03:10:26 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on e77b3fc47c45:38225 (size: 38.1 KiB, free: 366.2 MiB)\n",
            "23/04/10 03:10:26 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1478\n",
            "23/04/10 03:10:26 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 3 (MapPartitionsRDD[11] at saveAsTextFile at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
            "23/04/10 03:10:26 INFO TaskSchedulerImpl: Adding task set 3.0 with 2 tasks resource profile 0\n",
            "23/04/10 03:10:26 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 5) (e77b3fc47c45, executor driver, partition 0, PROCESS_LOCAL, 4433 bytes) taskResourceAssignments Map()\n",
            "23/04/10 03:10:26 INFO TaskSetManager: Starting task 1.0 in stage 3.0 (TID 6) (e77b3fc47c45, executor driver, partition 1, PROCESS_LOCAL, 4471 bytes) taskResourceAssignments Map()\n",
            "23/04/10 03:10:26 INFO Executor: Running task 0.0 in stage 3.0 (TID 5)\n",
            "23/04/10 03:10:26 INFO Executor: Running task 1.0 in stage 3.0 (TID 6)\n",
            "23/04/10 03:10:26 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
            "23/04/10 03:10:26 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
            "23/04/10 03:10:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "23/04/10 03:10:26 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "23/04/10 03:10:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "23/04/10 03:10:26 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "23/04/10 03:10:26 INFO PythonRunner: Times: total = 43, boot = -295, init = 338, finish = 0\n",
            "23/04/10 03:10:26 INFO FileOutputCommitter: Saved output of task 'attempt_202304100310263385440263815417053_0011_m_000001_0' to file:/content/2_group.out/_temporary/0/task_202304100310263385440263815417053_0011_m_000001\n",
            "23/04/10 03:10:26 INFO SparkHadoopMapRedUtil: attempt_202304100310263385440263815417053_0011_m_000001_0: Committed\n",
            "23/04/10 03:10:26 INFO PythonRunner: Times: total = 47, boot = -310, init = 357, finish = 0\n",
            "23/04/10 03:10:26 INFO FileOutputCommitter: Saved output of task 'attempt_202304100310263385440263815417053_0011_m_000000_0' to file:/content/2_group.out/_temporary/0/task_202304100310263385440263815417053_0011_m_000000\n",
            "23/04/10 03:10:26 INFO SparkHadoopMapRedUtil: attempt_202304100310263385440263815417053_0011_m_000000_0: Committed\n",
            "23/04/10 03:10:26 INFO Executor: Finished task 1.0 in stage 3.0 (TID 6). 1608 bytes result sent to driver\n",
            "23/04/10 03:10:26 INFO Executor: Finished task 0.0 in stage 3.0 (TID 5). 1565 bytes result sent to driver\n",
            "23/04/10 03:10:26 INFO TaskSetManager: Finished task 1.0 in stage 3.0 (TID 6) in 215 ms on e77b3fc47c45 (executor driver) (1/2)\n",
            "23/04/10 03:10:26 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 5) in 223 ms on e77b3fc47c45 (executor driver) (2/2)\n",
            "23/04/10 03:10:26 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
            "23/04/10 03:10:26 INFO DAGScheduler: ResultStage 3 (runJob at SparkHadoopWriter.scala:83) finished in 0.276 s\n",
            "23/04/10 03:10:26 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "23/04/10 03:10:26 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished\n",
            "23/04/10 03:10:26 INFO DAGScheduler: Job 2 finished: runJob at SparkHadoopWriter.scala:83, took 0.284236 s\n",
            "23/04/10 03:10:26 INFO SparkHadoopWriter: Start to commit write Job job_202304100310263385440263815417053_0011.\n",
            "23/04/10 03:10:26 INFO SparkHadoopWriter: Write Job job_202304100310263385440263815417053_0011 committed. Elapsed time: 18 ms.\n",
            "23/04/10 03:10:26 INFO SparkUI: Stopped Spark web UI at http://e77b3fc47c45:4040\n",
            "23/04/10 03:10:26 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "23/04/10 03:10:27 INFO MemoryStore: MemoryStore cleared\n",
            "23/04/10 03:10:27 INFO BlockManager: BlockManager stopped\n",
            "23/04/10 03:10:27 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "23/04/10 03:10:27 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "23/04/10 03:10:27 INFO SparkContext: Successfully stopped SparkContext\n",
            "23/04/10 03:10:27 INFO ShutdownHookManager: Shutdown hook called\n",
            "23/04/10 03:10:27 INFO ShutdownHookManager: Deleting directory /tmp/spark-5d0805fa-95a8-4be5-9cc3-ba34f59b71a8\n",
            "23/04/10 03:10:27 INFO ShutdownHookManager: Deleting directory /tmp/spark-12b0f44e-d31b-48b2-a540-92fc65be8a7c\n",
            "23/04/10 03:10:27 INFO ShutdownHookManager: Deleting directory /tmp/spark-12b0f44e-d31b-48b2-a540-92fc65be8a7c/pyspark-44248f98-3daf-443a-8816-76d6c615c59d\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!spark-submit --packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.4.7 2_group.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TjMRs2KYo0Hd"
      },
      "outputs": [],
      "source": [
        "def test2(lines):\n",
        "    assert lines[0].strip() == \"('rpsabo', 88)\"\n",
        "    print(\"passed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nvpqT474__D0",
        "outputId": "77cc0e13-df51-4977-89d0-5e4d351ef2f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "passed\n"
          ]
        }
      ],
      "source": [
        "# test local execution results\n",
        "with open('2_group.out/part-00001') as f:\n",
        "  lines = f.readlines()\n",
        "  test2(lines)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DeAaRlMR__D1"
      },
      "source": [
        "## Job 3. Count the tweets per day."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fTMTHj9O__D2",
        "outputId": "626abe57-946f-4098-ba8d-c282c3bdf36f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing 3_days.py\n"
          ]
        }
      ],
      "source": [
        "%%file 3_days.py\n",
        "\n",
        "'''\n",
        "TODO:\n",
        "Count the tweets per day.\n",
        "\n",
        "See http://mike.teczno.com/notes/streaming-data-from-twitter.html for parsing info.\n",
        "Get the screen name by accessing tweet['user']['screen_name']\n",
        "\n",
        "Look at tweet['created_at'] for datetime of creation. Just use the first word in the date to get the day.\n",
        "\n",
        "'''\n",
        "\n",
        "from pyspark import SparkContext, SparkConf\n",
        "import json\n",
        "from operator import add\n",
        "\n",
        "def checkjson(entry):\n",
        "  try:\n",
        "    tweet = json.loads(entry)\n",
        "    date = tweet['created_at']\n",
        "    day = date.split(' ', 1)[0]\n",
        "    return day, 1   \n",
        "  except:\n",
        "    pass\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  #conf = SparkConf().setAppName('1_count').set('spark.hadoop.validateOutputSpecs', False)\n",
        "  #sc = SparkContext(conf=conf).getOrCreate()\n",
        "  sc = SparkContext(appName=\"3_days\")\n",
        "\n",
        "\n",
        "  try:\n",
        "    tweets=sc.textFile('s3://vandy-bigdata-2/nashville-tweets-2019-01-28')\n",
        "    counts = tweets.map(lambda tweet: checkjson(tweet))\n",
        "    print(counts.take(2))\n",
        "\n",
        "    total = counts.map(lambda x: (x)).reduceByKey(lambda a, b: a + b)\n",
        "    total.repartition(1).saveAsTextFile(\"s3://vandy-bigdata-2/hw6/3_days.out\")\n",
        "\n",
        "  finally:\n",
        "    sc.stop()\n",
        "\n",
        "  pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G6phVcUW__D2",
        "outputId": "14889a5f-433a-475a-9b20-1d90aeb215ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ":: loading settings :: url = jar:file:/content/spark-3.2.3-bin-hadoop3.2/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
            "Ivy Default Cache set to: /root/.ivy2/cache\n",
            "The jars for the packages stored in: /root/.ivy2/jars\n",
            "org.apache.spark#spark-streaming-kafka-0-8_2.11 added as a dependency\n",
            ":: resolving dependencies :: org.apache.spark#spark-submit-parent-c32f8e2d-7f31-4065-92a8-8854bd6765c1;1.0\n",
            "\tconfs: [default]\n",
            "\tfound org.apache.spark#spark-streaming-kafka-0-8_2.11;2.4.7 in central\n",
            "\tfound org.apache.kafka#kafka_2.11;0.8.2.1 in central\n",
            "\tfound org.scala-lang.modules#scala-xml_2.11;1.0.2 in central\n",
            "\tfound com.yammer.metrics#metrics-core;2.2.0 in central\n",
            "\tfound org.slf4j#slf4j-api;1.7.16 in central\n",
            "\tfound org.scala-lang.modules#scala-parser-combinators_2.11;1.1.0 in central\n",
            "\tfound com.101tec#zkclient;0.3 in central\n",
            "\tfound log4j#log4j;1.2.17 in central\n",
            "\tfound org.apache.kafka#kafka-clients;0.8.2.1 in central\n",
            "\tfound net.jpountz.lz4#lz4;1.2.0 in central\n",
            "\tfound org.xerial.snappy#snappy-java;1.1.7.5 in central\n",
            "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
            ":: resolution report :: resolve 808ms :: artifacts dl 22ms\n",
            "\t:: modules in use:\n",
            "\tcom.101tec#zkclient;0.3 from central in [default]\n",
            "\tcom.yammer.metrics#metrics-core;2.2.0 from central in [default]\n",
            "\tlog4j#log4j;1.2.17 from central in [default]\n",
            "\tnet.jpountz.lz4#lz4;1.2.0 from central in [default]\n",
            "\torg.apache.kafka#kafka-clients;0.8.2.1 from central in [default]\n",
            "\torg.apache.kafka#kafka_2.11;0.8.2.1 from central in [default]\n",
            "\torg.apache.spark#spark-streaming-kafka-0-8_2.11;2.4.7 from central in [default]\n",
            "\torg.scala-lang.modules#scala-parser-combinators_2.11;1.1.0 from central in [default]\n",
            "\torg.scala-lang.modules#scala-xml_2.11;1.0.2 from central in [default]\n",
            "\torg.slf4j#slf4j-api;1.7.16 from central in [default]\n",
            "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
            "\torg.xerial.snappy#snappy-java;1.1.7.5 from central in [default]\n",
            "\t---------------------------------------------------------------------\n",
            "\t|                  |            modules            ||   artifacts   |\n",
            "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
            "\t---------------------------------------------------------------------\n",
            "\t|      default     |   12  |   0   |   0   |   0   ||   12  |   0   |\n",
            "\t---------------------------------------------------------------------\n",
            ":: retrieving :: org.apache.spark#spark-submit-parent-c32f8e2d-7f31-4065-92a8-8854bd6765c1\n",
            "\tconfs: [default]\n",
            "\t0 artifacts copied, 12 already retrieved (0kB/14ms)\n",
            "23/04/10 03:17:21 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
            "23/04/10 03:17:22 INFO SparkContext: Running Spark version 3.2.3\n",
            "23/04/10 03:17:22 INFO ResourceUtils: ==============================================================\n",
            "23/04/10 03:17:22 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "23/04/10 03:17:22 INFO ResourceUtils: ==============================================================\n",
            "23/04/10 03:17:22 INFO SparkContext: Submitted application: 1_count\n",
            "23/04/10 03:17:22 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "23/04/10 03:17:22 INFO ResourceProfile: Limiting resource is cpu\n",
            "23/04/10 03:17:22 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "23/04/10 03:17:22 INFO SecurityManager: Changing view acls to: root\n",
            "23/04/10 03:17:22 INFO SecurityManager: Changing modify acls to: root\n",
            "23/04/10 03:17:22 INFO SecurityManager: Changing view acls groups to: \n",
            "23/04/10 03:17:22 INFO SecurityManager: Changing modify acls groups to: \n",
            "23/04/10 03:17:22 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\n",
            "23/04/10 03:17:23 INFO Utils: Successfully started service 'sparkDriver' on port 46737.\n",
            "23/04/10 03:17:23 INFO SparkEnv: Registering MapOutputTracker\n",
            "23/04/10 03:17:23 INFO SparkEnv: Registering BlockManagerMaster\n",
            "23/04/10 03:17:23 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "23/04/10 03:17:23 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "23/04/10 03:17:23 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "23/04/10 03:17:23 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-a5399835-fdb1-4d9d-9569-b1bb33b05dbf\n",
            "23/04/10 03:17:23 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB\n",
            "23/04/10 03:17:23 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "23/04/10 03:17:24 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
            "23/04/10 03:17:24 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://e77b3fc47c45:4040\n",
            "23/04/10 03:17:24 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar at spark://e77b3fc47c45:46737/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1681096642783\n",
            "23/04/10 03:17:24 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar at spark://e77b3fc47c45:46737/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1681096642783\n",
            "23/04/10 03:17:24 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar at spark://e77b3fc47c45:46737/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1681096642783\n",
            "23/04/10 03:17:24 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar at spark://e77b3fc47c45:46737/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1681096642783\n",
            "23/04/10 03:17:24 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar at spark://e77b3fc47c45:46737/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1681096642783\n",
            "23/04/10 03:17:24 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar at spark://e77b3fc47c45:46737/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1681096642783\n",
            "23/04/10 03:17:24 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar at spark://e77b3fc47c45:46737/jars/com.101tec_zkclient-0.3.jar with timestamp 1681096642783\n",
            "23/04/10 03:17:24 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar at spark://e77b3fc47c45:46737/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1681096642783\n",
            "23/04/10 03:17:24 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar at spark://e77b3fc47c45:46737/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1681096642783\n",
            "23/04/10 03:17:24 INFO SparkContext: Added JAR file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar at spark://e77b3fc47c45:46737/jars/log4j_log4j-1.2.17.jar with timestamp 1681096642783\n",
            "23/04/10 03:17:24 INFO SparkContext: Added JAR file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar at spark://e77b3fc47c45:46737/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1681096642783\n",
            "23/04/10 03:17:24 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar at spark://e77b3fc47c45:46737/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1681096642783\n",
            "23/04/10 03:17:24 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar at file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1681096642783\n",
            "23/04/10 03:17:24 INFO Utils: Copying /root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar to /tmp/spark-4c48e978-a241-4af5-bf47-a44f8c9d3b50/userFiles-851bc9e1-2161-4e41-9621-b40a9518c024/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar\n",
            "23/04/10 03:17:24 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar at file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1681096642783\n",
            "23/04/10 03:17:24 INFO Utils: Copying /root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar to /tmp/spark-4c48e978-a241-4af5-bf47-a44f8c9d3b50/userFiles-851bc9e1-2161-4e41-9621-b40a9518c024/org.apache.kafka_kafka_2.11-0.8.2.1.jar\n",
            "23/04/10 03:17:24 INFO SparkContext: Added file file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar at file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1681096642783\n",
            "23/04/10 03:17:24 INFO Utils: Copying /root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-4c48e978-a241-4af5-bf47-a44f8c9d3b50/userFiles-851bc9e1-2161-4e41-9621-b40a9518c024/org.spark-project.spark_unused-1.0.0.jar\n",
            "23/04/10 03:17:24 INFO SparkContext: Added file file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar at file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1681096642783\n",
            "23/04/10 03:17:24 INFO Utils: Copying /root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar to /tmp/spark-4c48e978-a241-4af5-bf47-a44f8c9d3b50/userFiles-851bc9e1-2161-4e41-9621-b40a9518c024/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar\n",
            "23/04/10 03:17:24 INFO SparkContext: Added file file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar at file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1681096642783\n",
            "23/04/10 03:17:24 INFO Utils: Copying /root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar to /tmp/spark-4c48e978-a241-4af5-bf47-a44f8c9d3b50/userFiles-851bc9e1-2161-4e41-9621-b40a9518c024/com.yammer.metrics_metrics-core-2.2.0.jar\n",
            "23/04/10 03:17:24 INFO SparkContext: Added file file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar at file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1681096642783\n",
            "23/04/10 03:17:24 INFO Utils: Copying /root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar to /tmp/spark-4c48e978-a241-4af5-bf47-a44f8c9d3b50/userFiles-851bc9e1-2161-4e41-9621-b40a9518c024/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar\n",
            "23/04/10 03:17:24 INFO SparkContext: Added file file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar at file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar with timestamp 1681096642783\n",
            "23/04/10 03:17:24 INFO Utils: Copying /root/.ivy2/jars/com.101tec_zkclient-0.3.jar to /tmp/spark-4c48e978-a241-4af5-bf47-a44f8c9d3b50/userFiles-851bc9e1-2161-4e41-9621-b40a9518c024/com.101tec_zkclient-0.3.jar\n",
            "23/04/10 03:17:24 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar at file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1681096642783\n",
            "23/04/10 03:17:24 INFO Utils: Copying /root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar to /tmp/spark-4c48e978-a241-4af5-bf47-a44f8c9d3b50/userFiles-851bc9e1-2161-4e41-9621-b40a9518c024/org.apache.kafka_kafka-clients-0.8.2.1.jar\n",
            "23/04/10 03:17:24 INFO SparkContext: Added file file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar at file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1681096642783\n",
            "23/04/10 03:17:24 INFO Utils: Copying /root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar to /tmp/spark-4c48e978-a241-4af5-bf47-a44f8c9d3b50/userFiles-851bc9e1-2161-4e41-9621-b40a9518c024/org.slf4j_slf4j-api-1.7.16.jar\n",
            "23/04/10 03:17:24 INFO SparkContext: Added file file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar at file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar with timestamp 1681096642783\n",
            "23/04/10 03:17:24 INFO Utils: Copying /root/.ivy2/jars/log4j_log4j-1.2.17.jar to /tmp/spark-4c48e978-a241-4af5-bf47-a44f8c9d3b50/userFiles-851bc9e1-2161-4e41-9621-b40a9518c024/log4j_log4j-1.2.17.jar\n",
            "23/04/10 03:17:24 INFO SparkContext: Added file file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar at file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1681096642783\n",
            "23/04/10 03:17:24 INFO Utils: Copying /root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar to /tmp/spark-4c48e978-a241-4af5-bf47-a44f8c9d3b50/userFiles-851bc9e1-2161-4e41-9621-b40a9518c024/net.jpountz.lz4_lz4-1.2.0.jar\n",
            "23/04/10 03:17:24 INFO SparkContext: Added file file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar at file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1681096642783\n",
            "23/04/10 03:17:24 INFO Utils: Copying /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar to /tmp/spark-4c48e978-a241-4af5-bf47-a44f8c9d3b50/userFiles-851bc9e1-2161-4e41-9621-b40a9518c024/org.xerial.snappy_snappy-java-1.1.7.5.jar\n",
            "23/04/10 03:17:24 INFO Executor: Starting executor ID driver on host e77b3fc47c45\n",
            "23/04/10 03:17:24 INFO Executor: Fetching file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar with timestamp 1681096642783\n",
            "23/04/10 03:17:24 INFO Utils: /root/.ivy2/jars/log4j_log4j-1.2.17.jar has been previously copied to /tmp/spark-4c48e978-a241-4af5-bf47-a44f8c9d3b50/userFiles-851bc9e1-2161-4e41-9621-b40a9518c024/log4j_log4j-1.2.17.jar\n",
            "23/04/10 03:17:24 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1681096642783\n",
            "23/04/10 03:17:24 INFO Utils: /root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar has been previously copied to /tmp/spark-4c48e978-a241-4af5-bf47-a44f8c9d3b50/userFiles-851bc9e1-2161-4e41-9621-b40a9518c024/org.apache.kafka_kafka-clients-0.8.2.1.jar\n",
            "23/04/10 03:17:24 INFO Executor: Fetching file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1681096642783\n",
            "23/04/10 03:17:24 INFO Utils: /root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar has been previously copied to /tmp/spark-4c48e978-a241-4af5-bf47-a44f8c9d3b50/userFiles-851bc9e1-2161-4e41-9621-b40a9518c024/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar\n",
            "23/04/10 03:17:24 INFO Executor: Fetching file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1681096642783\n",
            "23/04/10 03:17:24 INFO Utils: /root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar has been previously copied to /tmp/spark-4c48e978-a241-4af5-bf47-a44f8c9d3b50/userFiles-851bc9e1-2161-4e41-9621-b40a9518c024/org.slf4j_slf4j-api-1.7.16.jar\n",
            "23/04/10 03:17:24 INFO Executor: Fetching file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1681096642783\n",
            "23/04/10 03:17:24 INFO Utils: /root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar has been previously copied to /tmp/spark-4c48e978-a241-4af5-bf47-a44f8c9d3b50/userFiles-851bc9e1-2161-4e41-9621-b40a9518c024/net.jpountz.lz4_lz4-1.2.0.jar\n",
            "23/04/10 03:17:24 INFO Executor: Fetching file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1681096642783\n",
            "23/04/10 03:17:24 INFO Utils: /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar has been previously copied to /tmp/spark-4c48e978-a241-4af5-bf47-a44f8c9d3b50/userFiles-851bc9e1-2161-4e41-9621-b40a9518c024/org.xerial.snappy_snappy-java-1.1.7.5.jar\n",
            "23/04/10 03:17:24 INFO Executor: Fetching file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1681096642783\n",
            "23/04/10 03:17:24 INFO Utils: /root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar has been previously copied to /tmp/spark-4c48e978-a241-4af5-bf47-a44f8c9d3b50/userFiles-851bc9e1-2161-4e41-9621-b40a9518c024/org.spark-project.spark_unused-1.0.0.jar\n",
            "23/04/10 03:17:24 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1681096642783\n",
            "23/04/10 03:17:24 INFO Utils: /root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar has been previously copied to /tmp/spark-4c48e978-a241-4af5-bf47-a44f8c9d3b50/userFiles-851bc9e1-2161-4e41-9621-b40a9518c024/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar\n",
            "23/04/10 03:17:24 INFO Executor: Fetching file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1681096642783\n",
            "23/04/10 03:17:24 INFO Utils: /root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar has been previously copied to /tmp/spark-4c48e978-a241-4af5-bf47-a44f8c9d3b50/userFiles-851bc9e1-2161-4e41-9621-b40a9518c024/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar\n",
            "23/04/10 03:17:24 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1681096642783\n",
            "23/04/10 03:17:24 INFO Utils: /root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar has been previously copied to /tmp/spark-4c48e978-a241-4af5-bf47-a44f8c9d3b50/userFiles-851bc9e1-2161-4e41-9621-b40a9518c024/org.apache.kafka_kafka_2.11-0.8.2.1.jar\n",
            "23/04/10 03:17:24 INFO Executor: Fetching file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar with timestamp 1681096642783\n",
            "23/04/10 03:17:24 INFO Utils: /root/.ivy2/jars/com.101tec_zkclient-0.3.jar has been previously copied to /tmp/spark-4c48e978-a241-4af5-bf47-a44f8c9d3b50/userFiles-851bc9e1-2161-4e41-9621-b40a9518c024/com.101tec_zkclient-0.3.jar\n",
            "23/04/10 03:17:24 INFO Executor: Fetching file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1681096642783\n",
            "23/04/10 03:17:24 INFO Utils: /root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar has been previously copied to /tmp/spark-4c48e978-a241-4af5-bf47-a44f8c9d3b50/userFiles-851bc9e1-2161-4e41-9621-b40a9518c024/com.yammer.metrics_metrics-core-2.2.0.jar\n",
            "23/04/10 03:17:24 INFO Executor: Fetching spark://e77b3fc47c45:46737/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1681096642783\n",
            "23/04/10 03:17:24 INFO TransportClientFactory: Successfully created connection to e77b3fc47c45/172.28.0.12:46737 after 46 ms (0 ms spent in bootstraps)\n",
            "23/04/10 03:17:24 INFO Utils: Fetching spark://e77b3fc47c45:46737/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar to /tmp/spark-4c48e978-a241-4af5-bf47-a44f8c9d3b50/userFiles-851bc9e1-2161-4e41-9621-b40a9518c024/fetchFileTemp1508996713494999743.tmp\n",
            "23/04/10 03:17:24 INFO Utils: /tmp/spark-4c48e978-a241-4af5-bf47-a44f8c9d3b50/userFiles-851bc9e1-2161-4e41-9621-b40a9518c024/fetchFileTemp1508996713494999743.tmp has been previously copied to /tmp/spark-4c48e978-a241-4af5-bf47-a44f8c9d3b50/userFiles-851bc9e1-2161-4e41-9621-b40a9518c024/org.apache.kafka_kafka_2.11-0.8.2.1.jar\n",
            "23/04/10 03:17:24 INFO Executor: Adding file:/tmp/spark-4c48e978-a241-4af5-bf47-a44f8c9d3b50/userFiles-851bc9e1-2161-4e41-9621-b40a9518c024/org.apache.kafka_kafka_2.11-0.8.2.1.jar to class loader\n",
            "23/04/10 03:17:24 INFO Executor: Fetching spark://e77b3fc47c45:46737/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1681096642783\n",
            "23/04/10 03:17:24 INFO Utils: Fetching spark://e77b3fc47c45:46737/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-4c48e978-a241-4af5-bf47-a44f8c9d3b50/userFiles-851bc9e1-2161-4e41-9621-b40a9518c024/fetchFileTemp6364252769945245276.tmp\n",
            "23/04/10 03:17:24 INFO Utils: /tmp/spark-4c48e978-a241-4af5-bf47-a44f8c9d3b50/userFiles-851bc9e1-2161-4e41-9621-b40a9518c024/fetchFileTemp6364252769945245276.tmp has been previously copied to /tmp/spark-4c48e978-a241-4af5-bf47-a44f8c9d3b50/userFiles-851bc9e1-2161-4e41-9621-b40a9518c024/org.spark-project.spark_unused-1.0.0.jar\n",
            "23/04/10 03:17:24 INFO Executor: Adding file:/tmp/spark-4c48e978-a241-4af5-bf47-a44f8c9d3b50/userFiles-851bc9e1-2161-4e41-9621-b40a9518c024/org.spark-project.spark_unused-1.0.0.jar to class loader\n",
            "23/04/10 03:17:24 INFO Executor: Fetching spark://e77b3fc47c45:46737/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1681096642783\n",
            "23/04/10 03:17:24 INFO Utils: Fetching spark://e77b3fc47c45:46737/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar to /tmp/spark-4c48e978-a241-4af5-bf47-a44f8c9d3b50/userFiles-851bc9e1-2161-4e41-9621-b40a9518c024/fetchFileTemp4474693085277621462.tmp\n",
            "23/04/10 03:17:24 INFO Utils: /tmp/spark-4c48e978-a241-4af5-bf47-a44f8c9d3b50/userFiles-851bc9e1-2161-4e41-9621-b40a9518c024/fetchFileTemp4474693085277621462.tmp has been previously copied to /tmp/spark-4c48e978-a241-4af5-bf47-a44f8c9d3b50/userFiles-851bc9e1-2161-4e41-9621-b40a9518c024/org.xerial.snappy_snappy-java-1.1.7.5.jar\n",
            "23/04/10 03:17:24 INFO Executor: Adding file:/tmp/spark-4c48e978-a241-4af5-bf47-a44f8c9d3b50/userFiles-851bc9e1-2161-4e41-9621-b40a9518c024/org.xerial.snappy_snappy-java-1.1.7.5.jar to class loader\n",
            "23/04/10 03:17:24 INFO Executor: Fetching spark://e77b3fc47c45:46737/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1681096642783\n",
            "23/04/10 03:17:24 INFO Utils: Fetching spark://e77b3fc47c45:46737/jars/org.slf4j_slf4j-api-1.7.16.jar to /tmp/spark-4c48e978-a241-4af5-bf47-a44f8c9d3b50/userFiles-851bc9e1-2161-4e41-9621-b40a9518c024/fetchFileTemp1816816455500415177.tmp\n",
            "23/04/10 03:17:24 INFO Utils: /tmp/spark-4c48e978-a241-4af5-bf47-a44f8c9d3b50/userFiles-851bc9e1-2161-4e41-9621-b40a9518c024/fetchFileTemp1816816455500415177.tmp has been previously copied to /tmp/spark-4c48e978-a241-4af5-bf47-a44f8c9d3b50/userFiles-851bc9e1-2161-4e41-9621-b40a9518c024/org.slf4j_slf4j-api-1.7.16.jar\n",
            "23/04/10 03:17:24 INFO Executor: Adding file:/tmp/spark-4c48e978-a241-4af5-bf47-a44f8c9d3b50/userFiles-851bc9e1-2161-4e41-9621-b40a9518c024/org.slf4j_slf4j-api-1.7.16.jar to class loader\n",
            "23/04/10 03:17:24 INFO Executor: Fetching spark://e77b3fc47c45:46737/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1681096642783\n",
            "23/04/10 03:17:24 INFO Utils: Fetching spark://e77b3fc47c45:46737/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar to /tmp/spark-4c48e978-a241-4af5-bf47-a44f8c9d3b50/userFiles-851bc9e1-2161-4e41-9621-b40a9518c024/fetchFileTemp8620943391687090338.tmp\n",
            "23/04/10 03:17:24 INFO Utils: /tmp/spark-4c48e978-a241-4af5-bf47-a44f8c9d3b50/userFiles-851bc9e1-2161-4e41-9621-b40a9518c024/fetchFileTemp8620943391687090338.tmp has been previously copied to /tmp/spark-4c48e978-a241-4af5-bf47-a44f8c9d3b50/userFiles-851bc9e1-2161-4e41-9621-b40a9518c024/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar\n",
            "23/04/10 03:17:24 INFO Executor: Adding file:/tmp/spark-4c48e978-a241-4af5-bf47-a44f8c9d3b50/userFiles-851bc9e1-2161-4e41-9621-b40a9518c024/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar to class loader\n",
            "23/04/10 03:17:24 INFO Executor: Fetching spark://e77b3fc47c45:46737/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1681096642783\n",
            "23/04/10 03:17:24 INFO Utils: Fetching spark://e77b3fc47c45:46737/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar to /tmp/spark-4c48e978-a241-4af5-bf47-a44f8c9d3b50/userFiles-851bc9e1-2161-4e41-9621-b40a9518c024/fetchFileTemp2368873691538666841.tmp\n",
            "23/04/10 03:17:24 INFO Utils: /tmp/spark-4c48e978-a241-4af5-bf47-a44f8c9d3b50/userFiles-851bc9e1-2161-4e41-9621-b40a9518c024/fetchFileTemp2368873691538666841.tmp has been previously copied to /tmp/spark-4c48e978-a241-4af5-bf47-a44f8c9d3b50/userFiles-851bc9e1-2161-4e41-9621-b40a9518c024/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar\n",
            "23/04/10 03:17:24 INFO Executor: Adding file:/tmp/spark-4c48e978-a241-4af5-bf47-a44f8c9d3b50/userFiles-851bc9e1-2161-4e41-9621-b40a9518c024/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar to class loader\n",
            "23/04/10 03:17:24 INFO Executor: Fetching spark://e77b3fc47c45:46737/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1681096642783\n",
            "23/04/10 03:17:24 INFO Utils: Fetching spark://e77b3fc47c45:46737/jars/net.jpountz.lz4_lz4-1.2.0.jar to /tmp/spark-4c48e978-a241-4af5-bf47-a44f8c9d3b50/userFiles-851bc9e1-2161-4e41-9621-b40a9518c024/fetchFileTemp4786602023410130633.tmp\n",
            "23/04/10 03:17:24 INFO Utils: /tmp/spark-4c48e978-a241-4af5-bf47-a44f8c9d3b50/userFiles-851bc9e1-2161-4e41-9621-b40a9518c024/fetchFileTemp4786602023410130633.tmp has been previously copied to /tmp/spark-4c48e978-a241-4af5-bf47-a44f8c9d3b50/userFiles-851bc9e1-2161-4e41-9621-b40a9518c024/net.jpountz.lz4_lz4-1.2.0.jar\n",
            "23/04/10 03:17:24 INFO Executor: Adding file:/tmp/spark-4c48e978-a241-4af5-bf47-a44f8c9d3b50/userFiles-851bc9e1-2161-4e41-9621-b40a9518c024/net.jpountz.lz4_lz4-1.2.0.jar to class loader\n",
            "23/04/10 03:17:24 INFO Executor: Fetching spark://e77b3fc47c45:46737/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1681096642783\n",
            "23/04/10 03:17:24 INFO Utils: Fetching spark://e77b3fc47c45:46737/jars/com.yammer.metrics_metrics-core-2.2.0.jar to /tmp/spark-4c48e978-a241-4af5-bf47-a44f8c9d3b50/userFiles-851bc9e1-2161-4e41-9621-b40a9518c024/fetchFileTemp1773570204901822036.tmp\n",
            "23/04/10 03:17:24 INFO Utils: /tmp/spark-4c48e978-a241-4af5-bf47-a44f8c9d3b50/userFiles-851bc9e1-2161-4e41-9621-b40a9518c024/fetchFileTemp1773570204901822036.tmp has been previously copied to /tmp/spark-4c48e978-a241-4af5-bf47-a44f8c9d3b50/userFiles-851bc9e1-2161-4e41-9621-b40a9518c024/com.yammer.metrics_metrics-core-2.2.0.jar\n",
            "23/04/10 03:17:24 INFO Executor: Adding file:/tmp/spark-4c48e978-a241-4af5-bf47-a44f8c9d3b50/userFiles-851bc9e1-2161-4e41-9621-b40a9518c024/com.yammer.metrics_metrics-core-2.2.0.jar to class loader\n",
            "23/04/10 03:17:24 INFO Executor: Fetching spark://e77b3fc47c45:46737/jars/log4j_log4j-1.2.17.jar with timestamp 1681096642783\n",
            "23/04/10 03:17:24 INFO Utils: Fetching spark://e77b3fc47c45:46737/jars/log4j_log4j-1.2.17.jar to /tmp/spark-4c48e978-a241-4af5-bf47-a44f8c9d3b50/userFiles-851bc9e1-2161-4e41-9621-b40a9518c024/fetchFileTemp3713658249280851139.tmp\n",
            "23/04/10 03:17:24 INFO Utils: /tmp/spark-4c48e978-a241-4af5-bf47-a44f8c9d3b50/userFiles-851bc9e1-2161-4e41-9621-b40a9518c024/fetchFileTemp3713658249280851139.tmp has been previously copied to /tmp/spark-4c48e978-a241-4af5-bf47-a44f8c9d3b50/userFiles-851bc9e1-2161-4e41-9621-b40a9518c024/log4j_log4j-1.2.17.jar\n",
            "23/04/10 03:17:24 INFO Executor: Adding file:/tmp/spark-4c48e978-a241-4af5-bf47-a44f8c9d3b50/userFiles-851bc9e1-2161-4e41-9621-b40a9518c024/log4j_log4j-1.2.17.jar to class loader\n",
            "23/04/10 03:17:24 INFO Executor: Fetching spark://e77b3fc47c45:46737/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1681096642783\n",
            "23/04/10 03:17:24 INFO Utils: Fetching spark://e77b3fc47c45:46737/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar to /tmp/spark-4c48e978-a241-4af5-bf47-a44f8c9d3b50/userFiles-851bc9e1-2161-4e41-9621-b40a9518c024/fetchFileTemp8081686956009798410.tmp\n",
            "23/04/10 03:17:24 INFO Utils: /tmp/spark-4c48e978-a241-4af5-bf47-a44f8c9d3b50/userFiles-851bc9e1-2161-4e41-9621-b40a9518c024/fetchFileTemp8081686956009798410.tmp has been previously copied to /tmp/spark-4c48e978-a241-4af5-bf47-a44f8c9d3b50/userFiles-851bc9e1-2161-4e41-9621-b40a9518c024/org.apache.kafka_kafka-clients-0.8.2.1.jar\n",
            "23/04/10 03:17:24 INFO Executor: Adding file:/tmp/spark-4c48e978-a241-4af5-bf47-a44f8c9d3b50/userFiles-851bc9e1-2161-4e41-9621-b40a9518c024/org.apache.kafka_kafka-clients-0.8.2.1.jar to class loader\n",
            "23/04/10 03:17:24 INFO Executor: Fetching spark://e77b3fc47c45:46737/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1681096642783\n",
            "23/04/10 03:17:24 INFO Utils: Fetching spark://e77b3fc47c45:46737/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar to /tmp/spark-4c48e978-a241-4af5-bf47-a44f8c9d3b50/userFiles-851bc9e1-2161-4e41-9621-b40a9518c024/fetchFileTemp7425057473224869810.tmp\n",
            "23/04/10 03:17:24 INFO Utils: /tmp/spark-4c48e978-a241-4af5-bf47-a44f8c9d3b50/userFiles-851bc9e1-2161-4e41-9621-b40a9518c024/fetchFileTemp7425057473224869810.tmp has been previously copied to /tmp/spark-4c48e978-a241-4af5-bf47-a44f8c9d3b50/userFiles-851bc9e1-2161-4e41-9621-b40a9518c024/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar\n",
            "23/04/10 03:17:24 INFO Executor: Adding file:/tmp/spark-4c48e978-a241-4af5-bf47-a44f8c9d3b50/userFiles-851bc9e1-2161-4e41-9621-b40a9518c024/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar to class loader\n",
            "23/04/10 03:17:24 INFO Executor: Fetching spark://e77b3fc47c45:46737/jars/com.101tec_zkclient-0.3.jar with timestamp 1681096642783\n",
            "23/04/10 03:17:24 INFO Utils: Fetching spark://e77b3fc47c45:46737/jars/com.101tec_zkclient-0.3.jar to /tmp/spark-4c48e978-a241-4af5-bf47-a44f8c9d3b50/userFiles-851bc9e1-2161-4e41-9621-b40a9518c024/fetchFileTemp4979859098002979462.tmp\n",
            "23/04/10 03:17:24 INFO Utils: /tmp/spark-4c48e978-a241-4af5-bf47-a44f8c9d3b50/userFiles-851bc9e1-2161-4e41-9621-b40a9518c024/fetchFileTemp4979859098002979462.tmp has been previously copied to /tmp/spark-4c48e978-a241-4af5-bf47-a44f8c9d3b50/userFiles-851bc9e1-2161-4e41-9621-b40a9518c024/com.101tec_zkclient-0.3.jar\n",
            "23/04/10 03:17:24 INFO Executor: Adding file:/tmp/spark-4c48e978-a241-4af5-bf47-a44f8c9d3b50/userFiles-851bc9e1-2161-4e41-9621-b40a9518c024/com.101tec_zkclient-0.3.jar to class loader\n",
            "23/04/10 03:17:24 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46699.\n",
            "23/04/10 03:17:24 INFO NettyBlockTransferService: Server created on e77b3fc47c45:46699\n",
            "23/04/10 03:17:24 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "23/04/10 03:17:24 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, e77b3fc47c45, 46699, None)\n",
            "23/04/10 03:17:25 INFO BlockManagerMasterEndpoint: Registering block manager e77b3fc47c45:46699 with 366.3 MiB RAM, BlockManagerId(driver, e77b3fc47c45, 46699, None)\n",
            "23/04/10 03:17:25 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, e77b3fc47c45, 46699, None)\n",
            "23/04/10 03:17:25 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, e77b3fc47c45, 46699, None)\n",
            "23/04/10 03:17:26 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 354.3 KiB, free 366.0 MiB)\n",
            "23/04/10 03:17:26 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 32.0 KiB, free 365.9 MiB)\n",
            "23/04/10 03:17:26 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on e77b3fc47c45:46699 (size: 32.0 KiB, free: 366.3 MiB)\n",
            "23/04/10 03:17:26 INFO SparkContext: Created broadcast 0 from textFile at NativeMethodAccessorImpl.java:0\n",
            "23/04/10 03:17:26 INFO FileInputFormat: Total input files to process : 1\n",
            "23/04/10 03:17:26 INFO SparkContext: Starting job: runJob at PythonRDD.scala:166\n",
            "23/04/10 03:17:26 INFO DAGScheduler: Got job 0 (runJob at PythonRDD.scala:166) with 1 output partitions\n",
            "23/04/10 03:17:26 INFO DAGScheduler: Final stage: ResultStage 0 (runJob at PythonRDD.scala:166)\n",
            "23/04/10 03:17:26 INFO DAGScheduler: Parents of final stage: List()\n",
            "23/04/10 03:17:26 INFO DAGScheduler: Missing parents: List()\n",
            "23/04/10 03:17:26 INFO DAGScheduler: Submitting ResultStage 0 (PythonRDD[2] at RDD at PythonRDD.scala:53), which has no missing parents\n",
            "23/04/10 03:17:26 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 8.9 KiB, free 365.9 MiB)\n",
            "23/04/10 03:17:26 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.5 KiB, free 365.9 MiB)\n",
            "23/04/10 03:17:26 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on e77b3fc47c45:46699 (size: 5.5 KiB, free: 366.3 MiB)\n",
            "23/04/10 03:17:26 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1478\n",
            "23/04/10 03:17:26 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (PythonRDD[2] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(0))\n",
            "23/04/10 03:17:26 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
            "23/04/10 03:17:27 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (e77b3fc47c45, executor driver, partition 0, PROCESS_LOCAL, 4510 bytes) taskResourceAssignments Map()\n",
            "23/04/10 03:17:27 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
            "23/04/10 03:17:28 INFO HadoopRDD: Input split: file:/content/nashville-tweets-2019-01-28:0+12519625\n",
            "23/04/10 03:17:29 INFO PythonRunner: Times: total = 1003, boot = 948, init = 54, finish = 1\n",
            "23/04/10 03:17:29 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1440 bytes result sent to driver\n",
            "23/04/10 03:17:29 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 2481 ms on e77b3fc47c45 (executor driver) (1/1)\n",
            "23/04/10 03:17:29 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
            "23/04/10 03:17:29 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 56257\n",
            "23/04/10 03:17:29 INFO DAGScheduler: ResultStage 0 (runJob at PythonRDD.scala:166) finished in 3.113 s\n",
            "23/04/10 03:17:29 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "23/04/10 03:17:29 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
            "23/04/10 03:17:29 INFO DAGScheduler: Job 0 finished: runJob at PythonRDD.scala:166, took 3.263446 s\n",
            "[('Sun', 1), ('Sun', 1)]\n",
            "23/04/10 03:17:30 INFO deprecation: mapred.output.dir is deprecated. Instead, use mapreduce.output.fileoutputformat.outputdir\n",
            "23/04/10 03:17:30 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
            "23/04/10 03:17:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "23/04/10 03:17:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "23/04/10 03:17:30 INFO SparkContext: Starting job: runJob at SparkHadoopWriter.scala:83\n",
            "23/04/10 03:17:30 INFO DAGScheduler: Registering RDD 4 (reduceByKey at /content/3_days.py:49) as input to shuffle 1\n",
            "23/04/10 03:17:30 INFO DAGScheduler: Registering RDD 8 (coalesce at NativeMethodAccessorImpl.java:0) as input to shuffle 0\n",
            "23/04/10 03:17:30 INFO DAGScheduler: Got job 1 (runJob at SparkHadoopWriter.scala:83) with 1 output partitions\n",
            "23/04/10 03:17:30 INFO DAGScheduler: Final stage: ResultStage 3 (runJob at SparkHadoopWriter.scala:83)\n",
            "23/04/10 03:17:30 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)\n",
            "23/04/10 03:17:30 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 2)\n",
            "23/04/10 03:17:30 INFO DAGScheduler: Submitting ShuffleMapStage 1 (PairwiseRDD[4] at reduceByKey at /content/3_days.py:49), which has no missing parents\n",
            "23/04/10 03:17:30 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 12.6 KiB, free 365.9 MiB)\n",
            "23/04/10 03:17:30 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 365.9 MiB)\n",
            "23/04/10 03:17:30 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on e77b3fc47c45:46699 (size: 7.7 KiB, free: 366.3 MiB)\n",
            "23/04/10 03:17:30 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1478\n",
            "23/04/10 03:17:30 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 1 (PairwiseRDD[4] at reduceByKey at /content/3_days.py:49) (first 15 tasks are for partitions Vector(0, 1))\n",
            "23/04/10 03:17:30 INFO TaskSchedulerImpl: Adding task set 1.0 with 2 tasks resource profile 0\n",
            "23/04/10 03:17:30 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (e77b3fc47c45, executor driver, partition 0, PROCESS_LOCAL, 4499 bytes) taskResourceAssignments Map()\n",
            "23/04/10 03:17:30 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 2) (e77b3fc47c45, executor driver, partition 1, PROCESS_LOCAL, 4499 bytes) taskResourceAssignments Map()\n",
            "23/04/10 03:17:30 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)\n",
            "23/04/10 03:17:30 INFO Executor: Running task 1.0 in stage 1.0 (TID 2)\n",
            "23/04/10 03:17:30 INFO HadoopRDD: Input split: file:/content/nashville-tweets-2019-01-28:12519625+12519626\n",
            "23/04/10 03:17:30 INFO HadoopRDD: Input split: file:/content/nashville-tweets-2019-01-28:0+12519625\n",
            "23/04/10 03:17:31 INFO PythonRunner: Times: total = 741, boot = 40, init = 62, finish = 639\n",
            "23/04/10 03:17:31 INFO PythonRunner: Times: total = 744, boot = 27, init = 16, finish = 701\n",
            "23/04/10 03:17:31 INFO Executor: Finished task 1.0 in stage 1.0 (TID 2). 1569 bytes result sent to driver\n",
            "23/04/10 03:17:31 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 2) in 1301 ms on e77b3fc47c45 (executor driver) (1/2)\n",
            "23/04/10 03:17:31 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1612 bytes result sent to driver\n",
            "23/04/10 03:17:31 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 1313 ms on e77b3fc47c45 (executor driver) (2/2)\n",
            "23/04/10 03:17:31 INFO DAGScheduler: ShuffleMapStage 1 (reduceByKey at /content/3_days.py:49) finished in 1.388 s\n",
            "23/04/10 03:17:31 INFO DAGScheduler: looking for newly runnable stages\n",
            "23/04/10 03:17:31 INFO DAGScheduler: running: Set()\n",
            "23/04/10 03:17:31 INFO DAGScheduler: waiting: Set(ShuffleMapStage 2, ResultStage 3)\n",
            "23/04/10 03:17:31 INFO DAGScheduler: failed: Set()\n",
            "23/04/10 03:17:31 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
            "23/04/10 03:17:31 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[8] at coalesce at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/04/10 03:17:31 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 10.6 KiB, free 365.9 MiB)\n",
            "23/04/10 03:17:31 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 6.2 KiB, free 365.9 MiB)\n",
            "23/04/10 03:17:31 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on e77b3fc47c45:46699 (size: 6.2 KiB, free: 366.2 MiB)\n",
            "23/04/10 03:17:31 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1478\n",
            "23/04/10 03:17:31 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[8] at coalesce at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
            "23/04/10 03:17:31 INFO TaskSchedulerImpl: Adding task set 2.0 with 2 tasks resource profile 0\n",
            "23/04/10 03:17:31 INFO TaskSetManager: Starting task 1.0 in stage 2.0 (TID 3) (e77b3fc47c45, executor driver, partition 1, NODE_LOCAL, 4260 bytes) taskResourceAssignments Map()\n",
            "23/04/10 03:17:31 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 4) (e77b3fc47c45, executor driver, partition 0, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()\n",
            "23/04/10 03:17:31 INFO Executor: Running task 0.0 in stage 2.0 (TID 4)\n",
            "23/04/10 03:17:31 INFO Executor: Running task 1.0 in stage 2.0 (TID 3)\n",
            "23/04/10 03:17:31 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/10 03:17:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 33 ms\n",
            "23/04/10 03:17:31 INFO ShuffleBlockFetcherIterator: Getting 2 (144.0 B) non-empty blocks including 2 (144.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/10 03:17:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 31 ms\n",
            "23/04/10 03:17:31 INFO PythonRunner: Times: total = 23, boot = -616, init = 639, finish = 0\n",
            "23/04/10 03:17:31 INFO Executor: Finished task 0.0 in stage 2.0 (TID 4). 1654 bytes result sent to driver\n",
            "23/04/10 03:17:32 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 4) in 151 ms on e77b3fc47c45 (executor driver) (1/2)\n",
            "23/04/10 03:17:32 INFO PythonRunner: Times: total = 64, boot = -634, init = 698, finish = 0\n",
            "23/04/10 03:17:32 INFO Executor: Finished task 1.0 in stage 2.0 (TID 3). 1783 bytes result sent to driver\n",
            "23/04/10 03:17:32 INFO TaskSetManager: Finished task 1.0 in stage 2.0 (TID 3) in 189 ms on e77b3fc47c45 (executor driver) (2/2)\n",
            "23/04/10 03:17:32 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
            "23/04/10 03:17:32 INFO DAGScheduler: ShuffleMapStage 2 (coalesce at NativeMethodAccessorImpl.java:0) finished in 0.220 s\n",
            "23/04/10 03:17:32 INFO DAGScheduler: looking for newly runnable stages\n",
            "23/04/10 03:17:32 INFO DAGScheduler: running: Set()\n",
            "23/04/10 03:17:32 INFO DAGScheduler: waiting: Set(ResultStage 3)\n",
            "23/04/10 03:17:32 INFO DAGScheduler: failed: Set()\n",
            "23/04/10 03:17:32 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[14] at saveAsTextFile at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/04/10 03:17:32 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 105.9 KiB, free 365.8 MiB)\n",
            "23/04/10 03:17:32 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 39.6 KiB, free 365.7 MiB)\n",
            "23/04/10 03:17:32 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on e77b3fc47c45:46699 (size: 39.6 KiB, free: 366.2 MiB)\n",
            "23/04/10 03:17:32 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1478\n",
            "23/04/10 03:17:32 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[14] at saveAsTextFile at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/04/10 03:17:32 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0\n",
            "23/04/10 03:17:32 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 5) (e77b3fc47c45, executor driver, partition 0, NODE_LOCAL, 4547 bytes) taskResourceAssignments Map()\n",
            "23/04/10 03:17:32 INFO Executor: Running task 0.0 in stage 3.0 (TID 5)\n",
            "23/04/10 03:17:32 INFO BlockManagerInfo: Removed broadcast_3_piece0 on e77b3fc47c45:46699 in memory (size: 6.2 KiB, free: 366.2 MiB)\n",
            "23/04/10 03:17:32 INFO BlockManagerInfo: Removed broadcast_2_piece0 on e77b3fc47c45:46699 in memory (size: 7.7 KiB, free: 366.2 MiB)\n",
            "23/04/10 03:17:32 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
            "23/04/10 03:17:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "23/04/10 03:17:32 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "23/04/10 03:17:32 INFO ShuffleBlockFetcherIterator: Getting 1 (72.0 B) non-empty blocks including 1 (72.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/10 03:17:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
            "23/04/10 03:17:32 INFO PythonRunner: Times: total = 56, boot = -312, init = 368, finish = 0\n",
            "23/04/10 03:17:32 INFO FileOutputCommitter: Saved output of task 'attempt_20230410031730423074992305391460_0014_m_000000_0' to file:/content/3_days.out/_temporary/0/task_20230410031730423074992305391460_0014_m_000000\n",
            "23/04/10 03:17:32 INFO SparkHadoopMapRedUtil: attempt_20230410031730423074992305391460_0014_m_000000_0: Committed\n",
            "23/04/10 03:17:32 INFO Executor: Finished task 0.0 in stage 3.0 (TID 5). 1995 bytes result sent to driver\n",
            "23/04/10 03:17:32 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 5) in 302 ms on e77b3fc47c45 (executor driver) (1/1)\n",
            "23/04/10 03:17:32 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
            "23/04/10 03:17:32 INFO DAGScheduler: ResultStage 3 (runJob at SparkHadoopWriter.scala:83) finished in 0.360 s\n",
            "23/04/10 03:17:32 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "23/04/10 03:17:32 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished\n",
            "23/04/10 03:17:32 INFO DAGScheduler: Job 1 finished: runJob at SparkHadoopWriter.scala:83, took 2.084660 s\n",
            "23/04/10 03:17:32 INFO SparkHadoopWriter: Start to commit write Job job_20230410031730423074992305391460_0014.\n",
            "23/04/10 03:17:32 INFO SparkHadoopWriter: Write Job job_20230410031730423074992305391460_0014 committed. Elapsed time: 16 ms.\n",
            "23/04/10 03:17:32 INFO SparkUI: Stopped Spark web UI at http://e77b3fc47c45:4040\n",
            "23/04/10 03:17:32 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "23/04/10 03:17:32 INFO MemoryStore: MemoryStore cleared\n",
            "23/04/10 03:17:32 INFO BlockManager: BlockManager stopped\n",
            "23/04/10 03:17:32 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "23/04/10 03:17:32 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "23/04/10 03:17:32 INFO SparkContext: Successfully stopped SparkContext\n",
            "23/04/10 03:17:33 INFO ShutdownHookManager: Shutdown hook called\n",
            "23/04/10 03:17:33 INFO ShutdownHookManager: Deleting directory /tmp/spark-4c48e978-a241-4af5-bf47-a44f8c9d3b50\n",
            "23/04/10 03:17:33 INFO ShutdownHookManager: Deleting directory /tmp/spark-28323b27-dad2-4bfd-b7a8-960f499d2dc1\n",
            "23/04/10 03:17:33 INFO ShutdownHookManager: Deleting directory /tmp/spark-4c48e978-a241-4af5-bf47-a44f8c9d3b50/pyspark-c52237f5-2714-47ca-b927-a52cee0576a9\n"
          ]
        }
      ],
      "source": [
        "# execute locally\n",
        "!spark-submit --packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.4.7 3_days.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XPRY68m2ptuN"
      },
      "outputs": [],
      "source": [
        "def test3(lines):\n",
        "    if lines[0].strip() == \"('Sun', 6294)\":\n",
        "        print(\"passed\")\n",
        "    else:\n",
        "        assert False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2h8y_YU2__D2",
        "outputId": "c5b63347-8ccb-4e6f-8fd4-1e0901ed1dd6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "passed\n"
          ]
        }
      ],
      "source": [
        "# test locall execution results\n",
        "with open('3_days.out/part-00000') as f:\n",
        "  lines = f.readlines()\n",
        "  test3(lines)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zdiDy-gu__D3"
      },
      "source": [
        "## Job 4. Join the batting and salaries data for Barry Bonds per year."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pf7QOOGcXsup",
        "outputId": "3ed16d8b-1645-416a-bc5d-f4eed1ae5dc1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing 4_join.py\n"
          ]
        }
      ],
      "source": [
        "%%file 4_join.py\n",
        "\n",
        "from pyspark import SparkContext, SparkConf\n",
        "\n",
        "def extract_keys(line):\n",
        "    data = line.split(',')\n",
        "    if data[0] == 'playerID' or len(data) < 2:\n",
        "        return None\n",
        "    elif data[0] == 'bondsba01':\n",
        "        # Batting data\n",
        "        key = (data[0], data[1])\n",
        "        return key, 'B,' + line\n",
        "    else:\n",
        "        # Salaries data\n",
        "        key = (data[3], data[0])\n",
        "        return key, 'S,' + line\n",
        "\n",
        "def join_rows(rows):\n",
        "    batting_rows = [r for r in rows if r.startswith('B,')]\n",
        "    salaries_rows = [r for r in rows if r.startswith('S,')]\n",
        "    if len(batting_rows) == 0 or len(salaries_rows) == 0:\n",
        "        return []\n",
        "    result = []\n",
        "    for bat in batting_rows:\n",
        "        for salary in salaries_rows:\n",
        "            final = bat[2:] + salary[2:]\n",
        "            result.append(final)\n",
        "    return result\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    #conf = SparkConf().setAppName('4_join').set('spark.hadoop.validateOutputSpecs', False)\n",
        "    #sc = SparkContext(conf=conf).getOrCreate()\n",
        "    sc = SparkContext(appName=\"4_join\")\n",
        "\n",
        "\n",
        "    try:\n",
        "        batting = sc.textFile('s3://vandy-bigdata-2/Batting.csv')\n",
        "        salaries = sc.textFile('s3://vandy-bigdata-2/Salaries.csv')\n",
        "\n",
        "        # Extract key-value pairs from both RDDs\n",
        "        batting_mapped = batting.map(extract_keys).filter(lambda x: x is not None)\n",
        "        salaries_mapped = salaries.map(extract_keys).filter(lambda x: x is not None)\n",
        "\n",
        "        # Join the two RDDs on the key (player ID and year)\n",
        "        joined_rdd = batting_mapped.union(salaries_mapped).groupByKey().flatMapValues(join_rows)\n",
        "\n",
        "        # Combine the values into a CSV string\n",
        "        combined_csv = joined_rdd.map(lambda x: ''.join(x[1])).collect()\n",
        "\n",
        "        final = []\n",
        "        for line in combined_csv:\n",
        "          final.append( ('join', line) )\n",
        "        \n",
        "        final_rdd = sc.parallelize(final).repartition(1)\n",
        "        final_rdd.saveAsTextFile(\"s3://vandy-bigdata-2/hw6/4_join.out\")\n",
        "\n",
        "\n",
        "    finally:\n",
        "        sc.stop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n5_a0GAy__D3",
        "outputId": "c69164c3-05e0-4009-8abe-9ea802754a9e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ":: loading settings :: url = jar:file:/content/spark-3.2.3-bin-hadoop3.2/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
            "Ivy Default Cache set to: /root/.ivy2/cache\n",
            "The jars for the packages stored in: /root/.ivy2/jars\n",
            "org.apache.spark#spark-streaming-kafka-0-8_2.11 added as a dependency\n",
            ":: resolving dependencies :: org.apache.spark#spark-submit-parent-598842d3-26b2-45d7-aed3-94f34f68e369;1.0\n",
            "\tconfs: [default]\n",
            "\tfound org.apache.spark#spark-streaming-kafka-0-8_2.11;2.4.7 in central\n",
            "\tfound org.apache.kafka#kafka_2.11;0.8.2.1 in central\n",
            "\tfound org.scala-lang.modules#scala-xml_2.11;1.0.2 in central\n",
            "\tfound com.yammer.metrics#metrics-core;2.2.0 in central\n",
            "\tfound org.slf4j#slf4j-api;1.7.16 in central\n",
            "\tfound org.scala-lang.modules#scala-parser-combinators_2.11;1.1.0 in central\n",
            "\tfound com.101tec#zkclient;0.3 in central\n",
            "\tfound log4j#log4j;1.2.17 in central\n",
            "\tfound org.apache.kafka#kafka-clients;0.8.2.1 in central\n",
            "\tfound net.jpountz.lz4#lz4;1.2.0 in central\n",
            "\tfound org.xerial.snappy#snappy-java;1.1.7.5 in central\n",
            "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
            ":: resolution report :: resolve 594ms :: artifacts dl 29ms\n",
            "\t:: modules in use:\n",
            "\tcom.101tec#zkclient;0.3 from central in [default]\n",
            "\tcom.yammer.metrics#metrics-core;2.2.0 from central in [default]\n",
            "\tlog4j#log4j;1.2.17 from central in [default]\n",
            "\tnet.jpountz.lz4#lz4;1.2.0 from central in [default]\n",
            "\torg.apache.kafka#kafka-clients;0.8.2.1 from central in [default]\n",
            "\torg.apache.kafka#kafka_2.11;0.8.2.1 from central in [default]\n",
            "\torg.apache.spark#spark-streaming-kafka-0-8_2.11;2.4.7 from central in [default]\n",
            "\torg.scala-lang.modules#scala-parser-combinators_2.11;1.1.0 from central in [default]\n",
            "\torg.scala-lang.modules#scala-xml_2.11;1.0.2 from central in [default]\n",
            "\torg.slf4j#slf4j-api;1.7.16 from central in [default]\n",
            "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
            "\torg.xerial.snappy#snappy-java;1.1.7.5 from central in [default]\n",
            "\t---------------------------------------------------------------------\n",
            "\t|                  |            modules            ||   artifacts   |\n",
            "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
            "\t---------------------------------------------------------------------\n",
            "\t|      default     |   12  |   0   |   0   |   0   ||   12  |   0   |\n",
            "\t---------------------------------------------------------------------\n",
            ":: retrieving :: org.apache.spark#spark-submit-parent-598842d3-26b2-45d7-aed3-94f34f68e369\n",
            "\tconfs: [default]\n",
            "\t0 artifacts copied, 12 already retrieved (0kB/12ms)\n",
            "23/04/12 03:21:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
            "23/04/12 03:21:14 INFO SparkContext: Running Spark version 3.2.3\n",
            "23/04/12 03:21:14 INFO ResourceUtils: ==============================================================\n",
            "23/04/12 03:21:14 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "23/04/12 03:21:14 INFO ResourceUtils: ==============================================================\n",
            "23/04/12 03:21:14 INFO SparkContext: Submitted application: 4_join\n",
            "23/04/12 03:21:14 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "23/04/12 03:21:14 INFO ResourceProfile: Limiting resource is cpu\n",
            "23/04/12 03:21:14 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "23/04/12 03:21:14 INFO SecurityManager: Changing view acls to: root\n",
            "23/04/12 03:21:14 INFO SecurityManager: Changing modify acls to: root\n",
            "23/04/12 03:21:14 INFO SecurityManager: Changing view acls groups to: \n",
            "23/04/12 03:21:14 INFO SecurityManager: Changing modify acls groups to: \n",
            "23/04/12 03:21:14 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\n",
            "23/04/12 03:21:14 INFO Utils: Successfully started service 'sparkDriver' on port 41073.\n",
            "23/04/12 03:21:14 INFO SparkEnv: Registering MapOutputTracker\n",
            "23/04/12 03:21:14 INFO SparkEnv: Registering BlockManagerMaster\n",
            "23/04/12 03:21:14 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "23/04/12 03:21:14 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "23/04/12 03:21:14 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "23/04/12 03:21:15 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-753eff5b-74f2-499b-bb82-569554b0a563\n",
            "23/04/12 03:21:15 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB\n",
            "23/04/12 03:21:15 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "23/04/12 03:21:15 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
            "23/04/12 03:21:15 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://f4c7b61d3b37:4040\n",
            "23/04/12 03:21:15 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar at spark://f4c7b61d3b37:41073/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1681269674469\n",
            "23/04/12 03:21:15 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar at spark://f4c7b61d3b37:41073/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1681269674469\n",
            "23/04/12 03:21:15 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar at spark://f4c7b61d3b37:41073/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1681269674469\n",
            "23/04/12 03:21:15 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar at spark://f4c7b61d3b37:41073/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1681269674469\n",
            "23/04/12 03:21:15 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar at spark://f4c7b61d3b37:41073/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1681269674469\n",
            "23/04/12 03:21:15 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar at spark://f4c7b61d3b37:41073/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1681269674469\n",
            "23/04/12 03:21:15 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar at spark://f4c7b61d3b37:41073/jars/com.101tec_zkclient-0.3.jar with timestamp 1681269674469\n",
            "23/04/12 03:21:15 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar at spark://f4c7b61d3b37:41073/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1681269674469\n",
            "23/04/12 03:21:15 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar at spark://f4c7b61d3b37:41073/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1681269674469\n",
            "23/04/12 03:21:15 INFO SparkContext: Added JAR file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar at spark://f4c7b61d3b37:41073/jars/log4j_log4j-1.2.17.jar with timestamp 1681269674469\n",
            "23/04/12 03:21:15 INFO SparkContext: Added JAR file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar at spark://f4c7b61d3b37:41073/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1681269674469\n",
            "23/04/12 03:21:15 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar at spark://f4c7b61d3b37:41073/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1681269674469\n",
            "23/04/12 03:21:15 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar at file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1681269674469\n",
            "23/04/12 03:21:15 INFO Utils: Copying /root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar to /tmp/spark-2749c8bf-e99b-445b-a265-a9739115088f/userFiles-3e64d614-08c6-4289-a015-11ebc6712678/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar\n",
            "23/04/12 03:21:15 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar at file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1681269674469\n",
            "23/04/12 03:21:15 INFO Utils: Copying /root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar to /tmp/spark-2749c8bf-e99b-445b-a265-a9739115088f/userFiles-3e64d614-08c6-4289-a015-11ebc6712678/org.apache.kafka_kafka_2.11-0.8.2.1.jar\n",
            "23/04/12 03:21:15 INFO SparkContext: Added file file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar at file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1681269674469\n",
            "23/04/12 03:21:15 INFO Utils: Copying /root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-2749c8bf-e99b-445b-a265-a9739115088f/userFiles-3e64d614-08c6-4289-a015-11ebc6712678/org.spark-project.spark_unused-1.0.0.jar\n",
            "23/04/12 03:21:15 INFO SparkContext: Added file file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar at file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1681269674469\n",
            "23/04/12 03:21:15 INFO Utils: Copying /root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar to /tmp/spark-2749c8bf-e99b-445b-a265-a9739115088f/userFiles-3e64d614-08c6-4289-a015-11ebc6712678/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar\n",
            "23/04/12 03:21:15 INFO SparkContext: Added file file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar at file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1681269674469\n",
            "23/04/12 03:21:15 INFO Utils: Copying /root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar to /tmp/spark-2749c8bf-e99b-445b-a265-a9739115088f/userFiles-3e64d614-08c6-4289-a015-11ebc6712678/com.yammer.metrics_metrics-core-2.2.0.jar\n",
            "23/04/12 03:21:15 INFO SparkContext: Added file file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar at file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1681269674469\n",
            "23/04/12 03:21:15 INFO Utils: Copying /root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar to /tmp/spark-2749c8bf-e99b-445b-a265-a9739115088f/userFiles-3e64d614-08c6-4289-a015-11ebc6712678/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar\n",
            "23/04/12 03:21:15 INFO SparkContext: Added file file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar at file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar with timestamp 1681269674469\n",
            "23/04/12 03:21:15 INFO Utils: Copying /root/.ivy2/jars/com.101tec_zkclient-0.3.jar to /tmp/spark-2749c8bf-e99b-445b-a265-a9739115088f/userFiles-3e64d614-08c6-4289-a015-11ebc6712678/com.101tec_zkclient-0.3.jar\n",
            "23/04/12 03:21:15 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar at file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1681269674469\n",
            "23/04/12 03:21:15 INFO Utils: Copying /root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar to /tmp/spark-2749c8bf-e99b-445b-a265-a9739115088f/userFiles-3e64d614-08c6-4289-a015-11ebc6712678/org.apache.kafka_kafka-clients-0.8.2.1.jar\n",
            "23/04/12 03:21:15 INFO SparkContext: Added file file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar at file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1681269674469\n",
            "23/04/12 03:21:15 INFO Utils: Copying /root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar to /tmp/spark-2749c8bf-e99b-445b-a265-a9739115088f/userFiles-3e64d614-08c6-4289-a015-11ebc6712678/org.slf4j_slf4j-api-1.7.16.jar\n",
            "23/04/12 03:21:15 INFO SparkContext: Added file file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar at file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar with timestamp 1681269674469\n",
            "23/04/12 03:21:15 INFO Utils: Copying /root/.ivy2/jars/log4j_log4j-1.2.17.jar to /tmp/spark-2749c8bf-e99b-445b-a265-a9739115088f/userFiles-3e64d614-08c6-4289-a015-11ebc6712678/log4j_log4j-1.2.17.jar\n",
            "23/04/12 03:21:15 INFO SparkContext: Added file file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar at file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1681269674469\n",
            "23/04/12 03:21:15 INFO Utils: Copying /root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar to /tmp/spark-2749c8bf-e99b-445b-a265-a9739115088f/userFiles-3e64d614-08c6-4289-a015-11ebc6712678/net.jpountz.lz4_lz4-1.2.0.jar\n",
            "23/04/12 03:21:15 INFO SparkContext: Added file file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar at file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1681269674469\n",
            "23/04/12 03:21:15 INFO Utils: Copying /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar to /tmp/spark-2749c8bf-e99b-445b-a265-a9739115088f/userFiles-3e64d614-08c6-4289-a015-11ebc6712678/org.xerial.snappy_snappy-java-1.1.7.5.jar\n",
            "23/04/12 03:21:15 INFO Executor: Starting executor ID driver on host f4c7b61d3b37\n",
            "23/04/12 03:21:15 INFO Executor: Fetching file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar with timestamp 1681269674469\n",
            "23/04/12 03:21:15 INFO Utils: /root/.ivy2/jars/log4j_log4j-1.2.17.jar has been previously copied to /tmp/spark-2749c8bf-e99b-445b-a265-a9739115088f/userFiles-3e64d614-08c6-4289-a015-11ebc6712678/log4j_log4j-1.2.17.jar\n",
            "23/04/12 03:21:15 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1681269674469\n",
            "23/04/12 03:21:15 INFO Utils: /root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar has been previously copied to /tmp/spark-2749c8bf-e99b-445b-a265-a9739115088f/userFiles-3e64d614-08c6-4289-a015-11ebc6712678/org.apache.kafka_kafka-clients-0.8.2.1.jar\n",
            "23/04/12 03:21:15 INFO Executor: Fetching file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1681269674469\n",
            "23/04/12 03:21:15 INFO Utils: /root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar has been previously copied to /tmp/spark-2749c8bf-e99b-445b-a265-a9739115088f/userFiles-3e64d614-08c6-4289-a015-11ebc6712678/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar\n",
            "23/04/12 03:21:15 INFO Executor: Fetching file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1681269674469\n",
            "23/04/12 03:21:15 INFO Utils: /root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar has been previously copied to /tmp/spark-2749c8bf-e99b-445b-a265-a9739115088f/userFiles-3e64d614-08c6-4289-a015-11ebc6712678/org.slf4j_slf4j-api-1.7.16.jar\n",
            "23/04/12 03:21:15 INFO Executor: Fetching file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1681269674469\n",
            "23/04/12 03:21:15 INFO Utils: /root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar has been previously copied to /tmp/spark-2749c8bf-e99b-445b-a265-a9739115088f/userFiles-3e64d614-08c6-4289-a015-11ebc6712678/net.jpountz.lz4_lz4-1.2.0.jar\n",
            "23/04/12 03:21:15 INFO Executor: Fetching file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1681269674469\n",
            "23/04/12 03:21:15 INFO Utils: /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar has been previously copied to /tmp/spark-2749c8bf-e99b-445b-a265-a9739115088f/userFiles-3e64d614-08c6-4289-a015-11ebc6712678/org.xerial.snappy_snappy-java-1.1.7.5.jar\n",
            "23/04/12 03:21:15 INFO Executor: Fetching file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1681269674469\n",
            "23/04/12 03:21:15 INFO Utils: /root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar has been previously copied to /tmp/spark-2749c8bf-e99b-445b-a265-a9739115088f/userFiles-3e64d614-08c6-4289-a015-11ebc6712678/org.spark-project.spark_unused-1.0.0.jar\n",
            "23/04/12 03:21:15 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1681269674469\n",
            "23/04/12 03:21:15 INFO Utils: /root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar has been previously copied to /tmp/spark-2749c8bf-e99b-445b-a265-a9739115088f/userFiles-3e64d614-08c6-4289-a015-11ebc6712678/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar\n",
            "23/04/12 03:21:15 INFO Executor: Fetching file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1681269674469\n",
            "23/04/12 03:21:15 INFO Utils: /root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar has been previously copied to /tmp/spark-2749c8bf-e99b-445b-a265-a9739115088f/userFiles-3e64d614-08c6-4289-a015-11ebc6712678/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar\n",
            "23/04/12 03:21:15 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1681269674469\n",
            "23/04/12 03:21:15 INFO Utils: /root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar has been previously copied to /tmp/spark-2749c8bf-e99b-445b-a265-a9739115088f/userFiles-3e64d614-08c6-4289-a015-11ebc6712678/org.apache.kafka_kafka_2.11-0.8.2.1.jar\n",
            "23/04/12 03:21:15 INFO Executor: Fetching file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar with timestamp 1681269674469\n",
            "23/04/12 03:21:15 INFO Utils: /root/.ivy2/jars/com.101tec_zkclient-0.3.jar has been previously copied to /tmp/spark-2749c8bf-e99b-445b-a265-a9739115088f/userFiles-3e64d614-08c6-4289-a015-11ebc6712678/com.101tec_zkclient-0.3.jar\n",
            "23/04/12 03:21:15 INFO Executor: Fetching file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1681269674469\n",
            "23/04/12 03:21:15 INFO Utils: /root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar has been previously copied to /tmp/spark-2749c8bf-e99b-445b-a265-a9739115088f/userFiles-3e64d614-08c6-4289-a015-11ebc6712678/com.yammer.metrics_metrics-core-2.2.0.jar\n",
            "23/04/12 03:21:15 INFO Executor: Fetching spark://f4c7b61d3b37:41073/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1681269674469\n",
            "23/04/12 03:21:15 INFO TransportClientFactory: Successfully created connection to f4c7b61d3b37/172.28.0.12:41073 after 33 ms (0 ms spent in bootstraps)\n",
            "23/04/12 03:21:15 INFO Utils: Fetching spark://f4c7b61d3b37:41073/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar to /tmp/spark-2749c8bf-e99b-445b-a265-a9739115088f/userFiles-3e64d614-08c6-4289-a015-11ebc6712678/fetchFileTemp1561706985930593559.tmp\n",
            "23/04/12 03:21:16 INFO Utils: /tmp/spark-2749c8bf-e99b-445b-a265-a9739115088f/userFiles-3e64d614-08c6-4289-a015-11ebc6712678/fetchFileTemp1561706985930593559.tmp has been previously copied to /tmp/spark-2749c8bf-e99b-445b-a265-a9739115088f/userFiles-3e64d614-08c6-4289-a015-11ebc6712678/org.xerial.snappy_snappy-java-1.1.7.5.jar\n",
            "23/04/12 03:21:16 INFO Executor: Adding file:/tmp/spark-2749c8bf-e99b-445b-a265-a9739115088f/userFiles-3e64d614-08c6-4289-a015-11ebc6712678/org.xerial.snappy_snappy-java-1.1.7.5.jar to class loader\n",
            "23/04/12 03:21:16 INFO Executor: Fetching spark://f4c7b61d3b37:41073/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1681269674469\n",
            "23/04/12 03:21:16 INFO Utils: Fetching spark://f4c7b61d3b37:41073/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar to /tmp/spark-2749c8bf-e99b-445b-a265-a9739115088f/userFiles-3e64d614-08c6-4289-a015-11ebc6712678/fetchFileTemp195581058183146429.tmp\n",
            "23/04/12 03:21:16 INFO Utils: /tmp/spark-2749c8bf-e99b-445b-a265-a9739115088f/userFiles-3e64d614-08c6-4289-a015-11ebc6712678/fetchFileTemp195581058183146429.tmp has been previously copied to /tmp/spark-2749c8bf-e99b-445b-a265-a9739115088f/userFiles-3e64d614-08c6-4289-a015-11ebc6712678/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar\n",
            "23/04/12 03:21:16 INFO Executor: Adding file:/tmp/spark-2749c8bf-e99b-445b-a265-a9739115088f/userFiles-3e64d614-08c6-4289-a015-11ebc6712678/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar to class loader\n",
            "23/04/12 03:21:16 INFO Executor: Fetching spark://f4c7b61d3b37:41073/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1681269674469\n",
            "23/04/12 03:21:16 INFO Utils: Fetching spark://f4c7b61d3b37:41073/jars/org.slf4j_slf4j-api-1.7.16.jar to /tmp/spark-2749c8bf-e99b-445b-a265-a9739115088f/userFiles-3e64d614-08c6-4289-a015-11ebc6712678/fetchFileTemp7967684192008497788.tmp\n",
            "23/04/12 03:21:16 INFO Utils: /tmp/spark-2749c8bf-e99b-445b-a265-a9739115088f/userFiles-3e64d614-08c6-4289-a015-11ebc6712678/fetchFileTemp7967684192008497788.tmp has been previously copied to /tmp/spark-2749c8bf-e99b-445b-a265-a9739115088f/userFiles-3e64d614-08c6-4289-a015-11ebc6712678/org.slf4j_slf4j-api-1.7.16.jar\n",
            "23/04/12 03:21:16 INFO Executor: Adding file:/tmp/spark-2749c8bf-e99b-445b-a265-a9739115088f/userFiles-3e64d614-08c6-4289-a015-11ebc6712678/org.slf4j_slf4j-api-1.7.16.jar to class loader\n",
            "23/04/12 03:21:16 INFO Executor: Fetching spark://f4c7b61d3b37:41073/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1681269674469\n",
            "23/04/12 03:21:16 INFO Utils: Fetching spark://f4c7b61d3b37:41073/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar to /tmp/spark-2749c8bf-e99b-445b-a265-a9739115088f/userFiles-3e64d614-08c6-4289-a015-11ebc6712678/fetchFileTemp1870736678833999409.tmp\n",
            "23/04/12 03:21:16 INFO Utils: /tmp/spark-2749c8bf-e99b-445b-a265-a9739115088f/userFiles-3e64d614-08c6-4289-a015-11ebc6712678/fetchFileTemp1870736678833999409.tmp has been previously copied to /tmp/spark-2749c8bf-e99b-445b-a265-a9739115088f/userFiles-3e64d614-08c6-4289-a015-11ebc6712678/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar\n",
            "23/04/12 03:21:16 INFO Executor: Adding file:/tmp/spark-2749c8bf-e99b-445b-a265-a9739115088f/userFiles-3e64d614-08c6-4289-a015-11ebc6712678/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar to class loader\n",
            "23/04/12 03:21:16 INFO Executor: Fetching spark://f4c7b61d3b37:41073/jars/log4j_log4j-1.2.17.jar with timestamp 1681269674469\n",
            "23/04/12 03:21:16 INFO Utils: Fetching spark://f4c7b61d3b37:41073/jars/log4j_log4j-1.2.17.jar to /tmp/spark-2749c8bf-e99b-445b-a265-a9739115088f/userFiles-3e64d614-08c6-4289-a015-11ebc6712678/fetchFileTemp8353735727409208563.tmp\n",
            "23/04/12 03:21:16 INFO Utils: /tmp/spark-2749c8bf-e99b-445b-a265-a9739115088f/userFiles-3e64d614-08c6-4289-a015-11ebc6712678/fetchFileTemp8353735727409208563.tmp has been previously copied to /tmp/spark-2749c8bf-e99b-445b-a265-a9739115088f/userFiles-3e64d614-08c6-4289-a015-11ebc6712678/log4j_log4j-1.2.17.jar\n",
            "23/04/12 03:21:16 INFO Executor: Adding file:/tmp/spark-2749c8bf-e99b-445b-a265-a9739115088f/userFiles-3e64d614-08c6-4289-a015-11ebc6712678/log4j_log4j-1.2.17.jar to class loader\n",
            "23/04/12 03:21:16 INFO Executor: Fetching spark://f4c7b61d3b37:41073/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1681269674469\n",
            "23/04/12 03:21:16 INFO Utils: Fetching spark://f4c7b61d3b37:41073/jars/com.yammer.metrics_metrics-core-2.2.0.jar to /tmp/spark-2749c8bf-e99b-445b-a265-a9739115088f/userFiles-3e64d614-08c6-4289-a015-11ebc6712678/fetchFileTemp3340282311704288748.tmp\n",
            "23/04/12 03:21:16 INFO Utils: /tmp/spark-2749c8bf-e99b-445b-a265-a9739115088f/userFiles-3e64d614-08c6-4289-a015-11ebc6712678/fetchFileTemp3340282311704288748.tmp has been previously copied to /tmp/spark-2749c8bf-e99b-445b-a265-a9739115088f/userFiles-3e64d614-08c6-4289-a015-11ebc6712678/com.yammer.metrics_metrics-core-2.2.0.jar\n",
            "23/04/12 03:21:16 INFO Executor: Adding file:/tmp/spark-2749c8bf-e99b-445b-a265-a9739115088f/userFiles-3e64d614-08c6-4289-a015-11ebc6712678/com.yammer.metrics_metrics-core-2.2.0.jar to class loader\n",
            "23/04/12 03:21:16 INFO Executor: Fetching spark://f4c7b61d3b37:41073/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1681269674469\n",
            "23/04/12 03:21:16 INFO Utils: Fetching spark://f4c7b61d3b37:41073/jars/net.jpountz.lz4_lz4-1.2.0.jar to /tmp/spark-2749c8bf-e99b-445b-a265-a9739115088f/userFiles-3e64d614-08c6-4289-a015-11ebc6712678/fetchFileTemp4118306892002921133.tmp\n",
            "23/04/12 03:21:16 INFO Utils: /tmp/spark-2749c8bf-e99b-445b-a265-a9739115088f/userFiles-3e64d614-08c6-4289-a015-11ebc6712678/fetchFileTemp4118306892002921133.tmp has been previously copied to /tmp/spark-2749c8bf-e99b-445b-a265-a9739115088f/userFiles-3e64d614-08c6-4289-a015-11ebc6712678/net.jpountz.lz4_lz4-1.2.0.jar\n",
            "23/04/12 03:21:16 INFO Executor: Adding file:/tmp/spark-2749c8bf-e99b-445b-a265-a9739115088f/userFiles-3e64d614-08c6-4289-a015-11ebc6712678/net.jpountz.lz4_lz4-1.2.0.jar to class loader\n",
            "23/04/12 03:21:16 INFO Executor: Fetching spark://f4c7b61d3b37:41073/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1681269674469\n",
            "23/04/12 03:21:16 INFO Utils: Fetching spark://f4c7b61d3b37:41073/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-2749c8bf-e99b-445b-a265-a9739115088f/userFiles-3e64d614-08c6-4289-a015-11ebc6712678/fetchFileTemp6429611600249535555.tmp\n",
            "23/04/12 03:21:16 INFO Utils: /tmp/spark-2749c8bf-e99b-445b-a265-a9739115088f/userFiles-3e64d614-08c6-4289-a015-11ebc6712678/fetchFileTemp6429611600249535555.tmp has been previously copied to /tmp/spark-2749c8bf-e99b-445b-a265-a9739115088f/userFiles-3e64d614-08c6-4289-a015-11ebc6712678/org.spark-project.spark_unused-1.0.0.jar\n",
            "23/04/12 03:21:16 INFO Executor: Adding file:/tmp/spark-2749c8bf-e99b-445b-a265-a9739115088f/userFiles-3e64d614-08c6-4289-a015-11ebc6712678/org.spark-project.spark_unused-1.0.0.jar to class loader\n",
            "23/04/12 03:21:16 INFO Executor: Fetching spark://f4c7b61d3b37:41073/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1681269674469\n",
            "23/04/12 03:21:16 INFO Utils: Fetching spark://f4c7b61d3b37:41073/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar to /tmp/spark-2749c8bf-e99b-445b-a265-a9739115088f/userFiles-3e64d614-08c6-4289-a015-11ebc6712678/fetchFileTemp6755215421403541309.tmp\n",
            "23/04/12 03:21:16 INFO Utils: /tmp/spark-2749c8bf-e99b-445b-a265-a9739115088f/userFiles-3e64d614-08c6-4289-a015-11ebc6712678/fetchFileTemp6755215421403541309.tmp has been previously copied to /tmp/spark-2749c8bf-e99b-445b-a265-a9739115088f/userFiles-3e64d614-08c6-4289-a015-11ebc6712678/org.apache.kafka_kafka_2.11-0.8.2.1.jar\n",
            "23/04/12 03:21:16 INFO Executor: Adding file:/tmp/spark-2749c8bf-e99b-445b-a265-a9739115088f/userFiles-3e64d614-08c6-4289-a015-11ebc6712678/org.apache.kafka_kafka_2.11-0.8.2.1.jar to class loader\n",
            "23/04/12 03:21:16 INFO Executor: Fetching spark://f4c7b61d3b37:41073/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1681269674469\n",
            "23/04/12 03:21:16 INFO Utils: Fetching spark://f4c7b61d3b37:41073/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar to /tmp/spark-2749c8bf-e99b-445b-a265-a9739115088f/userFiles-3e64d614-08c6-4289-a015-11ebc6712678/fetchFileTemp2586582865511217388.tmp\n",
            "23/04/12 03:21:16 INFO Utils: /tmp/spark-2749c8bf-e99b-445b-a265-a9739115088f/userFiles-3e64d614-08c6-4289-a015-11ebc6712678/fetchFileTemp2586582865511217388.tmp has been previously copied to /tmp/spark-2749c8bf-e99b-445b-a265-a9739115088f/userFiles-3e64d614-08c6-4289-a015-11ebc6712678/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar\n",
            "23/04/12 03:21:16 INFO Executor: Adding file:/tmp/spark-2749c8bf-e99b-445b-a265-a9739115088f/userFiles-3e64d614-08c6-4289-a015-11ebc6712678/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar to class loader\n",
            "23/04/12 03:21:16 INFO Executor: Fetching spark://f4c7b61d3b37:41073/jars/com.101tec_zkclient-0.3.jar with timestamp 1681269674469\n",
            "23/04/12 03:21:16 INFO Utils: Fetching spark://f4c7b61d3b37:41073/jars/com.101tec_zkclient-0.3.jar to /tmp/spark-2749c8bf-e99b-445b-a265-a9739115088f/userFiles-3e64d614-08c6-4289-a015-11ebc6712678/fetchFileTemp8156655626584284378.tmp\n",
            "23/04/12 03:21:16 INFO Utils: /tmp/spark-2749c8bf-e99b-445b-a265-a9739115088f/userFiles-3e64d614-08c6-4289-a015-11ebc6712678/fetchFileTemp8156655626584284378.tmp has been previously copied to /tmp/spark-2749c8bf-e99b-445b-a265-a9739115088f/userFiles-3e64d614-08c6-4289-a015-11ebc6712678/com.101tec_zkclient-0.3.jar\n",
            "23/04/12 03:21:16 INFO Executor: Adding file:/tmp/spark-2749c8bf-e99b-445b-a265-a9739115088f/userFiles-3e64d614-08c6-4289-a015-11ebc6712678/com.101tec_zkclient-0.3.jar to class loader\n",
            "23/04/12 03:21:16 INFO Executor: Fetching spark://f4c7b61d3b37:41073/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1681269674469\n",
            "23/04/12 03:21:16 INFO Utils: Fetching spark://f4c7b61d3b37:41073/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar to /tmp/spark-2749c8bf-e99b-445b-a265-a9739115088f/userFiles-3e64d614-08c6-4289-a015-11ebc6712678/fetchFileTemp3406291036439387038.tmp\n",
            "23/04/12 03:21:16 INFO Utils: /tmp/spark-2749c8bf-e99b-445b-a265-a9739115088f/userFiles-3e64d614-08c6-4289-a015-11ebc6712678/fetchFileTemp3406291036439387038.tmp has been previously copied to /tmp/spark-2749c8bf-e99b-445b-a265-a9739115088f/userFiles-3e64d614-08c6-4289-a015-11ebc6712678/org.apache.kafka_kafka-clients-0.8.2.1.jar\n",
            "23/04/12 03:21:16 INFO Executor: Adding file:/tmp/spark-2749c8bf-e99b-445b-a265-a9739115088f/userFiles-3e64d614-08c6-4289-a015-11ebc6712678/org.apache.kafka_kafka-clients-0.8.2.1.jar to class loader\n",
            "23/04/12 03:21:16 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39635.\n",
            "23/04/12 03:21:16 INFO NettyBlockTransferService: Server created on f4c7b61d3b37:39635\n",
            "23/04/12 03:21:16 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "23/04/12 03:21:16 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, f4c7b61d3b37, 39635, None)\n",
            "23/04/12 03:21:16 INFO BlockManagerMasterEndpoint: Registering block manager f4c7b61d3b37:39635 with 366.3 MiB RAM, BlockManagerId(driver, f4c7b61d3b37, 39635, None)\n",
            "23/04/12 03:21:16 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, f4c7b61d3b37, 39635, None)\n",
            "23/04/12 03:21:16 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, f4c7b61d3b37, 39635, None)\n",
            "23/04/12 03:21:16 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 354.3 KiB, free 366.0 MiB)\n",
            "23/04/12 03:21:16 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 32.0 KiB, free 365.9 MiB)\n",
            "23/04/12 03:21:16 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on f4c7b61d3b37:39635 (size: 32.0 KiB, free: 366.3 MiB)\n",
            "23/04/12 03:21:16 INFO SparkContext: Created broadcast 0 from textFile at NativeMethodAccessorImpl.java:0\n",
            "23/04/12 03:21:17 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 354.3 KiB, free 365.6 MiB)\n",
            "23/04/12 03:21:17 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 32.0 KiB, free 365.5 MiB)\n",
            "23/04/12 03:21:17 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on f4c7b61d3b37:39635 (size: 32.0 KiB, free: 366.2 MiB)\n",
            "23/04/12 03:21:17 INFO SparkContext: Created broadcast 1 from textFile at NativeMethodAccessorImpl.java:0\n",
            "23/04/12 03:21:17 INFO FileInputFormat: Total input files to process : 1\n",
            "23/04/12 03:21:17 INFO FileInputFormat: Total input files to process : 1\n",
            "23/04/12 03:21:17 INFO SparkContext: Starting job: collect at /content/4_join.py:46\n",
            "23/04/12 03:21:17 INFO DAGScheduler: Registering RDD 8 (groupByKey at /content/4_join.py:43) as input to shuffle 0\n",
            "23/04/12 03:21:17 INFO DAGScheduler: Got job 0 (collect at /content/4_join.py:46) with 4 output partitions\n",
            "23/04/12 03:21:17 INFO DAGScheduler: Final stage: ResultStage 1 (collect at /content/4_join.py:46)\n",
            "23/04/12 03:21:17 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)\n",
            "23/04/12 03:21:17 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 0)\n",
            "23/04/12 03:21:17 INFO DAGScheduler: Submitting ShuffleMapStage 0 (PairwiseRDD[8] at groupByKey at /content/4_join.py:43), which has no missing parents\n",
            "23/04/12 03:21:17 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 17.6 KiB, free 365.5 MiB)\n",
            "23/04/12 03:21:17 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 8.3 KiB, free 365.5 MiB)\n",
            "23/04/12 03:21:17 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on f4c7b61d3b37:39635 (size: 8.3 KiB, free: 366.2 MiB)\n",
            "23/04/12 03:21:17 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1478\n",
            "23/04/12 03:21:17 INFO DAGScheduler: Submitting 4 missing tasks from ShuffleMapStage 0 (PairwiseRDD[8] at groupByKey at /content/4_join.py:43) (first 15 tasks are for partitions Vector(0, 1, 2, 3))\n",
            "23/04/12 03:21:17 INFO TaskSchedulerImpl: Adding task set 0.0 with 4 tasks resource profile 0\n",
            "23/04/12 03:21:17 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (f4c7b61d3b37, executor driver, partition 0, PROCESS_LOCAL, 4592 bytes) taskResourceAssignments Map()\n",
            "23/04/12 03:21:17 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1) (f4c7b61d3b37, executor driver, partition 1, PROCESS_LOCAL, 4592 bytes) taskResourceAssignments Map()\n",
            "23/04/12 03:21:17 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)\n",
            "23/04/12 03:21:17 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
            "23/04/12 03:21:18 INFO HadoopRDD: Input split: file:/content/Batting.csv:3244373+3244374\n",
            "23/04/12 03:21:18 INFO HadoopRDD: Input split: file:/content/Batting.csv:0+3244373\n",
            "23/04/12 03:21:19 INFO PythonRunner: Times: total = 949, boot = 422, init = 37, finish = 490\n",
            "23/04/12 03:21:19 INFO PythonRunner: Times: total = 957, boot = 430, init = 45, finish = 482\n",
            "23/04/12 03:21:19 INFO PythonRunner: Times: total = 1074, boot = 4, init = 76, finish = 994\n",
            "23/04/12 03:21:19 INFO PythonRunner: Times: total = 1094, boot = 17, init = 39, finish = 1038\n",
            "23/04/12 03:21:19 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1657 bytes result sent to driver\n",
            "23/04/12 03:21:19 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 1614 bytes result sent to driver\n",
            "23/04/12 03:21:19 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2) (f4c7b61d3b37, executor driver, partition 2, PROCESS_LOCAL, 4593 bytes) taskResourceAssignments Map()\n",
            "23/04/12 03:21:19 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)\n",
            "23/04/12 03:21:19 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 2085 ms on f4c7b61d3b37 (executor driver) (1/4)\n",
            "23/04/12 03:21:19 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3) (f4c7b61d3b37, executor driver, partition 3, PROCESS_LOCAL, 4593 bytes) taskResourceAssignments Map()\n",
            "23/04/12 03:21:19 INFO Executor: Running task 3.0 in stage 0.0 (TID 3)\n",
            "23/04/12 03:21:19 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 2077 ms on f4c7b61d3b37 (executor driver) (2/4)\n",
            "23/04/12 03:21:19 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 54713\n",
            "23/04/12 03:21:19 INFO HadoopRDD: Input split: file:/content/Salaries.csv:350012+350012\n",
            "23/04/12 03:21:19 INFO HadoopRDD: Input split: file:/content/Salaries.csv:0+350012\n",
            "23/04/12 03:21:20 INFO PythonRunner: Times: total = 125, boot = -686, init = 695, finish = 116\n",
            "23/04/12 03:21:20 INFO PythonRunner: Times: total = 151, boot = -686, init = 721, finish = 116\n",
            "23/04/12 03:21:20 INFO PythonRunner: Times: total = 286, boot = -142, init = 178, finish = 250\n",
            "23/04/12 03:21:20 INFO Executor: Finished task 3.0 in stage 0.0 (TID 3). 1571 bytes result sent to driver\n",
            "23/04/12 03:21:20 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 358 ms on f4c7b61d3b37 (executor driver) (3/4)\n",
            "23/04/12 03:21:20 INFO PythonRunner: Times: total = 340, boot = -112, init = 169, finish = 283\n",
            "23/04/12 03:21:20 INFO Executor: Finished task 2.0 in stage 0.0 (TID 2). 1571 bytes result sent to driver\n",
            "23/04/12 03:21:20 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 440 ms on f4c7b61d3b37 (executor driver) (4/4)\n",
            "23/04/12 03:21:20 INFO DAGScheduler: ShuffleMapStage 0 (groupByKey at /content/4_join.py:43) finished in 2.742 s\n",
            "23/04/12 03:21:20 INFO DAGScheduler: looking for newly runnable stages\n",
            "23/04/12 03:21:20 INFO DAGScheduler: running: Set()\n",
            "23/04/12 03:21:20 INFO DAGScheduler: waiting: Set(ResultStage 1)\n",
            "23/04/12 03:21:20 INFO DAGScheduler: failed: Set()\n",
            "23/04/12 03:21:20 INFO DAGScheduler: Submitting ResultStage 1 (PythonRDD[11] at collect at /content/4_join.py:46), which has no missing parents\n",
            "23/04/12 03:21:20 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
            "23/04/12 03:21:20 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 12.3 KiB, free 365.5 MiB)\n",
            "23/04/12 03:21:20 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 7.1 KiB, free 365.5 MiB)\n",
            "23/04/12 03:21:20 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on f4c7b61d3b37:39635 (size: 7.1 KiB, free: 366.2 MiB)\n",
            "23/04/12 03:21:20 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1478\n",
            "23/04/12 03:21:20 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 1 (PythonRDD[11] at collect at /content/4_join.py:46) (first 15 tasks are for partitions Vector(0, 1, 2, 3))\n",
            "23/04/12 03:21:20 INFO TaskSchedulerImpl: Adding task set 1.0 with 4 tasks resource profile 0\n",
            "23/04/12 03:21:20 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 4) (f4c7b61d3b37, executor driver, partition 0, NODE_LOCAL, 4271 bytes) taskResourceAssignments Map()\n",
            "23/04/12 03:21:20 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 5) (f4c7b61d3b37, executor driver, partition 1, NODE_LOCAL, 4271 bytes) taskResourceAssignments Map()\n",
            "23/04/12 03:21:20 INFO Executor: Running task 0.0 in stage 1.0 (TID 4)\n",
            "23/04/12 03:21:20 INFO Executor: Running task 1.0 in stage 1.0 (TID 5)\n",
            "23/04/12 03:21:20 INFO ShuffleBlockFetcherIterator: Getting 4 (1231.1 KiB) non-empty blocks including 4 (1231.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/12 03:21:20 INFO ShuffleBlockFetcherIterator: Getting 4 (1231.1 KiB) non-empty blocks including 4 (1231.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/12 03:21:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 19 ms\n",
            "23/04/12 03:21:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 23 ms\n",
            "23/04/12 03:21:20 INFO PythonRunner: Times: total = 108, boot = -370, init = 379, finish = 99\n",
            "23/04/12 03:21:20 INFO Executor: Finished task 0.0 in stage 1.0 (TID 4). 2124 bytes result sent to driver\n",
            "23/04/12 03:21:20 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 6) (f4c7b61d3b37, executor driver, partition 2, NODE_LOCAL, 4271 bytes) taskResourceAssignments Map()\n",
            "23/04/12 03:21:20 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 4) in 215 ms on f4c7b61d3b37 (executor driver) (1/4)\n",
            "23/04/12 03:21:20 INFO Executor: Running task 2.0 in stage 1.0 (TID 6)\n",
            "23/04/12 03:21:20 INFO ShuffleBlockFetcherIterator: Getting 4 (1231.1 KiB) non-empty blocks including 4 (1231.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/12 03:21:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 7 ms\n",
            "23/04/12 03:21:20 INFO PythonRunner: Times: total = 151, boot = -385, init = 393, finish = 143\n",
            "23/04/12 03:21:20 INFO Executor: Finished task 1.0 in stage 1.0 (TID 5). 2697 bytes result sent to driver\n",
            "23/04/12 03:21:20 INFO TaskSetManager: Starting task 3.0 in stage 1.0 (TID 7) (f4c7b61d3b37, executor driver, partition 3, NODE_LOCAL, 4271 bytes) taskResourceAssignments Map()\n",
            "23/04/12 03:21:20 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 5) in 269 ms on f4c7b61d3b37 (executor driver) (2/4)\n",
            "23/04/12 03:21:20 INFO Executor: Running task 3.0 in stage 1.0 (TID 7)\n",
            "23/04/12 03:21:20 INFO ShuffleBlockFetcherIterator: Getting 4 (1180.8 KiB) non-empty blocks including 4 (1180.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/12 03:21:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 6 ms\n",
            "23/04/12 03:21:20 INFO PythonRunner: Times: total = 98, boot = -355, init = 364, finish = 89\n",
            "23/04/12 03:21:20 INFO Executor: Finished task 2.0 in stage 1.0 (TID 6). 2231 bytes result sent to driver\n",
            "23/04/12 03:21:20 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 6) in 133 ms on f4c7b61d3b37 (executor driver) (3/4)\n",
            "23/04/12 03:21:20 INFO PythonRunner: Times: total = 87, boot = -369, init = 372, finish = 84\n",
            "23/04/12 03:21:20 INFO Executor: Finished task 3.0 in stage 1.0 (TID 7). 2154 bytes result sent to driver\n",
            "23/04/12 03:21:20 INFO TaskSetManager: Finished task 3.0 in stage 1.0 (TID 7) in 135 ms on f4c7b61d3b37 (executor driver) (4/4)\n",
            "23/04/12 03:21:20 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
            "23/04/12 03:21:20 INFO DAGScheduler: ResultStage 1 (collect at /content/4_join.py:46) finished in 0.426 s\n",
            "23/04/12 03:21:20 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "23/04/12 03:21:20 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n",
            "23/04/12 03:21:20 INFO DAGScheduler: Job 0 finished: collect at /content/4_join.py:46, took 3.310107 s\n",
            "23/04/12 03:21:20 INFO deprecation: mapred.output.dir is deprecated. Instead, use mapreduce.output.fileoutputformat.outputdir\n",
            "23/04/12 03:21:20 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
            "23/04/12 03:21:20 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "23/04/12 03:21:20 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "23/04/12 03:21:20 INFO SparkContext: Starting job: runJob at SparkHadoopWriter.scala:83\n",
            "23/04/12 03:21:20 INFO DAGScheduler: Registering RDD 14 (coalesce at NativeMethodAccessorImpl.java:0) as input to shuffle 1\n",
            "23/04/12 03:21:20 INFO DAGScheduler: Got job 1 (runJob at SparkHadoopWriter.scala:83) with 1 output partitions\n",
            "23/04/12 03:21:20 INFO DAGScheduler: Final stage: ResultStage 3 (runJob at SparkHadoopWriter.scala:83)\n",
            "23/04/12 03:21:20 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)\n",
            "23/04/12 03:21:20 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 2)\n",
            "23/04/12 03:21:21 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[14] at coalesce at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/04/12 03:21:21 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 7.8 KiB, free 365.5 MiB)\n",
            "23/04/12 03:21:21 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 4.8 KiB, free 365.5 MiB)\n",
            "23/04/12 03:21:21 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on f4c7b61d3b37:39635 (size: 4.8 KiB, free: 366.2 MiB)\n",
            "23/04/12 03:21:21 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1478\n",
            "23/04/12 03:21:21 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[14] at coalesce at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
            "23/04/12 03:21:21 INFO TaskSchedulerImpl: Adding task set 2.0 with 2 tasks resource profile 0\n",
            "23/04/12 03:21:21 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 8) (f4c7b61d3b37, executor driver, partition 0, PROCESS_LOCAL, 5749 bytes) taskResourceAssignments Map()\n",
            "23/04/12 03:21:21 INFO TaskSetManager: Starting task 1.0 in stage 2.0 (TID 9) (f4c7b61d3b37, executor driver, partition 1, PROCESS_LOCAL, 5722 bytes) taskResourceAssignments Map()\n",
            "23/04/12 03:21:21 INFO Executor: Running task 0.0 in stage 2.0 (TID 8)\n",
            "23/04/12 03:21:21 INFO Executor: Running task 1.0 in stage 2.0 (TID 9)\n",
            "23/04/12 03:21:21 INFO PythonRunner: Times: total = 7, boot = -454, init = 461, finish = 0\n",
            "23/04/12 03:21:21 INFO Executor: Finished task 1.0 in stage 2.0 (TID 9). 1482 bytes result sent to driver\n",
            "23/04/12 03:21:21 INFO PythonRunner: Times: total = 3, boot = -466, init = 469, finish = 0\n",
            "23/04/12 03:21:21 INFO Executor: Finished task 0.0 in stage 2.0 (TID 8). 1482 bytes result sent to driver\n",
            "23/04/12 03:21:21 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 8) in 85 ms on f4c7b61d3b37 (executor driver) (1/2)\n",
            "23/04/12 03:21:21 INFO TaskSetManager: Finished task 1.0 in stage 2.0 (TID 9) in 81 ms on f4c7b61d3b37 (executor driver) (2/2)\n",
            "23/04/12 03:21:21 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
            "23/04/12 03:21:21 INFO DAGScheduler: ShuffleMapStage 2 (coalesce at NativeMethodAccessorImpl.java:0) finished in 0.110 s\n",
            "23/04/12 03:21:21 INFO DAGScheduler: looking for newly runnable stages\n",
            "23/04/12 03:21:21 INFO DAGScheduler: running: Set()\n",
            "23/04/12 03:21:21 INFO DAGScheduler: waiting: Set(ResultStage 3)\n",
            "23/04/12 03:21:21 INFO DAGScheduler: failed: Set()\n",
            "23/04/12 03:21:21 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[20] at saveAsTextFile at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/04/12 03:21:21 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 105.9 KiB, free 365.4 MiB)\n",
            "23/04/12 03:21:21 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 39.6 KiB, free 365.3 MiB)\n",
            "23/04/12 03:21:21 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on f4c7b61d3b37:39635 (size: 39.6 KiB, free: 366.2 MiB)\n",
            "23/04/12 03:21:21 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1478\n",
            "23/04/12 03:21:21 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[20] at saveAsTextFile at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/04/12 03:21:21 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0\n",
            "23/04/12 03:21:21 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 10) (f4c7b61d3b37, executor driver, partition 0, NODE_LOCAL, 4547 bytes) taskResourceAssignments Map()\n",
            "23/04/12 03:21:21 INFO Executor: Running task 0.0 in stage 3.0 (TID 10)\n",
            "23/04/12 03:21:21 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
            "23/04/12 03:21:21 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "23/04/12 03:21:21 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "23/04/12 03:21:21 INFO ShuffleBlockFetcherIterator: Getting 2 (1910.0 B) non-empty blocks including 2 (1910.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/12 03:21:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms\n",
            "23/04/12 03:21:21 INFO PythonRunner: Times: total = 36, boot = -627, init = 663, finish = 0\n",
            "23/04/12 03:21:21 INFO FileOutputCommitter: Saved output of task 'attempt_202304120321205482930662298370719_0020_m_000000_0' to file:/content/4_join.out/_temporary/0/task_202304120321205482930662298370719_0020_m_000000\n",
            "23/04/12 03:21:21 INFO SparkHadoopMapRedUtil: attempt_202304120321205482930662298370719_0020_m_000000_0: Committed\n",
            "23/04/12 03:21:21 INFO Executor: Finished task 0.0 in stage 3.0 (TID 10). 1952 bytes result sent to driver\n",
            "23/04/12 03:21:21 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 10) in 318 ms on f4c7b61d3b37 (executor driver) (1/1)\n",
            "23/04/12 03:21:21 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
            "23/04/12 03:21:21 INFO DAGScheduler: ResultStage 3 (runJob at SparkHadoopWriter.scala:83) finished in 0.375 s\n",
            "23/04/12 03:21:21 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "23/04/12 03:21:21 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished\n",
            "23/04/12 03:21:21 INFO DAGScheduler: Job 1 finished: runJob at SparkHadoopWriter.scala:83, took 0.513066 s\n",
            "23/04/12 03:21:21 INFO SparkHadoopWriter: Start to commit write Job job_202304120321205482930662298370719_0020.\n",
            "23/04/12 03:21:21 INFO SparkHadoopWriter: Write Job job_202304120321205482930662298370719_0020 committed. Elapsed time: 38 ms.\n",
            "23/04/12 03:21:21 INFO SparkUI: Stopped Spark web UI at http://f4c7b61d3b37:4040\n",
            "23/04/12 03:21:21 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "23/04/12 03:21:21 INFO MemoryStore: MemoryStore cleared\n",
            "23/04/12 03:21:21 INFO BlockManager: BlockManager stopped\n",
            "23/04/12 03:21:21 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "23/04/12 03:21:21 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "23/04/12 03:21:21 INFO SparkContext: Successfully stopped SparkContext\n",
            "23/04/12 03:21:22 INFO ShutdownHookManager: Shutdown hook called\n",
            "23/04/12 03:21:22 INFO ShutdownHookManager: Deleting directory /tmp/spark-2749c8bf-e99b-445b-a265-a9739115088f/pyspark-26cb3468-a830-49e7-be24-e74e00369316\n",
            "23/04/12 03:21:22 INFO ShutdownHookManager: Deleting directory /tmp/spark-2749c8bf-e99b-445b-a265-a9739115088f\n",
            "23/04/12 03:21:22 INFO ShutdownHookManager: Deleting directory /tmp/spark-7dc9a8df-044c-439c-b530-cccdd85783de\n"
          ]
        }
      ],
      "source": [
        "# execute locally\n",
        "!spark-submit --packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.4.7 4_join.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DrHOt9VOsRne"
      },
      "outputs": [],
      "source": [
        "# test locally\n",
        "results = [('join', 'bondsba01,1991,1,PIT,NL,153,153,510,95,149,28,5,25,116,43,13,107,73,25,4,0,13,8,1531991,PIT,NL,bondsba01,2300000'),\n",
        "('join', 'bondsba01,1993,1,SFN,NL,159,159,539,129,181,38,4,46,123,29,12,126,79,43,2,0,7,11,1591993,SFN,NL,bondsba01,4516666'),\n",
        "('join', 'bondsba01,2002,1,SFN,NL,143,143,403,117,149,31,2,46,110,9,2,198,47,68,9,0,2,4,1432002,SFN,NL,bondsba01,15000000'),\n",
        "('join', 'bondsba01,2004,1,SFN,NL,147,147,373,129,135,27,3,45,101,6,1,232,41,120,9,0,3,5,1472004,SFN,NL,bondsba01,18000000'),\n",
        "('join', 'bondsba01,1986,1,PIT,NL,113,113,413,72,92,26,3,16,48,36,7,65,102,2,2,2,2,4,1131986,PIT,NL,bondsba01,60000'),\n",
        "('join', 'bondsba01,1996,1,SFN,NL,158,158,517,122,159,27,3,42,129,40,7,151,76,30,1,0,6,11,1581996,SFN,NL,bondsba01,8416667'),\n",
        "('join', 'bondsba01,1997,1,SFN,NL,159,159,532,123,155,26,5,40,101,37,8,145,87,34,8,0,5,13,1591997,SFN,NL,bondsba01,8666667'),\n",
        "('join', 'bondsba01,1999,1,SFN,NL,102,102,355,91,93,20,2,34,83,15,2,73,62,9,3,0,3,6,1021999,SFN,NL,bondsba01,9381057'),\n",
        "('join', 'bondsba01,1990,1,PIT,NL,151,151,519,104,156,32,3,33,114,52,13,93,83,15,3,0,6,8,1511990,PIT,NL,bondsba01,850000'),\n",
        "('join', 'bondsba01,1994,1,SFN,NL,112,112,391,89,122,18,1,37,81,29,9,74,43,18,6,0,3,3,1121994,SFN,NL,bondsba01,5166666'),\n",
        "('join', 'bondsba01,1995,1,SFN,NL,144,144,506,109,149,30,7,33,104,31,10,120,83,22,5,0,4,12,1441995,SFN,NL,bondsba01,8166666'),\n",
        "('join', 'bondsba01,2003,1,SFN,NL,130,130,390,111,133,22,1,45,90,7,0,148,58,61,10,0,2,7,1302003,SFN,NL,bondsba01,15500000'),\n",
        "('join', 'bondsba01,2007,1,SFN,NL,126,126,340,75,94,14,0,28,66,5,0,132,54,43,3,0,2,13,1262007,SFN,NL,bondsba01,15533970'),\n",
        "('join', 'bondsba01,1987,1,PIT,NL,150,150,551,99,144,34,9,25,59,32,10,54,88,3,3,0,3,4,1501987,PIT,NL,bondsba01,100000'),\n",
        "('join', 'bondsba01,1988,1,PIT,NL,144,144,538,97,152,30,5,24,58,17,11,72,82,14,2,0,2,3,1441988,PIT,NL,bondsba01,220000'),\n",
        "('join', 'bondsba01,1989,1,PIT,NL,159,159,580,96,144,34,6,19,58,32,10,93,93,22,1,1,4,9,1591989,PIT,NL,bondsba01,360000'),\n",
        "('join', 'bondsba01,1992,1,PIT,NL,140,140,473,109,147,36,5,34,103,39,8,127,69,32,5,0,7,9,1401992,PIT,NL,bondsba01,4800000'),\n",
        "('join', 'bondsba01,1998,1,SFN,NL,156,156,552,120,167,44,7,37,122,28,12,130,92,29,8,1,6,15,1561998,SFN,NL,bondsba01,8916667'),\n",
        "('join', 'bondsba01,2000,1,SFN,NL,143,143,480,129,147,28,4,49,106,11,3,117,77,22,3,0,7,6,1432000,SFN,NL,bondsba01,10658826'),\n",
        "('join', 'bondsba01,2001,1,SFN,NL,153,153,476,129,156,32,2,73,137,13,3,177,93,35,9,0,2,5,1532001,SFN,NL,bondsba01,10300000'),\n",
        "('join', 'bondsba01,2005,1,SFN,NL,14,14,42,8,12,1,0,5,10,0,0,9,6,3,0,0,1,0,142005,SFN,NL,bondsba01,22000000'),\n",
        "('join', 'bondsba01,2006,1,SFN,NL,130,130,367,74,99,23,0,26,77,3,0,115,51,38,10,0,1,9,1302006,SFN,NL,bondsba01,19331470')]\n",
        "def test4(lines):\n",
        "  global results\n",
        "  results = [str(x) for x in results]\n",
        "  find_lines = 0\n",
        "  for  line in lines:\n",
        "    if line.strip() in results:\n",
        "        find_lines += 1\n",
        "  if find_lines != 22:\n",
        "      assert False\n",
        "  print('test passed')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XpkDNQCb__D3",
        "outputId": "c391b290-68b1-46c4-eae2-f44f5dc99580"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "test passed\n"
          ]
        }
      ],
      "source": [
        "with open('4_join.out/part-00000') as f:\n",
        "    lines = f.readlines()\n",
        "    test4(lines)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ed2gw0o387bf"
      },
      "source": [
        "# Step 3 Execute the same scripts on EMR.\n",
        "\n",
        "* Make sure that you have created an EMR cluster using the instructions in the main readme.\n",
        "* upload the main data to s3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dyeTsQLV-B-u"
      },
      "source": [
        "## Setup AWS Credentials and fill them here. Make sure you do not save this information back to github"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bMEsJXTU__Dx"
      },
      "outputs": [],
      "source": [
        "# Please fill your aws credential information here\n",
        "credentials = {\n",
        "    'region_name': 'us-east-1',\n",
        "    'aws_access_key_id': 'ASIA5D6DSQAUH2Y53ITH',\n",
        "    'aws_secret_access_key': 'hIdLVHrNBjXfFtS6Id49gu7Vr8DlrSAok/j0fNi5',\n",
        "    'aws_session_token': 'FwoGZXIvYXdzEO///////////wEaDDJl6Ua5q+p/XJh5rSLKAR4vU6L+OAtIV0gQdjxJ9phA8etJdhD8Bn9Ze8HQvtxYathGMp6Yan7viYKYa2Gtdc8l01iy3+nozLEBPTuep705KNQmccS1jKo80HcriJe0vOB6SuFpuCtouMfTz0mQKZVC6gsm/jWiXp0YO8+w6DXLCikCKyiFlf2znrIrh7mefFjt3MXv0C7/wbZO4XhuwJfZJW5XamLZBI2losoiwsG0LUqdyfDgS1315aKNnxTFzNyBTn2AuZwqFcM5+fTJqoIVpE1RRj13LQIo2tHcoQYyLcyHRWD6ZXlBGzweTzFO/8fo41jMkz4/fJIM2BMMxB2qZCb/moAQZlsT4wPsHA=='\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yRJjVwtO__Dx",
        "outputId": "54841394-653a-45c1-c839-d8e7f2f49c1d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting boto3\n",
            "  Downloading boto3-1.26.112-py3-none-any.whl (135 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.6/135.6 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting botocore<1.30.0,>=1.29.112\n",
            "  Downloading botocore-1.29.112-py3-none-any.whl (10.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.6/10.6 MB\u001b[0m \u001b[31m77.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting s3transfer<0.7.0,>=0.6.0\n",
            "  Downloading s3transfer-0.6.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.6/79.6 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.9/dist-packages (from botocore<1.30.0,>=1.29.112->boto3) (2.8.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /usr/local/lib/python3.9/dist-packages (from botocore<1.30.0,>=1.29.112->boto3) (1.26.15)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.30.0,>=1.29.112->boto3) (1.16.0)\n",
            "Installing collected packages: jmespath, botocore, s3transfer, boto3\n",
            "Successfully installed boto3-1.26.112 botocore-1.29.112 jmespath-1.0.1 s3transfer-0.6.0\n"
          ]
        }
      ],
      "source": [
        "!pip install boto3\n",
        "import boto3, json\n",
        "\n",
        "session = boto3.session.Session(**credentials)\n",
        "s3 = session.client('s3')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mss_SpU89_eo"
      },
      "source": [
        "## Upload Data to S3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8MzsIwij__Dy"
      },
      "outputs": [],
      "source": [
        "# upload tweets dataset to S3, please replace the bucket name and object keys with yours\n",
        "s3.upload_file(Filename='nashville-tweets-2019-01-28', Bucket='vandy-bigdata-2', Key='nashville-tweets-2019-01-28')\n",
        "s3.upload_file(Filename='Batting.csv', Bucket='vandy-bigdata-2', Key='Batting.csv')\n",
        "s3.upload_file(Filename='Salaries.csv', Bucket='vandy-bigdata-2', Key='Salaries.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lljwHyOy__D2"
      },
      "outputs": [],
      "source": [
        "# replae with your EMR cluster ID\n",
        "CLUSTER_ID = 'j-3JBU686HGGWY1'\n",
        "\n",
        "def submit_job(app_name, pyfile_uri):\n",
        "    emr = session.client('emr')\n",
        "    emr.add_job_flow_steps(JobFlowId=CLUSTER_ID, Steps=[{\n",
        "        'Name': app_name,\n",
        "        'ActionOnFailure': 'CANCEL_AND_WAIT',\n",
        "        'HadoopJarStep': {\n",
        "            'Args': ['spark-submit',\n",
        "                     '--master', 'yarn',\n",
        "                     '--deploy-mode', 'cluster',\n",
        "                     pyfile_uri],\n",
        "            'Jar': 'command-runner.jar'\n",
        "        }}])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PrBnuVrdl-nb"
      },
      "outputs": [],
      "source": [
        "# upload script to S3\n",
        "s3.upload_file(Filename='1_count.py', Bucket='vandy-bigdata-2', Key='hw6/1_count.py')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3JGJ6K0Cl_Ph"
      },
      "outputs": [],
      "source": [
        "# submit spark job to emr\n",
        "submit_job(app_name='1_count', pyfile_uri='s3://vandy-bigdata-2/hw6/1_count.py')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fdQzRsqama14",
        "outputId": "be3931b0-5704-4788-dfe3-0356841978b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "passed\n"
          ]
        }
      ],
      "source": [
        "# test emr execution results\n",
        "output_key = \"hw6/1_count.out/part-00000\"\n",
        "lines = s3.get_object(Bucket='vandy-bigdata-2', Key=output_key)['Body'].read().decode().splitlines()\n",
        "test1(lines)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pCzOGzTj__D1"
      },
      "outputs": [],
      "source": [
        "# upload script to S3\n",
        "s3.upload_file(Filename='2_group.py', Bucket='vandy-bigdata-2', Key='hw6/2_group.py')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eoqkfcEn__D1"
      },
      "outputs": [],
      "source": [
        "# submit spark job to emr\n",
        "submit_job(app_name='2_group', pyfile_uri='s3://vandy-bigdata-2/hw6/2_group.py')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gm7iAdlL__D1",
        "outputId": "b6981dff-9f16-4ea5-957f-2dd49e55fb5b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "passed\n"
          ]
        }
      ],
      "source": [
        "# test emr execution results\n",
        "output_key = \"hw6/2_group.out/part-00003\"\n",
        "lines = s3.get_object(Bucket='vandy-bigdata-2', Key=output_key)['Body'].read().decode().splitlines()\n",
        "test2(lines)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4VUV5ZIp__D2"
      },
      "outputs": [],
      "source": [
        "# upload script to S3 - Make sure that the S3 bucket name is changed to your own bucket\n",
        "s3.upload_file(Filename='3_days.py', Bucket='vandy-bigdata-2', Key='hw6/3_days.py')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YFWrjexF__D2"
      },
      "outputs": [],
      "source": [
        "# submit spark job to emr\n",
        "submit_job(app_name='3_days', pyfile_uri='s3://vandy-bigdata-2/hw6/3_days.py')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V1eFnxXZ__D3",
        "outputId": "28ed25ae-b573-46c2-eb73-174b5969714c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "passed\n"
          ]
        }
      ],
      "source": [
        "# test emr execution results\n",
        "output_key = \"hw6/3_days.out/part-00000\"\n",
        "lines = s3.get_object(Bucket='vandy-bigdata-2', Key=output_key)['Body'].read().decode().splitlines()\n",
        "test3(lines)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Dm0hI6OrpOT"
      },
      "outputs": [],
      "source": [
        "# upload script to S3 - Make sure that the S3 bucket name is changed to your own bucket\n",
        "s3.upload_file(Filename='4_join.py', Bucket='vandy-bigdata-2', Key='hw6/4_join.py')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OkQiNiXW__D4"
      },
      "outputs": [],
      "source": [
        "# submit spark job to emr\n",
        "submit_job(app_name='4_join', pyfile_uri='s3://vandy-bigdata-2/hw6/4_join.py')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VqyNRexw__D4",
        "outputId": "39109a6f-dc8a-4dde-d0b1-b75919faaec9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "test passed\n"
          ]
        }
      ],
      "source": [
        "# test emr execution results\n",
        "output_key = \"hw6/4_join.out/part-00000\"\n",
        "lines = s3.get_object(Bucket='vandy-bigdata-2', Key=output_key)['Body'].read().decode().splitlines()\n",
        "test4(lines)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "spark_jobs.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7-final"
    },
    "orig_nbformat": 2
  },
  "nbformat": 4,
  "nbformat_minor": 0
}