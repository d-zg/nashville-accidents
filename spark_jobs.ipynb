{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7-final"
    },
    "orig_nbformat": 2,
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "colab": {
      "name": "spark_jobs.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FateenAnam/Big-Data-Final-Project/blob/master/spark_jobs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rRZIkSqT__Dv"
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#install spark. we are using the one that uses hadoop as the underlying scheduler.\n",
        "!wget -q https://downloads.apache.org/spark/spark-3.2.3/spark-3.2.3-bin-hadoop3.2.tgz\n",
        "!tar xf  spark-3.2.3-bin-hadoop3.2.tgz\n",
        "!ls -l\n",
        "\n",
        "#Provides findspark.init() to make pyspark importable as a regular library.\n",
        "os.environ[\"SPARK_HOME\"] = \"spark-3.2.3-bin-hadoop3.2\"\n",
        "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.4.7 pyspark-shell'"
      ],
      "metadata": {
        "id": "_BcbpaYeKNlG",
        "outputId": "744fc960-c509-4e63-c5eb-75d9b32b96d2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 294088\n",
            "drwxr-xr-x  1 root root      4096 Apr 11 13:33 sample_data\n",
            "drwxr-xr-x 13  501 1000      4096 Nov 14 17:54 spark-3.2.3-bin-hadoop3.2\n",
            "-rw-r--r--  1 root root 301136158 Nov 14 18:47 spark-3.2.3-bin-hadoop3.2.tgz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q findspark pyspark\n",
        "import findspark\n",
        "findspark.init()"
      ],
      "metadata": {
        "id": "xB4TPZH16jmR",
        "outputId": "0bd77063-fde8-4849-b753-39d6c68a19bb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.4/281.4 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.7/199.7 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKheJ7h0AiQW"
      },
      "source": [
        "## Step 1: Upload the three files included in the data folder to colab. \n",
        "\n",
        "Upload the zip version and then use the !unzip command on the shell to unzip them"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))"
      ],
      "metadata": {
        "id": "GeRFy6Ks_TG-",
        "outputId": "a7c76d45-a1a6-4046-a577-5f6b54e4c5e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-31665aff-0979-42a5-9cb3-1f9cc6055e69\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-31665aff-0979-42a5-9cb3-1f9cc6055e69\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving nashville-tweets-2019-01-28.zip to nashville-tweets-2019-01-28.zip\n",
            "Saving Salaries.csv.zip to Salaries.csv.zip\n",
            "Saving Batting.csv.zip to Batting.csv.zip\n",
            "User uploaded file \"nashville-tweets-2019-01-28.zip\" with length 3237977 bytes\n",
            "User uploaded file \"Salaries.csv.zip\" with length 142130 bytes\n",
            "User uploaded file \"Batting.csv.zip\" with length 1843868 bytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vwScuHQQAt6-",
        "outputId": "d1e3f48e-f490-403f-eb9e-80175292222f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!unzip nashville-tweets-2019-01-28.zip"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  nashville-tweets-2019-01-28.zip\n",
            "  inflating: nashville-tweets-2019-01-28  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VRS3zN-wG1YQ",
        "outputId": "abbc58bf-a321-49f2-e33b-320447918b14",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#Note that this is not a csv file. So be careful. Do not load using read.csv in spark\n",
        "!head -3 nashville-tweets-2019-01-28.zip"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PK\u0003\u0004\u0014\u0000\b\u0000\b\u0000\u000bYyR\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000�\u0011~\u0001\u001b\u0000 \u0000nashville-tweets-2019-01-28UT\r\u0000\u0007v�\\`u�\\`��a`ux\u000b\u0000\u0001\u0004�\u0001\u0000\u0000\u0004\u0014\u0000\u0000\u0000�iw�8�.���\u0015HrW�\u0019\u001c�\u0000A\u0002��};�<&�㊝�N��EI�Ę\"\u0015�������\rj )J�dY���T9�\bN�\u0006�<{���� v�����_�\u0012m��l\u0006~\u001c��A\u001c�\u0011|�\u000f<\u000f��Áߴc�\u0005_�m/r�;�\u0007���M�.�پ%�F���EzA�~���aܵI7���n�\u001aW�A�����o�>��\t�p���\t�؎\u0007Q��\u001d:}ox\u001d\u0007���.ޜj\u0016ӄFMiR.uɄ\u000e����ט6�\u001a��2�\u0014\u001a�L8Զo��ͽ���n�:�\b�|9���_\u0005>~�?�L��\f]X\u0012nk�'l�Mu���\u0005(�_uWx�\u0010;e�\u0014�3j���_�v����\u000eÃ�oG����%�&�l���g9��ns���\u001a�l�\"��M?\u000b=�}�H�+���;N���/|�h�k\u0004�zC��kG���L~\u001f��c=���IO9?���rZ�p\u001c_k2�wn\u001c;!\bA��\u0001Z؞W�A�\u001c�W�\u0011�\u0016�W��\r�J��Zn������}�YI_4+Z���\n",
            "�zf�صj�n�oDx�^G]8�s����m9�u�\u001d�\u001a��a��s��K�V��yۯ|�w^f��&�d\u001a�LX#9p�\u0018��g�\u001dG}��O�Ԕ\u0014�q<�����\u000e>s���g���Vh�/7�p�z�I{jd�7à?>\u0001���Wv\u0002�l�\u0018\u0006\n",
            "B<�\u0013԰��O�Tw�\u0007`��k��_���zX�:�D5� l�>,\\���9 p��uO\r:58�)L3���(\u0005��������\u0015:���>~�����;�\u000b�VՅ����_\u0012�eh�\u0007����_����9i\u0007!q/�������e~}[�:��Tjv�g\u0005.�ס�+��\u001f\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EhzxQ6bnAyLS",
        "outputId": "8a60a9ac-76bd-46c1-e01e-71d97f15c91c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!unzip Salaries.csv.zip"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  Salaries.csv.zip\n",
            "  inflating: Salaries.csv            \n",
            "  inflating: __MACOSX/._Salaries.csv  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5K6OjtfA2us",
        "outputId": "206fe81f-840b-42c3-a4db-118616017d06",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!unzip Batting.csv.zip"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  Batting.csv.zip\n",
            "  inflating: Batting.csv             \n",
            "  inflating: __MACOSX/._Batting.csv  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Write Spark Code Locally and test the Code and Save it to your repository"
      ],
      "metadata": {
        "id": "o1prQTAn7Mbu"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pz7dMIVJ__Dy"
      },
      "source": [
        "# Step 2. Complete Spark Jobs Below Locally. \n",
        "\n",
        "Once they work you can submit them to EMR"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxUib2yt__Dy"
      },
      "source": [
        "## Job 1. Count the number of tweets.\n",
        "\n",
        "I have almost completed this for you. You still have to do the reduce and add - look into the wordcount example. But then use this as the template to finish the rest."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YDL78heZ__Dz",
        "outputId": "5fd64d5f-c334-4935-a9fe-edc70318d889",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%file 1_count.py\n",
        "'''\n",
        "TODO:\n",
        "Count the number of tweets.\n",
        "Parse tweets with json.loads -- note how the tweets are huge JSON blobs.\n",
        "Ignore tweets that error on load.\n",
        "'''\n",
        "\n",
        "from pyspark import SparkContext, SparkConf\n",
        "import json\n",
        "from operator import add\n",
        "#import other things as required\n",
        "\n",
        "#create spark context. This is very important. Do this similarly for the other parts\n",
        "# Note to read a file directly from s3 into an rdd you may have to do something like this\n",
        "\n",
        "def checkjson(entry):\n",
        "  try:\n",
        "    json.loads(entry)\n",
        "    #if load succeeded. We use correct as the key\n",
        "    return \"correct\", 1\n",
        "  except:\n",
        "    #there was an error in loading. We use incorrect as the key\n",
        "    return \"incorrect\", 1\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  # replace this line with the s3 pass when testing over EMR\n",
        "  conf = SparkConf().setAppName('1_count').set('spark.hadoop.validateOutputSpecs', False)\n",
        "  sc = SparkContext(conf=conf).getOrCreate()\n",
        "\n",
        "  try:\n",
        "    #@todo: fix the path as required\n",
        "    tweets=sc.textFile('./nashville-tweets-2019-01-28')\n",
        "    # review the page rank example for how to use the map operation\n",
        "    # review word count for reduce and add\n",
        "    # see how we use map to parse each row\n",
        "    counts = tweets.map(lambda tweet: checkjson(tweet))\n",
        "    print(counts.take(2))\n",
        "\n",
        "    # @todo: create an output rdd that uses reduce by key and add operator to add up the correct entries and incorrect entries\n",
        "    total = counts.map(lambda x: (x, 1)).reduceByKey(lambda a, b: a + b)\n",
        "\n",
        "    # @todo: the s3 version will have to save it to correct s3 path\n",
        "    total.repartition(1).saveAsTextFile(\"1_count.out\")\n",
        "\n",
        "  finally:\n",
        "    # very important: stop the context. Otherwise you may get an error that context is still alive. if you are on colab just restart the runtime if you face problem\n",
        "    #finally is used to make sure the context is stopped even with errors\n",
        "    sc.stop()\n",
        "  \n",
        "\n",
        " \n",
        "  \n",
        "  pass"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting 1_count.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test local Execution Results"
      ],
      "metadata": {
        "id": "PH2TFBJp7q0O"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3y2A34y__Dz",
        "outputId": "5523396e-f45b-4ce5-8f40-8191e4a2d85d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# execute locally and ensure everything works. If it works you should get the 1_count.out/part-00000 file. \n",
        "!spark-submit --packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.4.7 1_count.py\n",
        "# note the cell magic command %%file 1_count.py is used to create a local copy of the content of cell as a file 1_count.py on colab"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ":: loading settings :: url = jar:file:/content/spark-3.2.3-bin-hadoop3.2/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
            "Ivy Default Cache set to: /root/.ivy2/cache\n",
            "The jars for the packages stored in: /root/.ivy2/jars\n",
            "org.apache.spark#spark-streaming-kafka-0-8_2.11 added as a dependency\n",
            ":: resolving dependencies :: org.apache.spark#spark-submit-parent-e64045ba-d7b8-4a38-982e-785ae9af6a5c;1.0\n",
            "\tconfs: [default]\n",
            "\tfound org.apache.spark#spark-streaming-kafka-0-8_2.11;2.4.7 in central\n",
            "\tfound org.apache.kafka#kafka_2.11;0.8.2.1 in central\n",
            "\tfound org.scala-lang.modules#scala-xml_2.11;1.0.2 in central\n",
            "\tfound com.yammer.metrics#metrics-core;2.2.0 in central\n",
            "\tfound org.slf4j#slf4j-api;1.7.16 in central\n",
            "\tfound org.scala-lang.modules#scala-parser-combinators_2.11;1.1.0 in central\n",
            "\tfound com.101tec#zkclient;0.3 in central\n",
            "\tfound log4j#log4j;1.2.17 in central\n",
            "\tfound org.apache.kafka#kafka-clients;0.8.2.1 in central\n",
            "\tfound net.jpountz.lz4#lz4;1.2.0 in central\n",
            "\tfound org.xerial.snappy#snappy-java;1.1.7.5 in central\n",
            "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
            "downloading https://repo1.maven.org/maven2/org/apache/spark/spark-streaming-kafka-0-8_2.11/2.4.7/spark-streaming-kafka-0-8_2.11-2.4.7.jar ...\n",
            "\t[SUCCESSFUL ] org.apache.spark#spark-streaming-kafka-0-8_2.11;2.4.7!spark-streaming-kafka-0-8_2.11.jar (35ms)\n",
            "downloading https://repo1.maven.org/maven2/org/apache/kafka/kafka_2.11/0.8.2.1/kafka_2.11-0.8.2.1.jar ...\n",
            "\t[SUCCESSFUL ] org.apache.kafka#kafka_2.11;0.8.2.1!kafka_2.11.jar (172ms)\n",
            "downloading https://repo1.maven.org/maven2/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar ...\n",
            "\t[SUCCESSFUL ] org.spark-project.spark#unused;1.0.0!unused.jar (8ms)\n",
            "downloading https://repo1.maven.org/maven2/org/scala-lang/modules/scala-xml_2.11/1.0.2/scala-xml_2.11-1.0.2.jar ...\n",
            "\t[SUCCESSFUL ] org.scala-lang.modules#scala-xml_2.11;1.0.2!scala-xml_2.11.jar(bundle) (36ms)\n",
            "downloading https://repo1.maven.org/maven2/com/yammer/metrics/metrics-core/2.2.0/metrics-core-2.2.0.jar ...\n",
            "\t[SUCCESSFUL ] com.yammer.metrics#metrics-core;2.2.0!metrics-core.jar (12ms)\n",
            "downloading https://repo1.maven.org/maven2/org/scala-lang/modules/scala-parser-combinators_2.11/1.1.0/scala-parser-combinators_2.11-1.1.0.jar ...\n",
            "\t[SUCCESSFUL ] org.scala-lang.modules#scala-parser-combinators_2.11;1.1.0!scala-parser-combinators_2.11.jar(bundle) (28ms)\n",
            "downloading https://repo1.maven.org/maven2/com/101tec/zkclient/0.3/zkclient-0.3.jar ...\n",
            "\t[SUCCESSFUL ] com.101tec#zkclient;0.3!zkclient.jar (8ms)\n",
            "downloading https://repo1.maven.org/maven2/org/apache/kafka/kafka-clients/0.8.2.1/kafka-clients-0.8.2.1.jar ...\n",
            "\t[SUCCESSFUL ] org.apache.kafka#kafka-clients;0.8.2.1!kafka-clients.jar (25ms)\n",
            "downloading https://repo1.maven.org/maven2/org/slf4j/slf4j-api/1.7.16/slf4j-api-1.7.16.jar ...\n",
            "\t[SUCCESSFUL ] org.slf4j#slf4j-api;1.7.16!slf4j-api.jar (9ms)\n",
            "downloading https://repo1.maven.org/maven2/log4j/log4j/1.2.17/log4j-1.2.17.jar ...\n",
            "\t[SUCCESSFUL ] log4j#log4j;1.2.17!log4j.jar(bundle) (69ms)\n",
            "downloading https://repo1.maven.org/maven2/net/jpountz/lz4/lz4/1.2.0/lz4-1.2.0.jar ...\n",
            "\t[SUCCESSFUL ] net.jpountz.lz4#lz4;1.2.0!lz4.jar (14ms)\n",
            "downloading https://repo1.maven.org/maven2/org/xerial/snappy/snappy-java/1.1.7.5/snappy-java-1.1.7.5.jar ...\n",
            "\t[SUCCESSFUL ] org.xerial.snappy#snappy-java;1.1.7.5!snappy-java.jar(bundle) (64ms)\n",
            ":: resolution report :: resolve 5038ms :: artifacts dl 499ms\n",
            "\t:: modules in use:\n",
            "\tcom.101tec#zkclient;0.3 from central in [default]\n",
            "\tcom.yammer.metrics#metrics-core;2.2.0 from central in [default]\n",
            "\tlog4j#log4j;1.2.17 from central in [default]\n",
            "\tnet.jpountz.lz4#lz4;1.2.0 from central in [default]\n",
            "\torg.apache.kafka#kafka-clients;0.8.2.1 from central in [default]\n",
            "\torg.apache.kafka#kafka_2.11;0.8.2.1 from central in [default]\n",
            "\torg.apache.spark#spark-streaming-kafka-0-8_2.11;2.4.7 from central in [default]\n",
            "\torg.scala-lang.modules#scala-parser-combinators_2.11;1.1.0 from central in [default]\n",
            "\torg.scala-lang.modules#scala-xml_2.11;1.0.2 from central in [default]\n",
            "\torg.slf4j#slf4j-api;1.7.16 from central in [default]\n",
            "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
            "\torg.xerial.snappy#snappy-java;1.1.7.5 from central in [default]\n",
            "\t---------------------------------------------------------------------\n",
            "\t|                  |            modules            ||   artifacts   |\n",
            "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
            "\t---------------------------------------------------------------------\n",
            "\t|      default     |   12  |   12  |   12  |   0   ||   12  |   12  |\n",
            "\t---------------------------------------------------------------------\n",
            ":: retrieving :: org.apache.spark#spark-submit-parent-e64045ba-d7b8-4a38-982e-785ae9af6a5c\n",
            "\tconfs: [default]\n",
            "\t12 artifacts copied, 0 already retrieved (8282kB/59ms)\n",
            "23/04/12 23:13:00 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
            "23/04/12 23:13:01 INFO SparkContext: Running Spark version 3.2.3\n",
            "23/04/12 23:13:01 INFO ResourceUtils: ==============================================================\n",
            "23/04/12 23:13:01 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "23/04/12 23:13:01 INFO ResourceUtils: ==============================================================\n",
            "23/04/12 23:13:01 INFO SparkContext: Submitted application: 1_count\n",
            "23/04/12 23:13:01 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "23/04/12 23:13:01 INFO ResourceProfile: Limiting resource is cpu\n",
            "23/04/12 23:13:01 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "23/04/12 23:13:01 INFO SecurityManager: Changing view acls to: root\n",
            "23/04/12 23:13:01 INFO SecurityManager: Changing modify acls to: root\n",
            "23/04/12 23:13:01 INFO SecurityManager: Changing view acls groups to: \n",
            "23/04/12 23:13:01 INFO SecurityManager: Changing modify acls groups to: \n",
            "23/04/12 23:13:01 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\n",
            "23/04/12 23:13:02 INFO Utils: Successfully started service 'sparkDriver' on port 38227.\n",
            "23/04/12 23:13:02 INFO SparkEnv: Registering MapOutputTracker\n",
            "23/04/12 23:13:02 INFO SparkEnv: Registering BlockManagerMaster\n",
            "23/04/12 23:13:02 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "23/04/12 23:13:02 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "23/04/12 23:13:02 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "23/04/12 23:13:02 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-d9c75dac-c79b-4d50-86be-4fa1c8542524\n",
            "23/04/12 23:13:02 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB\n",
            "23/04/12 23:13:02 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "23/04/12 23:13:03 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
            "23/04/12 23:13:03 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://8c2d110fc661:4040\n",
            "23/04/12 23:13:03 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar at spark://8c2d110fc661:38227/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1681341181635\n",
            "23/04/12 23:13:03 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar at spark://8c2d110fc661:38227/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1681341181635\n",
            "23/04/12 23:13:03 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar at spark://8c2d110fc661:38227/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1681341181635\n",
            "23/04/12 23:13:03 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar at spark://8c2d110fc661:38227/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1681341181635\n",
            "23/04/12 23:13:03 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar at spark://8c2d110fc661:38227/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1681341181635\n",
            "23/04/12 23:13:03 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar at spark://8c2d110fc661:38227/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1681341181635\n",
            "23/04/12 23:13:03 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar at spark://8c2d110fc661:38227/jars/com.101tec_zkclient-0.3.jar with timestamp 1681341181635\n",
            "23/04/12 23:13:03 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar at spark://8c2d110fc661:38227/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1681341181635\n",
            "23/04/12 23:13:03 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar at spark://8c2d110fc661:38227/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1681341181635\n",
            "23/04/12 23:13:03 INFO SparkContext: Added JAR file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar at spark://8c2d110fc661:38227/jars/log4j_log4j-1.2.17.jar with timestamp 1681341181635\n",
            "23/04/12 23:13:03 INFO SparkContext: Added JAR file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar at spark://8c2d110fc661:38227/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1681341181635\n",
            "23/04/12 23:13:03 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar at spark://8c2d110fc661:38227/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1681341181635\n",
            "23/04/12 23:13:03 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar at file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1681341181635\n",
            "23/04/12 23:13:03 INFO Utils: Copying /root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar to /tmp/spark-5a1e6bf1-b7eb-4038-bb34-8002c9c6dbfc/userFiles-b55df87f-e987-40fb-b645-f55fcc1f8b7f/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar\n",
            "23/04/12 23:13:03 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar at file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1681341181635\n",
            "23/04/12 23:13:03 INFO Utils: Copying /root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar to /tmp/spark-5a1e6bf1-b7eb-4038-bb34-8002c9c6dbfc/userFiles-b55df87f-e987-40fb-b645-f55fcc1f8b7f/org.apache.kafka_kafka_2.11-0.8.2.1.jar\n",
            "23/04/12 23:13:03 INFO SparkContext: Added file file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar at file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1681341181635\n",
            "23/04/12 23:13:03 INFO Utils: Copying /root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-5a1e6bf1-b7eb-4038-bb34-8002c9c6dbfc/userFiles-b55df87f-e987-40fb-b645-f55fcc1f8b7f/org.spark-project.spark_unused-1.0.0.jar\n",
            "23/04/12 23:13:03 INFO SparkContext: Added file file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar at file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1681341181635\n",
            "23/04/12 23:13:03 INFO Utils: Copying /root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar to /tmp/spark-5a1e6bf1-b7eb-4038-bb34-8002c9c6dbfc/userFiles-b55df87f-e987-40fb-b645-f55fcc1f8b7f/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar\n",
            "23/04/12 23:13:03 INFO SparkContext: Added file file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar at file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1681341181635\n",
            "23/04/12 23:13:03 INFO Utils: Copying /root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar to /tmp/spark-5a1e6bf1-b7eb-4038-bb34-8002c9c6dbfc/userFiles-b55df87f-e987-40fb-b645-f55fcc1f8b7f/com.yammer.metrics_metrics-core-2.2.0.jar\n",
            "23/04/12 23:13:03 INFO SparkContext: Added file file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar at file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1681341181635\n",
            "23/04/12 23:13:03 INFO Utils: Copying /root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar to /tmp/spark-5a1e6bf1-b7eb-4038-bb34-8002c9c6dbfc/userFiles-b55df87f-e987-40fb-b645-f55fcc1f8b7f/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar\n",
            "23/04/12 23:13:03 INFO SparkContext: Added file file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar at file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar with timestamp 1681341181635\n",
            "23/04/12 23:13:03 INFO Utils: Copying /root/.ivy2/jars/com.101tec_zkclient-0.3.jar to /tmp/spark-5a1e6bf1-b7eb-4038-bb34-8002c9c6dbfc/userFiles-b55df87f-e987-40fb-b645-f55fcc1f8b7f/com.101tec_zkclient-0.3.jar\n",
            "23/04/12 23:13:03 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar at file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1681341181635\n",
            "23/04/12 23:13:03 INFO Utils: Copying /root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar to /tmp/spark-5a1e6bf1-b7eb-4038-bb34-8002c9c6dbfc/userFiles-b55df87f-e987-40fb-b645-f55fcc1f8b7f/org.apache.kafka_kafka-clients-0.8.2.1.jar\n",
            "23/04/12 23:13:03 INFO SparkContext: Added file file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar at file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1681341181635\n",
            "23/04/12 23:13:03 INFO Utils: Copying /root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar to /tmp/spark-5a1e6bf1-b7eb-4038-bb34-8002c9c6dbfc/userFiles-b55df87f-e987-40fb-b645-f55fcc1f8b7f/org.slf4j_slf4j-api-1.7.16.jar\n",
            "23/04/12 23:13:03 INFO SparkContext: Added file file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar at file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar with timestamp 1681341181635\n",
            "23/04/12 23:13:03 INFO Utils: Copying /root/.ivy2/jars/log4j_log4j-1.2.17.jar to /tmp/spark-5a1e6bf1-b7eb-4038-bb34-8002c9c6dbfc/userFiles-b55df87f-e987-40fb-b645-f55fcc1f8b7f/log4j_log4j-1.2.17.jar\n",
            "23/04/12 23:13:03 INFO SparkContext: Added file file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar at file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1681341181635\n",
            "23/04/12 23:13:03 INFO Utils: Copying /root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar to /tmp/spark-5a1e6bf1-b7eb-4038-bb34-8002c9c6dbfc/userFiles-b55df87f-e987-40fb-b645-f55fcc1f8b7f/net.jpountz.lz4_lz4-1.2.0.jar\n",
            "23/04/12 23:13:03 INFO SparkContext: Added file file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar at file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1681341181635\n",
            "23/04/12 23:13:03 INFO Utils: Copying /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar to /tmp/spark-5a1e6bf1-b7eb-4038-bb34-8002c9c6dbfc/userFiles-b55df87f-e987-40fb-b645-f55fcc1f8b7f/org.xerial.snappy_snappy-java-1.1.7.5.jar\n",
            "23/04/12 23:13:04 INFO Executor: Starting executor ID driver on host 8c2d110fc661\n",
            "23/04/12 23:13:04 INFO Executor: Fetching file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar with timestamp 1681341181635\n",
            "23/04/12 23:13:04 INFO Utils: /root/.ivy2/jars/log4j_log4j-1.2.17.jar has been previously copied to /tmp/spark-5a1e6bf1-b7eb-4038-bb34-8002c9c6dbfc/userFiles-b55df87f-e987-40fb-b645-f55fcc1f8b7f/log4j_log4j-1.2.17.jar\n",
            "23/04/12 23:13:04 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1681341181635\n",
            "23/04/12 23:13:04 INFO Utils: /root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar has been previously copied to /tmp/spark-5a1e6bf1-b7eb-4038-bb34-8002c9c6dbfc/userFiles-b55df87f-e987-40fb-b645-f55fcc1f8b7f/org.apache.kafka_kafka-clients-0.8.2.1.jar\n",
            "23/04/12 23:13:04 INFO Executor: Fetching file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1681341181635\n",
            "23/04/12 23:13:04 INFO Utils: /root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar has been previously copied to /tmp/spark-5a1e6bf1-b7eb-4038-bb34-8002c9c6dbfc/userFiles-b55df87f-e987-40fb-b645-f55fcc1f8b7f/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar\n",
            "23/04/12 23:13:04 INFO Executor: Fetching file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1681341181635\n",
            "23/04/12 23:13:04 INFO Utils: /root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar has been previously copied to /tmp/spark-5a1e6bf1-b7eb-4038-bb34-8002c9c6dbfc/userFiles-b55df87f-e987-40fb-b645-f55fcc1f8b7f/org.slf4j_slf4j-api-1.7.16.jar\n",
            "23/04/12 23:13:04 INFO Executor: Fetching file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1681341181635\n",
            "23/04/12 23:13:04 INFO Utils: /root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar has been previously copied to /tmp/spark-5a1e6bf1-b7eb-4038-bb34-8002c9c6dbfc/userFiles-b55df87f-e987-40fb-b645-f55fcc1f8b7f/net.jpountz.lz4_lz4-1.2.0.jar\n",
            "23/04/12 23:13:04 INFO Executor: Fetching file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1681341181635\n",
            "23/04/12 23:13:04 INFO Utils: /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar has been previously copied to /tmp/spark-5a1e6bf1-b7eb-4038-bb34-8002c9c6dbfc/userFiles-b55df87f-e987-40fb-b645-f55fcc1f8b7f/org.xerial.snappy_snappy-java-1.1.7.5.jar\n",
            "23/04/12 23:13:04 INFO Executor: Fetching file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1681341181635\n",
            "23/04/12 23:13:04 INFO Utils: /root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar has been previously copied to /tmp/spark-5a1e6bf1-b7eb-4038-bb34-8002c9c6dbfc/userFiles-b55df87f-e987-40fb-b645-f55fcc1f8b7f/org.spark-project.spark_unused-1.0.0.jar\n",
            "23/04/12 23:13:04 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1681341181635\n",
            "23/04/12 23:13:04 INFO Utils: /root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar has been previously copied to /tmp/spark-5a1e6bf1-b7eb-4038-bb34-8002c9c6dbfc/userFiles-b55df87f-e987-40fb-b645-f55fcc1f8b7f/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar\n",
            "23/04/12 23:13:04 INFO Executor: Fetching file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1681341181635\n",
            "23/04/12 23:13:04 INFO Utils: /root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar has been previously copied to /tmp/spark-5a1e6bf1-b7eb-4038-bb34-8002c9c6dbfc/userFiles-b55df87f-e987-40fb-b645-f55fcc1f8b7f/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar\n",
            "23/04/12 23:13:04 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1681341181635\n",
            "23/04/12 23:13:04 INFO Utils: /root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar has been previously copied to /tmp/spark-5a1e6bf1-b7eb-4038-bb34-8002c9c6dbfc/userFiles-b55df87f-e987-40fb-b645-f55fcc1f8b7f/org.apache.kafka_kafka_2.11-0.8.2.1.jar\n",
            "23/04/12 23:13:04 INFO Executor: Fetching file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar with timestamp 1681341181635\n",
            "23/04/12 23:13:04 INFO Utils: /root/.ivy2/jars/com.101tec_zkclient-0.3.jar has been previously copied to /tmp/spark-5a1e6bf1-b7eb-4038-bb34-8002c9c6dbfc/userFiles-b55df87f-e987-40fb-b645-f55fcc1f8b7f/com.101tec_zkclient-0.3.jar\n",
            "23/04/12 23:13:04 INFO Executor: Fetching file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1681341181635\n",
            "23/04/12 23:13:04 INFO Utils: /root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar has been previously copied to /tmp/spark-5a1e6bf1-b7eb-4038-bb34-8002c9c6dbfc/userFiles-b55df87f-e987-40fb-b645-f55fcc1f8b7f/com.yammer.metrics_metrics-core-2.2.0.jar\n",
            "23/04/12 23:13:04 INFO Executor: Fetching spark://8c2d110fc661:38227/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1681341181635\n",
            "23/04/12 23:13:04 INFO TransportClientFactory: Successfully created connection to 8c2d110fc661/172.28.0.12:38227 after 63 ms (0 ms spent in bootstraps)\n",
            "23/04/12 23:13:04 INFO Utils: Fetching spark://8c2d110fc661:38227/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar to /tmp/spark-5a1e6bf1-b7eb-4038-bb34-8002c9c6dbfc/userFiles-b55df87f-e987-40fb-b645-f55fcc1f8b7f/fetchFileTemp9101984983856158522.tmp\n",
            "23/04/12 23:13:04 INFO Utils: /tmp/spark-5a1e6bf1-b7eb-4038-bb34-8002c9c6dbfc/userFiles-b55df87f-e987-40fb-b645-f55fcc1f8b7f/fetchFileTemp9101984983856158522.tmp has been previously copied to /tmp/spark-5a1e6bf1-b7eb-4038-bb34-8002c9c6dbfc/userFiles-b55df87f-e987-40fb-b645-f55fcc1f8b7f/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar\n",
            "23/04/12 23:13:04 INFO Executor: Adding file:/tmp/spark-5a1e6bf1-b7eb-4038-bb34-8002c9c6dbfc/userFiles-b55df87f-e987-40fb-b645-f55fcc1f8b7f/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar to class loader\n",
            "23/04/12 23:13:04 INFO Executor: Fetching spark://8c2d110fc661:38227/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1681341181635\n",
            "23/04/12 23:13:04 INFO Utils: Fetching spark://8c2d110fc661:38227/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar to /tmp/spark-5a1e6bf1-b7eb-4038-bb34-8002c9c6dbfc/userFiles-b55df87f-e987-40fb-b645-f55fcc1f8b7f/fetchFileTemp2249917270334771510.tmp\n",
            "23/04/12 23:13:04 INFO Utils: /tmp/spark-5a1e6bf1-b7eb-4038-bb34-8002c9c6dbfc/userFiles-b55df87f-e987-40fb-b645-f55fcc1f8b7f/fetchFileTemp2249917270334771510.tmp has been previously copied to /tmp/spark-5a1e6bf1-b7eb-4038-bb34-8002c9c6dbfc/userFiles-b55df87f-e987-40fb-b645-f55fcc1f8b7f/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar\n",
            "23/04/12 23:13:04 INFO Executor: Adding file:/tmp/spark-5a1e6bf1-b7eb-4038-bb34-8002c9c6dbfc/userFiles-b55df87f-e987-40fb-b645-f55fcc1f8b7f/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar to class loader\n",
            "23/04/12 23:13:04 INFO Executor: Fetching spark://8c2d110fc661:38227/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1681341181635\n",
            "23/04/12 23:13:04 INFO Utils: Fetching spark://8c2d110fc661:38227/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar to /tmp/spark-5a1e6bf1-b7eb-4038-bb34-8002c9c6dbfc/userFiles-b55df87f-e987-40fb-b645-f55fcc1f8b7f/fetchFileTemp4412217182379422083.tmp\n",
            "23/04/12 23:13:04 INFO Utils: /tmp/spark-5a1e6bf1-b7eb-4038-bb34-8002c9c6dbfc/userFiles-b55df87f-e987-40fb-b645-f55fcc1f8b7f/fetchFileTemp4412217182379422083.tmp has been previously copied to /tmp/spark-5a1e6bf1-b7eb-4038-bb34-8002c9c6dbfc/userFiles-b55df87f-e987-40fb-b645-f55fcc1f8b7f/org.apache.kafka_kafka_2.11-0.8.2.1.jar\n",
            "23/04/12 23:13:04 INFO Executor: Adding file:/tmp/spark-5a1e6bf1-b7eb-4038-bb34-8002c9c6dbfc/userFiles-b55df87f-e987-40fb-b645-f55fcc1f8b7f/org.apache.kafka_kafka_2.11-0.8.2.1.jar to class loader\n",
            "23/04/12 23:13:04 INFO Executor: Fetching spark://8c2d110fc661:38227/jars/log4j_log4j-1.2.17.jar with timestamp 1681341181635\n",
            "23/04/12 23:13:04 INFO Utils: Fetching spark://8c2d110fc661:38227/jars/log4j_log4j-1.2.17.jar to /tmp/spark-5a1e6bf1-b7eb-4038-bb34-8002c9c6dbfc/userFiles-b55df87f-e987-40fb-b645-f55fcc1f8b7f/fetchFileTemp3443471934367985898.tmp\n",
            "23/04/12 23:13:04 INFO Utils: /tmp/spark-5a1e6bf1-b7eb-4038-bb34-8002c9c6dbfc/userFiles-b55df87f-e987-40fb-b645-f55fcc1f8b7f/fetchFileTemp3443471934367985898.tmp has been previously copied to /tmp/spark-5a1e6bf1-b7eb-4038-bb34-8002c9c6dbfc/userFiles-b55df87f-e987-40fb-b645-f55fcc1f8b7f/log4j_log4j-1.2.17.jar\n",
            "23/04/12 23:13:04 INFO Executor: Adding file:/tmp/spark-5a1e6bf1-b7eb-4038-bb34-8002c9c6dbfc/userFiles-b55df87f-e987-40fb-b645-f55fcc1f8b7f/log4j_log4j-1.2.17.jar to class loader\n",
            "23/04/12 23:13:04 INFO Executor: Fetching spark://8c2d110fc661:38227/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1681341181635\n",
            "23/04/12 23:13:04 INFO Utils: Fetching spark://8c2d110fc661:38227/jars/com.yammer.metrics_metrics-core-2.2.0.jar to /tmp/spark-5a1e6bf1-b7eb-4038-bb34-8002c9c6dbfc/userFiles-b55df87f-e987-40fb-b645-f55fcc1f8b7f/fetchFileTemp8639169588820116744.tmp\n",
            "23/04/12 23:13:04 INFO Utils: /tmp/spark-5a1e6bf1-b7eb-4038-bb34-8002c9c6dbfc/userFiles-b55df87f-e987-40fb-b645-f55fcc1f8b7f/fetchFileTemp8639169588820116744.tmp has been previously copied to /tmp/spark-5a1e6bf1-b7eb-4038-bb34-8002c9c6dbfc/userFiles-b55df87f-e987-40fb-b645-f55fcc1f8b7f/com.yammer.metrics_metrics-core-2.2.0.jar\n",
            "23/04/12 23:13:04 INFO Executor: Adding file:/tmp/spark-5a1e6bf1-b7eb-4038-bb34-8002c9c6dbfc/userFiles-b55df87f-e987-40fb-b645-f55fcc1f8b7f/com.yammer.metrics_metrics-core-2.2.0.jar to class loader\n",
            "23/04/12 23:13:04 INFO Executor: Fetching spark://8c2d110fc661:38227/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1681341181635\n",
            "23/04/12 23:13:04 INFO Utils: Fetching spark://8c2d110fc661:38227/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar to /tmp/spark-5a1e6bf1-b7eb-4038-bb34-8002c9c6dbfc/userFiles-b55df87f-e987-40fb-b645-f55fcc1f8b7f/fetchFileTemp8976140438710729010.tmp\n",
            "23/04/12 23:13:04 INFO Utils: /tmp/spark-5a1e6bf1-b7eb-4038-bb34-8002c9c6dbfc/userFiles-b55df87f-e987-40fb-b645-f55fcc1f8b7f/fetchFileTemp8976140438710729010.tmp has been previously copied to /tmp/spark-5a1e6bf1-b7eb-4038-bb34-8002c9c6dbfc/userFiles-b55df87f-e987-40fb-b645-f55fcc1f8b7f/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar\n",
            "23/04/12 23:13:04 INFO Executor: Adding file:/tmp/spark-5a1e6bf1-b7eb-4038-bb34-8002c9c6dbfc/userFiles-b55df87f-e987-40fb-b645-f55fcc1f8b7f/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar to class loader\n",
            "23/04/12 23:13:04 INFO Executor: Fetching spark://8c2d110fc661:38227/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1681341181635\n",
            "23/04/12 23:13:04 INFO Utils: Fetching spark://8c2d110fc661:38227/jars/org.slf4j_slf4j-api-1.7.16.jar to /tmp/spark-5a1e6bf1-b7eb-4038-bb34-8002c9c6dbfc/userFiles-b55df87f-e987-40fb-b645-f55fcc1f8b7f/fetchFileTemp6321753388122523345.tmp\n",
            "23/04/12 23:13:04 INFO Utils: /tmp/spark-5a1e6bf1-b7eb-4038-bb34-8002c9c6dbfc/userFiles-b55df87f-e987-40fb-b645-f55fcc1f8b7f/fetchFileTemp6321753388122523345.tmp has been previously copied to /tmp/spark-5a1e6bf1-b7eb-4038-bb34-8002c9c6dbfc/userFiles-b55df87f-e987-40fb-b645-f55fcc1f8b7f/org.slf4j_slf4j-api-1.7.16.jar\n",
            "23/04/12 23:13:04 INFO Executor: Adding file:/tmp/spark-5a1e6bf1-b7eb-4038-bb34-8002c9c6dbfc/userFiles-b55df87f-e987-40fb-b645-f55fcc1f8b7f/org.slf4j_slf4j-api-1.7.16.jar to class loader\n",
            "23/04/12 23:13:04 INFO Executor: Fetching spark://8c2d110fc661:38227/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1681341181635\n",
            "23/04/12 23:13:04 INFO Utils: Fetching spark://8c2d110fc661:38227/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-5a1e6bf1-b7eb-4038-bb34-8002c9c6dbfc/userFiles-b55df87f-e987-40fb-b645-f55fcc1f8b7f/fetchFileTemp4663794016909893454.tmp\n",
            "23/04/12 23:13:04 INFO Utils: /tmp/spark-5a1e6bf1-b7eb-4038-bb34-8002c9c6dbfc/userFiles-b55df87f-e987-40fb-b645-f55fcc1f8b7f/fetchFileTemp4663794016909893454.tmp has been previously copied to /tmp/spark-5a1e6bf1-b7eb-4038-bb34-8002c9c6dbfc/userFiles-b55df87f-e987-40fb-b645-f55fcc1f8b7f/org.spark-project.spark_unused-1.0.0.jar\n",
            "23/04/12 23:13:04 INFO Executor: Adding file:/tmp/spark-5a1e6bf1-b7eb-4038-bb34-8002c9c6dbfc/userFiles-b55df87f-e987-40fb-b645-f55fcc1f8b7f/org.spark-project.spark_unused-1.0.0.jar to class loader\n",
            "23/04/12 23:13:04 INFO Executor: Fetching spark://8c2d110fc661:38227/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1681341181635\n",
            "23/04/12 23:13:04 INFO Utils: Fetching spark://8c2d110fc661:38227/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar to /tmp/spark-5a1e6bf1-b7eb-4038-bb34-8002c9c6dbfc/userFiles-b55df87f-e987-40fb-b645-f55fcc1f8b7f/fetchFileTemp3675839553899709337.tmp\n",
            "23/04/12 23:13:04 INFO Utils: /tmp/spark-5a1e6bf1-b7eb-4038-bb34-8002c9c6dbfc/userFiles-b55df87f-e987-40fb-b645-f55fcc1f8b7f/fetchFileTemp3675839553899709337.tmp has been previously copied to /tmp/spark-5a1e6bf1-b7eb-4038-bb34-8002c9c6dbfc/userFiles-b55df87f-e987-40fb-b645-f55fcc1f8b7f/org.apache.kafka_kafka-clients-0.8.2.1.jar\n",
            "23/04/12 23:13:04 INFO Executor: Adding file:/tmp/spark-5a1e6bf1-b7eb-4038-bb34-8002c9c6dbfc/userFiles-b55df87f-e987-40fb-b645-f55fcc1f8b7f/org.apache.kafka_kafka-clients-0.8.2.1.jar to class loader\n",
            "23/04/12 23:13:04 INFO Executor: Fetching spark://8c2d110fc661:38227/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1681341181635\n",
            "23/04/12 23:13:04 INFO Utils: Fetching spark://8c2d110fc661:38227/jars/net.jpountz.lz4_lz4-1.2.0.jar to /tmp/spark-5a1e6bf1-b7eb-4038-bb34-8002c9c6dbfc/userFiles-b55df87f-e987-40fb-b645-f55fcc1f8b7f/fetchFileTemp1288014476607573864.tmp\n",
            "23/04/12 23:13:04 INFO Utils: /tmp/spark-5a1e6bf1-b7eb-4038-bb34-8002c9c6dbfc/userFiles-b55df87f-e987-40fb-b645-f55fcc1f8b7f/fetchFileTemp1288014476607573864.tmp has been previously copied to /tmp/spark-5a1e6bf1-b7eb-4038-bb34-8002c9c6dbfc/userFiles-b55df87f-e987-40fb-b645-f55fcc1f8b7f/net.jpountz.lz4_lz4-1.2.0.jar\n",
            "23/04/12 23:13:04 INFO Executor: Adding file:/tmp/spark-5a1e6bf1-b7eb-4038-bb34-8002c9c6dbfc/userFiles-b55df87f-e987-40fb-b645-f55fcc1f8b7f/net.jpountz.lz4_lz4-1.2.0.jar to class loader\n",
            "23/04/12 23:13:04 INFO Executor: Fetching spark://8c2d110fc661:38227/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1681341181635\n",
            "23/04/12 23:13:04 INFO Utils: Fetching spark://8c2d110fc661:38227/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar to /tmp/spark-5a1e6bf1-b7eb-4038-bb34-8002c9c6dbfc/userFiles-b55df87f-e987-40fb-b645-f55fcc1f8b7f/fetchFileTemp5869411888292655644.tmp\n",
            "23/04/12 23:13:04 INFO Utils: /tmp/spark-5a1e6bf1-b7eb-4038-bb34-8002c9c6dbfc/userFiles-b55df87f-e987-40fb-b645-f55fcc1f8b7f/fetchFileTemp5869411888292655644.tmp has been previously copied to /tmp/spark-5a1e6bf1-b7eb-4038-bb34-8002c9c6dbfc/userFiles-b55df87f-e987-40fb-b645-f55fcc1f8b7f/org.xerial.snappy_snappy-java-1.1.7.5.jar\n",
            "23/04/12 23:13:04 INFO Executor: Adding file:/tmp/spark-5a1e6bf1-b7eb-4038-bb34-8002c9c6dbfc/userFiles-b55df87f-e987-40fb-b645-f55fcc1f8b7f/org.xerial.snappy_snappy-java-1.1.7.5.jar to class loader\n",
            "23/04/12 23:13:04 INFO Executor: Fetching spark://8c2d110fc661:38227/jars/com.101tec_zkclient-0.3.jar with timestamp 1681341181635\n",
            "23/04/12 23:13:04 INFO Utils: Fetching spark://8c2d110fc661:38227/jars/com.101tec_zkclient-0.3.jar to /tmp/spark-5a1e6bf1-b7eb-4038-bb34-8002c9c6dbfc/userFiles-b55df87f-e987-40fb-b645-f55fcc1f8b7f/fetchFileTemp4501878137787804240.tmp\n",
            "23/04/12 23:13:04 INFO Utils: /tmp/spark-5a1e6bf1-b7eb-4038-bb34-8002c9c6dbfc/userFiles-b55df87f-e987-40fb-b645-f55fcc1f8b7f/fetchFileTemp4501878137787804240.tmp has been previously copied to /tmp/spark-5a1e6bf1-b7eb-4038-bb34-8002c9c6dbfc/userFiles-b55df87f-e987-40fb-b645-f55fcc1f8b7f/com.101tec_zkclient-0.3.jar\n",
            "23/04/12 23:13:04 INFO Executor: Adding file:/tmp/spark-5a1e6bf1-b7eb-4038-bb34-8002c9c6dbfc/userFiles-b55df87f-e987-40fb-b645-f55fcc1f8b7f/com.101tec_zkclient-0.3.jar to class loader\n",
            "23/04/12 23:13:04 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 41875.\n",
            "23/04/12 23:13:04 INFO NettyBlockTransferService: Server created on 8c2d110fc661:41875\n",
            "23/04/12 23:13:04 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "23/04/12 23:13:04 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8c2d110fc661, 41875, None)\n",
            "23/04/12 23:13:04 INFO BlockManagerMasterEndpoint: Registering block manager 8c2d110fc661:41875 with 366.3 MiB RAM, BlockManagerId(driver, 8c2d110fc661, 41875, None)\n",
            "23/04/12 23:13:04 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8c2d110fc661, 41875, None)\n",
            "23/04/12 23:13:04 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 8c2d110fc661, 41875, None)\n",
            "23/04/12 23:13:05 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 354.3 KiB, free 366.0 MiB)\n",
            "23/04/12 23:13:05 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 32.0 KiB, free 365.9 MiB)\n",
            "23/04/12 23:13:05 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 8c2d110fc661:41875 (size: 32.0 KiB, free: 366.3 MiB)\n",
            "23/04/12 23:13:05 INFO SparkContext: Created broadcast 0 from textFile at NativeMethodAccessorImpl.java:0\n",
            "23/04/12 23:13:06 INFO FileInputFormat: Total input files to process : 1\n",
            "23/04/12 23:13:06 INFO SparkContext: Starting job: runJob at PythonRDD.scala:166\n",
            "23/04/12 23:13:06 INFO DAGScheduler: Got job 0 (runJob at PythonRDD.scala:166) with 1 output partitions\n",
            "23/04/12 23:13:06 INFO DAGScheduler: Final stage: ResultStage 0 (runJob at PythonRDD.scala:166)\n",
            "23/04/12 23:13:06 INFO DAGScheduler: Parents of final stage: List()\n",
            "23/04/12 23:13:06 INFO DAGScheduler: Missing parents: List()\n",
            "23/04/12 23:13:06 INFO DAGScheduler: Submitting ResultStage 0 (PythonRDD[2] at RDD at PythonRDD.scala:53), which has no missing parents\n",
            "23/04/12 23:13:06 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 8.9 KiB, free 365.9 MiB)\n",
            "23/04/12 23:13:06 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.5 KiB, free 365.9 MiB)\n",
            "23/04/12 23:13:06 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 8c2d110fc661:41875 (size: 5.5 KiB, free: 366.3 MiB)\n",
            "23/04/12 23:13:06 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1478\n",
            "23/04/12 23:13:06 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (PythonRDD[2] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(0))\n",
            "23/04/12 23:13:06 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
            "23/04/12 23:13:07 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (8c2d110fc661, executor driver, partition 0, PROCESS_LOCAL, 4510 bytes) taskResourceAssignments Map()\n",
            "23/04/12 23:13:07 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
            "23/04/12 23:13:08 INFO HadoopRDD: Input split: file:/content/nashville-tweets-2019-01-28:0+12519625\n",
            "23/04/12 23:13:09 INFO PythonRunner: Times: total = 1030, boot = 988, init = 41, finish = 1\n",
            "23/04/12 23:13:09 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1448 bytes result sent to driver\n",
            "23/04/12 23:13:09 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 2851 ms on 8c2d110fc661 (executor driver) (1/1)\n",
            "23/04/12 23:13:09 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
            "23/04/12 23:13:09 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 42767\n",
            "23/04/12 23:13:09 INFO DAGScheduler: ResultStage 0 (runJob at PythonRDD.scala:166) finished in 3.379 s\n",
            "23/04/12 23:13:09 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "23/04/12 23:13:09 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
            "23/04/12 23:13:09 INFO DAGScheduler: Job 0 finished: runJob at PythonRDD.scala:166, took 3.586245 s\n",
            "[('correct', 1), ('correct', 1)]\n",
            "23/04/12 23:13:10 INFO deprecation: mapred.output.dir is deprecated. Instead, use mapreduce.output.fileoutputformat.outputdir\n",
            "23/04/12 23:13:10 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
            "23/04/12 23:13:10 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "23/04/12 23:13:10 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "23/04/12 23:13:10 INFO SparkContext: Starting job: runJob at SparkHadoopWriter.scala:83\n",
            "23/04/12 23:13:10 INFO DAGScheduler: Registering RDD 4 (reduceByKey at /content/1_count.py:41) as input to shuffle 1\n",
            "23/04/12 23:13:10 INFO DAGScheduler: Registering RDD 8 (coalesce at NativeMethodAccessorImpl.java:0) as input to shuffle 0\n",
            "23/04/12 23:13:10 INFO DAGScheduler: Got job 1 (runJob at SparkHadoopWriter.scala:83) with 1 output partitions\n",
            "23/04/12 23:13:10 INFO DAGScheduler: Final stage: ResultStage 3 (runJob at SparkHadoopWriter.scala:83)\n",
            "23/04/12 23:13:10 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)\n",
            "23/04/12 23:13:10 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 2)\n",
            "23/04/12 23:13:10 INFO DAGScheduler: Submitting ShuffleMapStage 1 (PairwiseRDD[4] at reduceByKey at /content/1_count.py:41), which has no missing parents\n",
            "23/04/12 23:13:10 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 12.6 KiB, free 365.9 MiB)\n",
            "23/04/12 23:13:10 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 365.9 MiB)\n",
            "23/04/12 23:13:10 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 8c2d110fc661:41875 (size: 7.7 KiB, free: 366.3 MiB)\n",
            "23/04/12 23:13:10 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1478\n",
            "23/04/12 23:13:10 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 1 (PairwiseRDD[4] at reduceByKey at /content/1_count.py:41) (first 15 tasks are for partitions Vector(0, 1))\n",
            "23/04/12 23:13:10 INFO TaskSchedulerImpl: Adding task set 1.0 with 2 tasks resource profile 0\n",
            "23/04/12 23:13:10 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (8c2d110fc661, executor driver, partition 0, PROCESS_LOCAL, 4499 bytes) taskResourceAssignments Map()\n",
            "23/04/12 23:13:10 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 2) (8c2d110fc661, executor driver, partition 1, PROCESS_LOCAL, 4499 bytes) taskResourceAssignments Map()\n",
            "23/04/12 23:13:10 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)\n",
            "23/04/12 23:13:10 INFO Executor: Running task 1.0 in stage 1.0 (TID 2)\n",
            "23/04/12 23:13:10 INFO HadoopRDD: Input split: file:/content/nashville-tweets-2019-01-28:0+12519625\n",
            "23/04/12 23:13:10 INFO HadoopRDD: Input split: file:/content/nashville-tweets-2019-01-28:12519625+12519626\n",
            "23/04/12 23:13:10 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 8c2d110fc661:41875 in memory (size: 5.5 KiB, free: 366.3 MiB)\n",
            "23/04/12 23:13:12 INFO PythonRunner: Times: total = 847, boot = 11, init = 19, finish = 817\n",
            "23/04/12 23:13:12 INFO PythonRunner: Times: total = 982, boot = 29, init = 36, finish = 917\n",
            "23/04/12 23:13:12 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1612 bytes result sent to driver\n",
            "23/04/12 23:13:12 INFO Executor: Finished task 1.0 in stage 1.0 (TID 2). 1655 bytes result sent to driver\n",
            "23/04/12 23:13:12 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 1555 ms on 8c2d110fc661 (executor driver) (1/2)\n",
            "23/04/12 23:13:12 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 2) in 1558 ms on 8c2d110fc661 (executor driver) (2/2)\n",
            "23/04/12 23:13:12 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
            "23/04/12 23:13:12 INFO DAGScheduler: ShuffleMapStage 1 (reduceByKey at /content/1_count.py:41) finished in 1.610 s\n",
            "23/04/12 23:13:12 INFO DAGScheduler: looking for newly runnable stages\n",
            "23/04/12 23:13:12 INFO DAGScheduler: running: Set()\n",
            "23/04/12 23:13:12 INFO DAGScheduler: waiting: Set(ShuffleMapStage 2, ResultStage 3)\n",
            "23/04/12 23:13:12 INFO DAGScheduler: failed: Set()\n",
            "23/04/12 23:13:12 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[8] at coalesce at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/04/12 23:13:12 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 10.6 KiB, free 365.9 MiB)\n",
            "23/04/12 23:13:12 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 6.2 KiB, free 365.9 MiB)\n",
            "23/04/12 23:13:12 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 8c2d110fc661:41875 (size: 6.2 KiB, free: 366.3 MiB)\n",
            "23/04/12 23:13:12 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1478\n",
            "23/04/12 23:13:12 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[8] at coalesce at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
            "23/04/12 23:13:12 INFO TaskSchedulerImpl: Adding task set 2.0 with 2 tasks resource profile 0\n",
            "23/04/12 23:13:12 INFO TaskSetManager: Starting task 1.0 in stage 2.0 (TID 3) (8c2d110fc661, executor driver, partition 1, NODE_LOCAL, 4260 bytes) taskResourceAssignments Map()\n",
            "23/04/12 23:13:12 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 4) (8c2d110fc661, executor driver, partition 0, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()\n",
            "23/04/12 23:13:12 INFO Executor: Running task 1.0 in stage 2.0 (TID 3)\n",
            "23/04/12 23:13:12 INFO Executor: Running task 0.0 in stage 2.0 (TID 4)\n",
            "23/04/12 23:13:12 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/12 23:13:12 INFO ShuffleBlockFetcherIterator: Getting 2 (160.0 B) non-empty blocks including 2 (160.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/12 23:13:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 19 ms\n",
            "23/04/12 23:13:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 21 ms\n",
            "23/04/12 23:13:12 INFO PythonRunner: Times: total = 47, boot = -748, init = 795, finish = 0\n",
            "23/04/12 23:13:12 INFO Executor: Finished task 0.0 in stage 2.0 (TID 4). 1654 bytes result sent to driver\n",
            "23/04/12 23:13:12 INFO PythonRunner: Times: total = 57, boot = -645, init = 701, finish = 1\n",
            "23/04/12 23:13:12 INFO Executor: Finished task 1.0 in stage 2.0 (TID 3). 1783 bytes result sent to driver\n",
            "23/04/12 23:13:12 INFO TaskSetManager: Finished task 1.0 in stage 2.0 (TID 3) in 178 ms on 8c2d110fc661 (executor driver) (1/2)\n",
            "23/04/12 23:13:12 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 4) in 184 ms on 8c2d110fc661 (executor driver) (2/2)\n",
            "23/04/12 23:13:12 INFO DAGScheduler: ShuffleMapStage 2 (coalesce at NativeMethodAccessorImpl.java:0) finished in 0.229 s\n",
            "23/04/12 23:13:12 INFO DAGScheduler: looking for newly runnable stages\n",
            "23/04/12 23:13:12 INFO DAGScheduler: running: Set()\n",
            "23/04/12 23:13:12 INFO DAGScheduler: waiting: Set(ResultStage 3)\n",
            "23/04/12 23:13:12 INFO DAGScheduler: failed: Set()\n",
            "23/04/12 23:13:12 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[14] at saveAsTextFile at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/04/12 23:13:12 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
            "23/04/12 23:13:12 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 105.9 KiB, free 365.8 MiB)\n",
            "23/04/12 23:13:12 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 39.6 KiB, free 365.7 MiB)\n",
            "23/04/12 23:13:12 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 8c2d110fc661:41875 (size: 39.6 KiB, free: 366.2 MiB)\n",
            "23/04/12 23:13:12 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1478\n",
            "23/04/12 23:13:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[14] at saveAsTextFile at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/04/12 23:13:12 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0\n",
            "23/04/12 23:13:12 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 5) (8c2d110fc661, executor driver, partition 0, NODE_LOCAL, 4547 bytes) taskResourceAssignments Map()\n",
            "23/04/12 23:13:12 INFO Executor: Running task 0.0 in stage 3.0 (TID 5)\n",
            "23/04/12 23:13:12 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
            "23/04/12 23:13:12 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "23/04/12 23:13:12 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "23/04/12 23:13:12 INFO ShuffleBlockFetcherIterator: Getting 1 (80.0 B) non-empty blocks including 1 (80.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/12 23:13:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
            "23/04/12 23:13:12 INFO PythonRunner: Times: total = 42, boot = -167, init = 209, finish = 0\n",
            "23/04/12 23:13:12 INFO FileOutputCommitter: Saved output of task 'attempt_202304122313102186825092691624496_0014_m_000000_0' to file:/content/1_count.out/_temporary/0/task_202304122313102186825092691624496_0014_m_000000\n",
            "23/04/12 23:13:12 INFO SparkHadoopMapRedUtil: attempt_202304122313102186825092691624496_0014_m_000000_0: Committed\n",
            "23/04/12 23:13:12 INFO Executor: Finished task 0.0 in stage 3.0 (TID 5). 1952 bytes result sent to driver\n",
            "23/04/12 23:13:12 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 5) in 214 ms on 8c2d110fc661 (executor driver) (1/1)\n",
            "23/04/12 23:13:12 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
            "23/04/12 23:13:12 INFO DAGScheduler: ResultStage 3 (runJob at SparkHadoopWriter.scala:83) finished in 0.253 s\n",
            "23/04/12 23:13:12 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "23/04/12 23:13:12 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished\n",
            "23/04/12 23:13:12 INFO DAGScheduler: Job 1 finished: runJob at SparkHadoopWriter.scala:83, took 2.163436 s\n",
            "23/04/12 23:13:12 INFO SparkHadoopWriter: Start to commit write Job job_202304122313102186825092691624496_0014.\n",
            "23/04/12 23:13:12 INFO SparkHadoopWriter: Write Job job_202304122313102186825092691624496_0014 committed. Elapsed time: 18 ms.\n",
            "23/04/12 23:13:12 INFO SparkUI: Stopped Spark web UI at http://8c2d110fc661:4040\n",
            "23/04/12 23:13:12 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "23/04/12 23:13:12 INFO MemoryStore: MemoryStore cleared\n",
            "23/04/12 23:13:12 INFO BlockManager: BlockManager stopped\n",
            "23/04/12 23:13:12 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "23/04/12 23:13:12 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "23/04/12 23:13:12 INFO SparkContext: Successfully stopped SparkContext\n",
            "23/04/12 23:13:13 INFO ShutdownHookManager: Shutdown hook called\n",
            "23/04/12 23:13:13 INFO ShutdownHookManager: Deleting directory /tmp/spark-5a1e6bf1-b7eb-4038-bb34-8002c9c6dbfc\n",
            "23/04/12 23:13:13 INFO ShutdownHookManager: Deleting directory /tmp/spark-5a1e6bf1-b7eb-4038-bb34-8002c9c6dbfc/pyspark-e0370c84-3992-40af-a6a4-86c8f108b9ba\n",
            "23/04/12 23:13:13 INFO ShutdownHookManager: Deleting directory /tmp/spark-3d41712e-08b5-42a5-9269-ca6900025702\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TrkWukH-__Dz"
      },
      "source": [
        "def test1(lines):\n",
        "  if '6294' in lines[0] and 'correct' in lines[0]:\n",
        "    print(\"passed\")\n",
        "  else:\n",
        "    print(\"failed\")\n"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# test local execution results\n",
        "with open('1_count.out/part-00000') as f:\n",
        "  lines = f.readlines()\n",
        "  print(lines)\n",
        "  test1(lines)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BYMwjqpsn2Le",
        "outputId": "e08cdbd4-b8a0-41ce-de73-12f2f5c783c6"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"(('correct', 1), 6294)\\n\"]\n",
            "passed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jy9ZZT06__Dy"
      },
      "source": [
        "### Please save the output of each job as a single text file into your S3 bucket.\n",
        "\n",
        "Hint:\n",
        "\n",
        "1. You may call the **saveAsTextFile** function to populate the output file. \n",
        "2. Note spark may generate multiple output files due to partitioning, you can use the **repartition** or **coalesce** function to merge them to a single one.\n",
        "\n",
        "**You need to replace all s3 uri shown in below cells with yours.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3BPCSIi0__D0"
      },
      "source": [
        "## Job 2. Count the screen name with the most tweets and its counts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tvw5wFdq__D0",
        "outputId": "3d646bfa-09ff-4291-d62e-75064f5583ff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%file 2_group.py\n",
        "\n",
        "'''\n",
        "TODO:\n",
        "Count the screen name with the most tweets and its counts.\n",
        "\n",
        "See http://mike.teczno.com/notes/streaming-data-from-twitter.html for parsing info.\n",
        "Get the screen name by accessing tweet['user']['screen_name']\n",
        "\n",
        "Output:\n",
        "number_of_most_tweets    username\n",
        "\n",
        "'''\n",
        "\n",
        "\n",
        "from pyspark import SparkContext, SparkConf\n",
        "import json\n",
        "from operator import add\n",
        "\n",
        "def checkjson(entry):\n",
        "  try:\n",
        "    tweet = json.loads(entry)\n",
        "    screen_name = tweet['user']['screen_name']\n",
        "    return screen_name, 1\n",
        "  except:\n",
        "    pass\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  #conf = SparkConf().setAppName('1_count').set('spark.hadoop.validateOutputSpecs', False)\n",
        "  #sc = SparkContext(conf=conf).getOrCreate()\n",
        "  sc = SparkContext(appName=\"2_group\")\n",
        "\n",
        "\n",
        "  try:\n",
        "    tweets=sc.textFile('s3://vandy-bigdata-2/nashville-tweets-2019-01-28')\n",
        "    counts = tweets.map(lambda tweet: checkjson(tweet))\n",
        "    print(counts.take(2))\n",
        "\n",
        "    total = counts.map(lambda x: (x)).reduceByKey(lambda a, b: a + b)\n",
        "    max_tuple = total.max(key=lambda x: x[1])\n",
        "    print(max_tuple)\n",
        "    sc.parallelize([max_tuple]).saveAsTextFile(\"s3://vandy-bigdata-2/hw6/2_group.out\")\n",
        "\n",
        "  finally:\n",
        "    sc.stop()\n",
        "  \n",
        "\n",
        " \n",
        "  \n",
        "  pass"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing 2_group.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Execute and test locally"
      ],
      "metadata": {
        "id": "1xe2ZqP18Uso"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tH6TK-Nk__D0",
        "outputId": "5b97e22f-47c6-460f-e228-13deb6c27de2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "\n",
        "!spark-submit --packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.4.7 2_group.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ":: loading settings :: url = jar:file:/content/spark-3.2.3-bin-hadoop3.2/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
            "Ivy Default Cache set to: /root/.ivy2/cache\n",
            "The jars for the packages stored in: /root/.ivy2/jars\n",
            "org.apache.spark#spark-streaming-kafka-0-8_2.11 added as a dependency\n",
            ":: resolving dependencies :: org.apache.spark#spark-submit-parent-f5912cf3-e1a4-4ea8-bd05-38c8033db672;1.0\n",
            "\tconfs: [default]\n",
            "\tfound org.apache.spark#spark-streaming-kafka-0-8_2.11;2.4.7 in central\n",
            "\tfound org.apache.kafka#kafka_2.11;0.8.2.1 in central\n",
            "\tfound org.scala-lang.modules#scala-xml_2.11;1.0.2 in central\n",
            "\tfound com.yammer.metrics#metrics-core;2.2.0 in central\n",
            "\tfound org.slf4j#slf4j-api;1.7.16 in central\n",
            "\tfound org.scala-lang.modules#scala-parser-combinators_2.11;1.1.0 in central\n",
            "\tfound com.101tec#zkclient;0.3 in central\n",
            "\tfound log4j#log4j;1.2.17 in central\n",
            "\tfound org.apache.kafka#kafka-clients;0.8.2.1 in central\n",
            "\tfound net.jpountz.lz4#lz4;1.2.0 in central\n",
            "\tfound org.xerial.snappy#snappy-java;1.1.7.5 in central\n",
            "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
            ":: resolution report :: resolve 1507ms :: artifacts dl 18ms\n",
            "\t:: modules in use:\n",
            "\tcom.101tec#zkclient;0.3 from central in [default]\n",
            "\tcom.yammer.metrics#metrics-core;2.2.0 from central in [default]\n",
            "\tlog4j#log4j;1.2.17 from central in [default]\n",
            "\tnet.jpountz.lz4#lz4;1.2.0 from central in [default]\n",
            "\torg.apache.kafka#kafka-clients;0.8.2.1 from central in [default]\n",
            "\torg.apache.kafka#kafka_2.11;0.8.2.1 from central in [default]\n",
            "\torg.apache.spark#spark-streaming-kafka-0-8_2.11;2.4.7 from central in [default]\n",
            "\torg.scala-lang.modules#scala-parser-combinators_2.11;1.1.0 from central in [default]\n",
            "\torg.scala-lang.modules#scala-xml_2.11;1.0.2 from central in [default]\n",
            "\torg.slf4j#slf4j-api;1.7.16 from central in [default]\n",
            "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
            "\torg.xerial.snappy#snappy-java;1.1.7.5 from central in [default]\n",
            "\t---------------------------------------------------------------------\n",
            "\t|                  |            modules            ||   artifacts   |\n",
            "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
            "\t---------------------------------------------------------------------\n",
            "\t|      default     |   12  |   0   |   0   |   0   ||   12  |   0   |\n",
            "\t---------------------------------------------------------------------\n",
            ":: retrieving :: org.apache.spark#spark-submit-parent-f5912cf3-e1a4-4ea8-bd05-38c8033db672\n",
            "\tconfs: [default]\n",
            "\t0 artifacts copied, 12 already retrieved (0kB/25ms)\n",
            "23/04/10 03:10:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
            "23/04/10 03:10:17 INFO SparkContext: Running Spark version 3.2.3\n",
            "23/04/10 03:10:17 INFO ResourceUtils: ==============================================================\n",
            "23/04/10 03:10:17 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "23/04/10 03:10:17 INFO ResourceUtils: ==============================================================\n",
            "23/04/10 03:10:17 INFO SparkContext: Submitted application: 1_count\n",
            "23/04/10 03:10:17 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "23/04/10 03:10:17 INFO ResourceProfile: Limiting resource is cpu\n",
            "23/04/10 03:10:17 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "23/04/10 03:10:17 INFO SecurityManager: Changing view acls to: root\n",
            "23/04/10 03:10:17 INFO SecurityManager: Changing modify acls to: root\n",
            "23/04/10 03:10:17 INFO SecurityManager: Changing view acls groups to: \n",
            "23/04/10 03:10:17 INFO SecurityManager: Changing modify acls groups to: \n",
            "23/04/10 03:10:17 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\n",
            "23/04/10 03:10:18 INFO Utils: Successfully started service 'sparkDriver' on port 43005.\n",
            "23/04/10 03:10:18 INFO SparkEnv: Registering MapOutputTracker\n",
            "23/04/10 03:10:18 INFO SparkEnv: Registering BlockManagerMaster\n",
            "23/04/10 03:10:18 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "23/04/10 03:10:18 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "23/04/10 03:10:18 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "23/04/10 03:10:18 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-387c554d-72c4-4a55-a347-5d50fcc93655\n",
            "23/04/10 03:10:18 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB\n",
            "23/04/10 03:10:18 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "23/04/10 03:10:19 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
            "23/04/10 03:10:19 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://e77b3fc47c45:4040\n",
            "23/04/10 03:10:19 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar at spark://e77b3fc47c45:43005/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1681096217749\n",
            "23/04/10 03:10:19 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar at spark://e77b3fc47c45:43005/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1681096217749\n",
            "23/04/10 03:10:19 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar at spark://e77b3fc47c45:43005/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1681096217749\n",
            "23/04/10 03:10:19 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar at spark://e77b3fc47c45:43005/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1681096217749\n",
            "23/04/10 03:10:19 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar at spark://e77b3fc47c45:43005/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1681096217749\n",
            "23/04/10 03:10:19 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar at spark://e77b3fc47c45:43005/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1681096217749\n",
            "23/04/10 03:10:19 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar at spark://e77b3fc47c45:43005/jars/com.101tec_zkclient-0.3.jar with timestamp 1681096217749\n",
            "23/04/10 03:10:19 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar at spark://e77b3fc47c45:43005/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1681096217749\n",
            "23/04/10 03:10:19 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar at spark://e77b3fc47c45:43005/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1681096217749\n",
            "23/04/10 03:10:19 INFO SparkContext: Added JAR file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar at spark://e77b3fc47c45:43005/jars/log4j_log4j-1.2.17.jar with timestamp 1681096217749\n",
            "23/04/10 03:10:19 INFO SparkContext: Added JAR file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar at spark://e77b3fc47c45:43005/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1681096217749\n",
            "23/04/10 03:10:19 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar at spark://e77b3fc47c45:43005/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1681096217749\n",
            "23/04/10 03:10:19 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar at file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1681096217749\n",
            "23/04/10 03:10:19 INFO Utils: Copying /root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar to /tmp/spark-12b0f44e-d31b-48b2-a540-92fc65be8a7c/userFiles-ebdca2e3-14f5-4d81-ae13-0aded0310a4b/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar\n",
            "23/04/10 03:10:19 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar at file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1681096217749\n",
            "23/04/10 03:10:19 INFO Utils: Copying /root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar to /tmp/spark-12b0f44e-d31b-48b2-a540-92fc65be8a7c/userFiles-ebdca2e3-14f5-4d81-ae13-0aded0310a4b/org.apache.kafka_kafka_2.11-0.8.2.1.jar\n",
            "23/04/10 03:10:19 INFO SparkContext: Added file file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar at file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1681096217749\n",
            "23/04/10 03:10:19 INFO Utils: Copying /root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-12b0f44e-d31b-48b2-a540-92fc65be8a7c/userFiles-ebdca2e3-14f5-4d81-ae13-0aded0310a4b/org.spark-project.spark_unused-1.0.0.jar\n",
            "23/04/10 03:10:19 INFO SparkContext: Added file file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar at file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1681096217749\n",
            "23/04/10 03:10:19 INFO Utils: Copying /root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar to /tmp/spark-12b0f44e-d31b-48b2-a540-92fc65be8a7c/userFiles-ebdca2e3-14f5-4d81-ae13-0aded0310a4b/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar\n",
            "23/04/10 03:10:19 INFO SparkContext: Added file file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar at file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1681096217749\n",
            "23/04/10 03:10:19 INFO Utils: Copying /root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar to /tmp/spark-12b0f44e-d31b-48b2-a540-92fc65be8a7c/userFiles-ebdca2e3-14f5-4d81-ae13-0aded0310a4b/com.yammer.metrics_metrics-core-2.2.0.jar\n",
            "23/04/10 03:10:19 INFO SparkContext: Added file file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar at file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1681096217749\n",
            "23/04/10 03:10:19 INFO Utils: Copying /root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar to /tmp/spark-12b0f44e-d31b-48b2-a540-92fc65be8a7c/userFiles-ebdca2e3-14f5-4d81-ae13-0aded0310a4b/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar\n",
            "23/04/10 03:10:19 INFO SparkContext: Added file file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar at file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar with timestamp 1681096217749\n",
            "23/04/10 03:10:19 INFO Utils: Copying /root/.ivy2/jars/com.101tec_zkclient-0.3.jar to /tmp/spark-12b0f44e-d31b-48b2-a540-92fc65be8a7c/userFiles-ebdca2e3-14f5-4d81-ae13-0aded0310a4b/com.101tec_zkclient-0.3.jar\n",
            "23/04/10 03:10:19 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar at file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1681096217749\n",
            "23/04/10 03:10:19 INFO Utils: Copying /root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar to /tmp/spark-12b0f44e-d31b-48b2-a540-92fc65be8a7c/userFiles-ebdca2e3-14f5-4d81-ae13-0aded0310a4b/org.apache.kafka_kafka-clients-0.8.2.1.jar\n",
            "23/04/10 03:10:19 INFO SparkContext: Added file file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar at file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1681096217749\n",
            "23/04/10 03:10:19 INFO Utils: Copying /root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar to /tmp/spark-12b0f44e-d31b-48b2-a540-92fc65be8a7c/userFiles-ebdca2e3-14f5-4d81-ae13-0aded0310a4b/org.slf4j_slf4j-api-1.7.16.jar\n",
            "23/04/10 03:10:19 INFO SparkContext: Added file file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar at file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar with timestamp 1681096217749\n",
            "23/04/10 03:10:19 INFO Utils: Copying /root/.ivy2/jars/log4j_log4j-1.2.17.jar to /tmp/spark-12b0f44e-d31b-48b2-a540-92fc65be8a7c/userFiles-ebdca2e3-14f5-4d81-ae13-0aded0310a4b/log4j_log4j-1.2.17.jar\n",
            "23/04/10 03:10:19 INFO SparkContext: Added file file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar at file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1681096217749\n",
            "23/04/10 03:10:19 INFO Utils: Copying /root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar to /tmp/spark-12b0f44e-d31b-48b2-a540-92fc65be8a7c/userFiles-ebdca2e3-14f5-4d81-ae13-0aded0310a4b/net.jpountz.lz4_lz4-1.2.0.jar\n",
            "23/04/10 03:10:19 INFO SparkContext: Added file file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar at file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1681096217749\n",
            "23/04/10 03:10:19 INFO Utils: Copying /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar to /tmp/spark-12b0f44e-d31b-48b2-a540-92fc65be8a7c/userFiles-ebdca2e3-14f5-4d81-ae13-0aded0310a4b/org.xerial.snappy_snappy-java-1.1.7.5.jar\n",
            "23/04/10 03:10:19 INFO Executor: Starting executor ID driver on host e77b3fc47c45\n",
            "23/04/10 03:10:19 INFO Executor: Fetching file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar with timestamp 1681096217749\n",
            "23/04/10 03:10:19 INFO Utils: /root/.ivy2/jars/log4j_log4j-1.2.17.jar has been previously copied to /tmp/spark-12b0f44e-d31b-48b2-a540-92fc65be8a7c/userFiles-ebdca2e3-14f5-4d81-ae13-0aded0310a4b/log4j_log4j-1.2.17.jar\n",
            "23/04/10 03:10:19 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1681096217749\n",
            "23/04/10 03:10:19 INFO Utils: /root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar has been previously copied to /tmp/spark-12b0f44e-d31b-48b2-a540-92fc65be8a7c/userFiles-ebdca2e3-14f5-4d81-ae13-0aded0310a4b/org.apache.kafka_kafka-clients-0.8.2.1.jar\n",
            "23/04/10 03:10:19 INFO Executor: Fetching file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1681096217749\n",
            "23/04/10 03:10:19 INFO Utils: /root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar has been previously copied to /tmp/spark-12b0f44e-d31b-48b2-a540-92fc65be8a7c/userFiles-ebdca2e3-14f5-4d81-ae13-0aded0310a4b/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar\n",
            "23/04/10 03:10:19 INFO Executor: Fetching file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1681096217749\n",
            "23/04/10 03:10:19 INFO Utils: /root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar has been previously copied to /tmp/spark-12b0f44e-d31b-48b2-a540-92fc65be8a7c/userFiles-ebdca2e3-14f5-4d81-ae13-0aded0310a4b/org.slf4j_slf4j-api-1.7.16.jar\n",
            "23/04/10 03:10:19 INFO Executor: Fetching file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1681096217749\n",
            "23/04/10 03:10:19 INFO Utils: /root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar has been previously copied to /tmp/spark-12b0f44e-d31b-48b2-a540-92fc65be8a7c/userFiles-ebdca2e3-14f5-4d81-ae13-0aded0310a4b/net.jpountz.lz4_lz4-1.2.0.jar\n",
            "23/04/10 03:10:19 INFO Executor: Fetching file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1681096217749\n",
            "23/04/10 03:10:19 INFO Utils: /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar has been previously copied to /tmp/spark-12b0f44e-d31b-48b2-a540-92fc65be8a7c/userFiles-ebdca2e3-14f5-4d81-ae13-0aded0310a4b/org.xerial.snappy_snappy-java-1.1.7.5.jar\n",
            "23/04/10 03:10:19 INFO Executor: Fetching file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1681096217749\n",
            "23/04/10 03:10:19 INFO Utils: /root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar has been previously copied to /tmp/spark-12b0f44e-d31b-48b2-a540-92fc65be8a7c/userFiles-ebdca2e3-14f5-4d81-ae13-0aded0310a4b/org.spark-project.spark_unused-1.0.0.jar\n",
            "23/04/10 03:10:19 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1681096217749\n",
            "23/04/10 03:10:19 INFO Utils: /root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar has been previously copied to /tmp/spark-12b0f44e-d31b-48b2-a540-92fc65be8a7c/userFiles-ebdca2e3-14f5-4d81-ae13-0aded0310a4b/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar\n",
            "23/04/10 03:10:19 INFO Executor: Fetching file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1681096217749\n",
            "23/04/10 03:10:19 INFO Utils: /root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar has been previously copied to /tmp/spark-12b0f44e-d31b-48b2-a540-92fc65be8a7c/userFiles-ebdca2e3-14f5-4d81-ae13-0aded0310a4b/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar\n",
            "23/04/10 03:10:19 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1681096217749\n",
            "23/04/10 03:10:19 INFO Utils: /root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar has been previously copied to /tmp/spark-12b0f44e-d31b-48b2-a540-92fc65be8a7c/userFiles-ebdca2e3-14f5-4d81-ae13-0aded0310a4b/org.apache.kafka_kafka_2.11-0.8.2.1.jar\n",
            "23/04/10 03:10:19 INFO Executor: Fetching file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar with timestamp 1681096217749\n",
            "23/04/10 03:10:19 INFO Utils: /root/.ivy2/jars/com.101tec_zkclient-0.3.jar has been previously copied to /tmp/spark-12b0f44e-d31b-48b2-a540-92fc65be8a7c/userFiles-ebdca2e3-14f5-4d81-ae13-0aded0310a4b/com.101tec_zkclient-0.3.jar\n",
            "23/04/10 03:10:19 INFO Executor: Fetching file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1681096217749\n",
            "23/04/10 03:10:19 INFO Utils: /root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar has been previously copied to /tmp/spark-12b0f44e-d31b-48b2-a540-92fc65be8a7c/userFiles-ebdca2e3-14f5-4d81-ae13-0aded0310a4b/com.yammer.metrics_metrics-core-2.2.0.jar\n",
            "23/04/10 03:10:19 INFO Executor: Fetching spark://e77b3fc47c45:43005/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1681096217749\n",
            "23/04/10 03:10:19 INFO TransportClientFactory: Successfully created connection to e77b3fc47c45/172.28.0.12:43005 after 58 ms (0 ms spent in bootstraps)\n",
            "23/04/10 03:10:19 INFO Utils: Fetching spark://e77b3fc47c45:43005/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar to /tmp/spark-12b0f44e-d31b-48b2-a540-92fc65be8a7c/userFiles-ebdca2e3-14f5-4d81-ae13-0aded0310a4b/fetchFileTemp7102877649691876810.tmp\n",
            "23/04/10 03:10:19 INFO Utils: /tmp/spark-12b0f44e-d31b-48b2-a540-92fc65be8a7c/userFiles-ebdca2e3-14f5-4d81-ae13-0aded0310a4b/fetchFileTemp7102877649691876810.tmp has been previously copied to /tmp/spark-12b0f44e-d31b-48b2-a540-92fc65be8a7c/userFiles-ebdca2e3-14f5-4d81-ae13-0aded0310a4b/org.apache.kafka_kafka-clients-0.8.2.1.jar\n",
            "23/04/10 03:10:19 INFO Executor: Adding file:/tmp/spark-12b0f44e-d31b-48b2-a540-92fc65be8a7c/userFiles-ebdca2e3-14f5-4d81-ae13-0aded0310a4b/org.apache.kafka_kafka-clients-0.8.2.1.jar to class loader\n",
            "23/04/10 03:10:19 INFO Executor: Fetching spark://e77b3fc47c45:43005/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1681096217749\n",
            "23/04/10 03:10:19 INFO Utils: Fetching spark://e77b3fc47c45:43005/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar to /tmp/spark-12b0f44e-d31b-48b2-a540-92fc65be8a7c/userFiles-ebdca2e3-14f5-4d81-ae13-0aded0310a4b/fetchFileTemp1775824629672992605.tmp\n",
            "23/04/10 03:10:20 INFO Utils: /tmp/spark-12b0f44e-d31b-48b2-a540-92fc65be8a7c/userFiles-ebdca2e3-14f5-4d81-ae13-0aded0310a4b/fetchFileTemp1775824629672992605.tmp has been previously copied to /tmp/spark-12b0f44e-d31b-48b2-a540-92fc65be8a7c/userFiles-ebdca2e3-14f5-4d81-ae13-0aded0310a4b/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar\n",
            "23/04/10 03:10:20 INFO Executor: Adding file:/tmp/spark-12b0f44e-d31b-48b2-a540-92fc65be8a7c/userFiles-ebdca2e3-14f5-4d81-ae13-0aded0310a4b/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar to class loader\n",
            "23/04/10 03:10:20 INFO Executor: Fetching spark://e77b3fc47c45:43005/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1681096217749\n",
            "23/04/10 03:10:20 INFO Utils: Fetching spark://e77b3fc47c45:43005/jars/com.yammer.metrics_metrics-core-2.2.0.jar to /tmp/spark-12b0f44e-d31b-48b2-a540-92fc65be8a7c/userFiles-ebdca2e3-14f5-4d81-ae13-0aded0310a4b/fetchFileTemp5566460557792601086.tmp\n",
            "23/04/10 03:10:20 INFO Utils: /tmp/spark-12b0f44e-d31b-48b2-a540-92fc65be8a7c/userFiles-ebdca2e3-14f5-4d81-ae13-0aded0310a4b/fetchFileTemp5566460557792601086.tmp has been previously copied to /tmp/spark-12b0f44e-d31b-48b2-a540-92fc65be8a7c/userFiles-ebdca2e3-14f5-4d81-ae13-0aded0310a4b/com.yammer.metrics_metrics-core-2.2.0.jar\n",
            "23/04/10 03:10:20 INFO Executor: Adding file:/tmp/spark-12b0f44e-d31b-48b2-a540-92fc65be8a7c/userFiles-ebdca2e3-14f5-4d81-ae13-0aded0310a4b/com.yammer.metrics_metrics-core-2.2.0.jar to class loader\n",
            "23/04/10 03:10:20 INFO Executor: Fetching spark://e77b3fc47c45:43005/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1681096217749\n",
            "23/04/10 03:10:20 INFO Utils: Fetching spark://e77b3fc47c45:43005/jars/org.slf4j_slf4j-api-1.7.16.jar to /tmp/spark-12b0f44e-d31b-48b2-a540-92fc65be8a7c/userFiles-ebdca2e3-14f5-4d81-ae13-0aded0310a4b/fetchFileTemp360109160503918848.tmp\n",
            "23/04/10 03:10:20 INFO Utils: /tmp/spark-12b0f44e-d31b-48b2-a540-92fc65be8a7c/userFiles-ebdca2e3-14f5-4d81-ae13-0aded0310a4b/fetchFileTemp360109160503918848.tmp has been previously copied to /tmp/spark-12b0f44e-d31b-48b2-a540-92fc65be8a7c/userFiles-ebdca2e3-14f5-4d81-ae13-0aded0310a4b/org.slf4j_slf4j-api-1.7.16.jar\n",
            "23/04/10 03:10:20 INFO Executor: Adding file:/tmp/spark-12b0f44e-d31b-48b2-a540-92fc65be8a7c/userFiles-ebdca2e3-14f5-4d81-ae13-0aded0310a4b/org.slf4j_slf4j-api-1.7.16.jar to class loader\n",
            "23/04/10 03:10:20 INFO Executor: Fetching spark://e77b3fc47c45:43005/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1681096217749\n",
            "23/04/10 03:10:20 INFO Utils: Fetching spark://e77b3fc47c45:43005/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar to /tmp/spark-12b0f44e-d31b-48b2-a540-92fc65be8a7c/userFiles-ebdca2e3-14f5-4d81-ae13-0aded0310a4b/fetchFileTemp7657407171218489451.tmp\n",
            "23/04/10 03:10:20 INFO Utils: /tmp/spark-12b0f44e-d31b-48b2-a540-92fc65be8a7c/userFiles-ebdca2e3-14f5-4d81-ae13-0aded0310a4b/fetchFileTemp7657407171218489451.tmp has been previously copied to /tmp/spark-12b0f44e-d31b-48b2-a540-92fc65be8a7c/userFiles-ebdca2e3-14f5-4d81-ae13-0aded0310a4b/org.apache.kafka_kafka_2.11-0.8.2.1.jar\n",
            "23/04/10 03:10:20 INFO Executor: Adding file:/tmp/spark-12b0f44e-d31b-48b2-a540-92fc65be8a7c/userFiles-ebdca2e3-14f5-4d81-ae13-0aded0310a4b/org.apache.kafka_kafka_2.11-0.8.2.1.jar to class loader\n",
            "23/04/10 03:10:20 INFO Executor: Fetching spark://e77b3fc47c45:43005/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1681096217749\n",
            "23/04/10 03:10:20 INFO Utils: Fetching spark://e77b3fc47c45:43005/jars/net.jpountz.lz4_lz4-1.2.0.jar to /tmp/spark-12b0f44e-d31b-48b2-a540-92fc65be8a7c/userFiles-ebdca2e3-14f5-4d81-ae13-0aded0310a4b/fetchFileTemp1296487743603112335.tmp\n",
            "23/04/10 03:10:20 INFO Utils: /tmp/spark-12b0f44e-d31b-48b2-a540-92fc65be8a7c/userFiles-ebdca2e3-14f5-4d81-ae13-0aded0310a4b/fetchFileTemp1296487743603112335.tmp has been previously copied to /tmp/spark-12b0f44e-d31b-48b2-a540-92fc65be8a7c/userFiles-ebdca2e3-14f5-4d81-ae13-0aded0310a4b/net.jpountz.lz4_lz4-1.2.0.jar\n",
            "23/04/10 03:10:20 INFO Executor: Adding file:/tmp/spark-12b0f44e-d31b-48b2-a540-92fc65be8a7c/userFiles-ebdca2e3-14f5-4d81-ae13-0aded0310a4b/net.jpountz.lz4_lz4-1.2.0.jar to class loader\n",
            "23/04/10 03:10:20 INFO Executor: Fetching spark://e77b3fc47c45:43005/jars/com.101tec_zkclient-0.3.jar with timestamp 1681096217749\n",
            "23/04/10 03:10:20 INFO Utils: Fetching spark://e77b3fc47c45:43005/jars/com.101tec_zkclient-0.3.jar to /tmp/spark-12b0f44e-d31b-48b2-a540-92fc65be8a7c/userFiles-ebdca2e3-14f5-4d81-ae13-0aded0310a4b/fetchFileTemp1410559581546843249.tmp\n",
            "23/04/10 03:10:20 INFO Utils: /tmp/spark-12b0f44e-d31b-48b2-a540-92fc65be8a7c/userFiles-ebdca2e3-14f5-4d81-ae13-0aded0310a4b/fetchFileTemp1410559581546843249.tmp has been previously copied to /tmp/spark-12b0f44e-d31b-48b2-a540-92fc65be8a7c/userFiles-ebdca2e3-14f5-4d81-ae13-0aded0310a4b/com.101tec_zkclient-0.3.jar\n",
            "23/04/10 03:10:20 INFO Executor: Adding file:/tmp/spark-12b0f44e-d31b-48b2-a540-92fc65be8a7c/userFiles-ebdca2e3-14f5-4d81-ae13-0aded0310a4b/com.101tec_zkclient-0.3.jar to class loader\n",
            "23/04/10 03:10:20 INFO Executor: Fetching spark://e77b3fc47c45:43005/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1681096217749\n",
            "23/04/10 03:10:20 INFO Utils: Fetching spark://e77b3fc47c45:43005/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar to /tmp/spark-12b0f44e-d31b-48b2-a540-92fc65be8a7c/userFiles-ebdca2e3-14f5-4d81-ae13-0aded0310a4b/fetchFileTemp1817708846455861208.tmp\n",
            "23/04/10 03:10:20 INFO Utils: /tmp/spark-12b0f44e-d31b-48b2-a540-92fc65be8a7c/userFiles-ebdca2e3-14f5-4d81-ae13-0aded0310a4b/fetchFileTemp1817708846455861208.tmp has been previously copied to /tmp/spark-12b0f44e-d31b-48b2-a540-92fc65be8a7c/userFiles-ebdca2e3-14f5-4d81-ae13-0aded0310a4b/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar\n",
            "23/04/10 03:10:20 INFO Executor: Adding file:/tmp/spark-12b0f44e-d31b-48b2-a540-92fc65be8a7c/userFiles-ebdca2e3-14f5-4d81-ae13-0aded0310a4b/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar to class loader\n",
            "23/04/10 03:10:20 INFO Executor: Fetching spark://e77b3fc47c45:43005/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1681096217749\n",
            "23/04/10 03:10:20 INFO Utils: Fetching spark://e77b3fc47c45:43005/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar to /tmp/spark-12b0f44e-d31b-48b2-a540-92fc65be8a7c/userFiles-ebdca2e3-14f5-4d81-ae13-0aded0310a4b/fetchFileTemp6529258773345545793.tmp\n",
            "23/04/10 03:10:20 INFO Utils: /tmp/spark-12b0f44e-d31b-48b2-a540-92fc65be8a7c/userFiles-ebdca2e3-14f5-4d81-ae13-0aded0310a4b/fetchFileTemp6529258773345545793.tmp has been previously copied to /tmp/spark-12b0f44e-d31b-48b2-a540-92fc65be8a7c/userFiles-ebdca2e3-14f5-4d81-ae13-0aded0310a4b/org.xerial.snappy_snappy-java-1.1.7.5.jar\n",
            "23/04/10 03:10:20 INFO Executor: Adding file:/tmp/spark-12b0f44e-d31b-48b2-a540-92fc65be8a7c/userFiles-ebdca2e3-14f5-4d81-ae13-0aded0310a4b/org.xerial.snappy_snappy-java-1.1.7.5.jar to class loader\n",
            "23/04/10 03:10:20 INFO Executor: Fetching spark://e77b3fc47c45:43005/jars/log4j_log4j-1.2.17.jar with timestamp 1681096217749\n",
            "23/04/10 03:10:20 INFO Utils: Fetching spark://e77b3fc47c45:43005/jars/log4j_log4j-1.2.17.jar to /tmp/spark-12b0f44e-d31b-48b2-a540-92fc65be8a7c/userFiles-ebdca2e3-14f5-4d81-ae13-0aded0310a4b/fetchFileTemp5250713497614100392.tmp\n",
            "23/04/10 03:10:20 INFO Utils: /tmp/spark-12b0f44e-d31b-48b2-a540-92fc65be8a7c/userFiles-ebdca2e3-14f5-4d81-ae13-0aded0310a4b/fetchFileTemp5250713497614100392.tmp has been previously copied to /tmp/spark-12b0f44e-d31b-48b2-a540-92fc65be8a7c/userFiles-ebdca2e3-14f5-4d81-ae13-0aded0310a4b/log4j_log4j-1.2.17.jar\n",
            "23/04/10 03:10:20 INFO Executor: Adding file:/tmp/spark-12b0f44e-d31b-48b2-a540-92fc65be8a7c/userFiles-ebdca2e3-14f5-4d81-ae13-0aded0310a4b/log4j_log4j-1.2.17.jar to class loader\n",
            "23/04/10 03:10:20 INFO Executor: Fetching spark://e77b3fc47c45:43005/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1681096217749\n",
            "23/04/10 03:10:20 INFO Utils: Fetching spark://e77b3fc47c45:43005/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-12b0f44e-d31b-48b2-a540-92fc65be8a7c/userFiles-ebdca2e3-14f5-4d81-ae13-0aded0310a4b/fetchFileTemp182677384746708001.tmp\n",
            "23/04/10 03:10:20 INFO Utils: /tmp/spark-12b0f44e-d31b-48b2-a540-92fc65be8a7c/userFiles-ebdca2e3-14f5-4d81-ae13-0aded0310a4b/fetchFileTemp182677384746708001.tmp has been previously copied to /tmp/spark-12b0f44e-d31b-48b2-a540-92fc65be8a7c/userFiles-ebdca2e3-14f5-4d81-ae13-0aded0310a4b/org.spark-project.spark_unused-1.0.0.jar\n",
            "23/04/10 03:10:20 INFO Executor: Adding file:/tmp/spark-12b0f44e-d31b-48b2-a540-92fc65be8a7c/userFiles-ebdca2e3-14f5-4d81-ae13-0aded0310a4b/org.spark-project.spark_unused-1.0.0.jar to class loader\n",
            "23/04/10 03:10:20 INFO Executor: Fetching spark://e77b3fc47c45:43005/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1681096217749\n",
            "23/04/10 03:10:20 INFO Utils: Fetching spark://e77b3fc47c45:43005/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar to /tmp/spark-12b0f44e-d31b-48b2-a540-92fc65be8a7c/userFiles-ebdca2e3-14f5-4d81-ae13-0aded0310a4b/fetchFileTemp1503678614670746479.tmp\n",
            "23/04/10 03:10:20 INFO Utils: /tmp/spark-12b0f44e-d31b-48b2-a540-92fc65be8a7c/userFiles-ebdca2e3-14f5-4d81-ae13-0aded0310a4b/fetchFileTemp1503678614670746479.tmp has been previously copied to /tmp/spark-12b0f44e-d31b-48b2-a540-92fc65be8a7c/userFiles-ebdca2e3-14f5-4d81-ae13-0aded0310a4b/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar\n",
            "23/04/10 03:10:20 INFO Executor: Adding file:/tmp/spark-12b0f44e-d31b-48b2-a540-92fc65be8a7c/userFiles-ebdca2e3-14f5-4d81-ae13-0aded0310a4b/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar to class loader\n",
            "23/04/10 03:10:20 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38225.\n",
            "23/04/10 03:10:20 INFO NettyBlockTransferService: Server created on e77b3fc47c45:38225\n",
            "23/04/10 03:10:20 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "23/04/10 03:10:20 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, e77b3fc47c45, 38225, None)\n",
            "23/04/10 03:10:20 INFO BlockManagerMasterEndpoint: Registering block manager e77b3fc47c45:38225 with 366.3 MiB RAM, BlockManagerId(driver, e77b3fc47c45, 38225, None)\n",
            "23/04/10 03:10:20 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, e77b3fc47c45, 38225, None)\n",
            "23/04/10 03:10:20 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, e77b3fc47c45, 38225, None)\n",
            "23/04/10 03:10:21 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 354.3 KiB, free 366.0 MiB)\n",
            "23/04/10 03:10:21 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 32.0 KiB, free 365.9 MiB)\n",
            "23/04/10 03:10:21 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on e77b3fc47c45:38225 (size: 32.0 KiB, free: 366.3 MiB)\n",
            "23/04/10 03:10:21 INFO SparkContext: Created broadcast 0 from textFile at NativeMethodAccessorImpl.java:0\n",
            "23/04/10 03:10:22 INFO FileInputFormat: Total input files to process : 1\n",
            "23/04/10 03:10:22 INFO SparkContext: Starting job: runJob at PythonRDD.scala:166\n",
            "23/04/10 03:10:22 INFO DAGScheduler: Got job 0 (runJob at PythonRDD.scala:166) with 1 output partitions\n",
            "23/04/10 03:10:22 INFO DAGScheduler: Final stage: ResultStage 0 (runJob at PythonRDD.scala:166)\n",
            "23/04/10 03:10:22 INFO DAGScheduler: Parents of final stage: List()\n",
            "23/04/10 03:10:22 INFO DAGScheduler: Missing parents: List()\n",
            "23/04/10 03:10:22 INFO DAGScheduler: Submitting ResultStage 0 (PythonRDD[2] at RDD at PythonRDD.scala:53), which has no missing parents\n",
            "23/04/10 03:10:22 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 8.9 KiB, free 365.9 MiB)\n",
            "23/04/10 03:10:22 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.5 KiB, free 365.9 MiB)\n",
            "23/04/10 03:10:22 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on e77b3fc47c45:38225 (size: 5.5 KiB, free: 366.3 MiB)\n",
            "23/04/10 03:10:22 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1478\n",
            "23/04/10 03:10:22 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (PythonRDD[2] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(0))\n",
            "23/04/10 03:10:22 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
            "23/04/10 03:10:22 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (e77b3fc47c45, executor driver, partition 0, PROCESS_LOCAL, 4510 bytes) taskResourceAssignments Map()\n",
            "23/04/10 03:10:22 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
            "23/04/10 03:10:23 INFO HadoopRDD: Input split: file:/content/nashville-tweets-2019-01-28:0+12519625\n",
            "23/04/10 03:10:24 INFO PythonRunner: Times: total = 835, boot = 801, init = 33, finish = 1\n",
            "23/04/10 03:10:24 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1462 bytes result sent to driver\n",
            "23/04/10 03:10:24 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1716 ms on e77b3fc47c45 (executor driver) (1/1)\n",
            "23/04/10 03:10:24 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
            "23/04/10 03:10:24 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 41753\n",
            "23/04/10 03:10:24 INFO DAGScheduler: ResultStage 0 (runJob at PythonRDD.scala:166) finished in 2.144 s\n",
            "23/04/10 03:10:24 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "23/04/10 03:10:24 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
            "23/04/10 03:10:24 INFO DAGScheduler: Job 0 finished: runJob at PythonRDD.scala:166, took 2.267590 s\n",
            "[('RandallCantrel4', 1), ('LakerMike2416', 1)]\n",
            "23/04/10 03:10:24 INFO SparkContext: Starting job: max at /content/2_group.py:50\n",
            "23/04/10 03:10:24 INFO DAGScheduler: Registering RDD 4 (reduceByKey at /content/2_group.py:49) as input to shuffle 0\n",
            "23/04/10 03:10:24 INFO DAGScheduler: Got job 1 (max at /content/2_group.py:50) with 2 output partitions\n",
            "23/04/10 03:10:24 INFO DAGScheduler: Final stage: ResultStage 2 (max at /content/2_group.py:50)\n",
            "23/04/10 03:10:24 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 1)\n",
            "23/04/10 03:10:24 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 1)\n",
            "23/04/10 03:10:24 INFO DAGScheduler: Submitting ShuffleMapStage 1 (PairwiseRDD[4] at reduceByKey at /content/2_group.py:49), which has no missing parents\n",
            "23/04/10 03:10:24 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 12.6 KiB, free 365.9 MiB)\n",
            "23/04/10 03:10:24 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 365.9 MiB)\n",
            "23/04/10 03:10:24 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on e77b3fc47c45:38225 (size: 7.7 KiB, free: 366.3 MiB)\n",
            "23/04/10 03:10:24 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1478\n",
            "23/04/10 03:10:24 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 1 (PairwiseRDD[4] at reduceByKey at /content/2_group.py:49) (first 15 tasks are for partitions Vector(0, 1))\n",
            "23/04/10 03:10:24 INFO TaskSchedulerImpl: Adding task set 1.0 with 2 tasks resource profile 0\n",
            "23/04/10 03:10:24 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (e77b3fc47c45, executor driver, partition 0, PROCESS_LOCAL, 4499 bytes) taskResourceAssignments Map()\n",
            "23/04/10 03:10:24 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 2) (e77b3fc47c45, executor driver, partition 1, PROCESS_LOCAL, 4499 bytes) taskResourceAssignments Map()\n",
            "23/04/10 03:10:24 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)\n",
            "23/04/10 03:10:24 INFO Executor: Running task 1.0 in stage 1.0 (TID 2)\n",
            "23/04/10 03:10:24 INFO HadoopRDD: Input split: file:/content/nashville-tweets-2019-01-28:12519625+12519626\n",
            "23/04/10 03:10:24 INFO HadoopRDD: Input split: file:/content/nashville-tweets-2019-01-28:0+12519625\n",
            "23/04/10 03:10:26 INFO PythonRunner: Times: total = 1022, boot = 26, init = 12, finish = 984\n",
            "23/04/10 03:10:26 INFO PythonRunner: Times: total = 956, boot = 15, init = 32, finish = 909\n",
            "23/04/10 03:10:26 INFO Executor: Finished task 1.0 in stage 1.0 (TID 2). 1655 bytes result sent to driver\n",
            "23/04/10 03:10:26 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1655 bytes result sent to driver\n",
            "23/04/10 03:10:26 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 1618 ms on e77b3fc47c45 (executor driver) (1/2)\n",
            "23/04/10 03:10:26 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 2) in 1622 ms on e77b3fc47c45 (executor driver) (2/2)\n",
            "23/04/10 03:10:26 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
            "23/04/10 03:10:26 INFO DAGScheduler: ShuffleMapStage 1 (reduceByKey at /content/2_group.py:49) finished in 1.667 s\n",
            "23/04/10 03:10:26 INFO DAGScheduler: looking for newly runnable stages\n",
            "23/04/10 03:10:26 INFO DAGScheduler: running: Set()\n",
            "23/04/10 03:10:26 INFO DAGScheduler: waiting: Set(ResultStage 2)\n",
            "23/04/10 03:10:26 INFO DAGScheduler: failed: Set()\n",
            "23/04/10 03:10:26 INFO DAGScheduler: Submitting ResultStage 2 (PythonRDD[7] at max at /content/2_group.py:50), which has no missing parents\n",
            "23/04/10 03:10:26 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 10.8 KiB, free 365.9 MiB)\n",
            "23/04/10 03:10:26 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 365.9 MiB)\n",
            "23/04/10 03:10:26 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on e77b3fc47c45:38225 (size: 6.3 KiB, free: 366.2 MiB)\n",
            "23/04/10 03:10:26 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1478\n",
            "23/04/10 03:10:26 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 2 (PythonRDD[7] at max at /content/2_group.py:50) (first 15 tasks are for partitions Vector(0, 1))\n",
            "23/04/10 03:10:26 INFO TaskSchedulerImpl: Adding task set 2.0 with 2 tasks resource profile 0\n",
            "23/04/10 03:10:26 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 3) (e77b3fc47c45, executor driver, partition 0, NODE_LOCAL, 4271 bytes) taskResourceAssignments Map()\n",
            "23/04/10 03:10:26 INFO TaskSetManager: Starting task 1.0 in stage 2.0 (TID 4) (e77b3fc47c45, executor driver, partition 1, NODE_LOCAL, 4271 bytes) taskResourceAssignments Map()\n",
            "23/04/10 03:10:26 INFO Executor: Running task 0.0 in stage 2.0 (TID 3)\n",
            "23/04/10 03:10:26 INFO Executor: Running task 1.0 in stage 2.0 (TID 4)\n",
            "23/04/10 03:10:26 INFO ShuffleBlockFetcherIterator: Getting 2 (21.2 KiB) non-empty blocks including 2 (21.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/10 03:10:26 INFO ShuffleBlockFetcherIterator: Getting 2 (21.2 KiB) non-empty blocks including 2 (21.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/10 03:10:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 14 ms\n",
            "23/04/10 03:10:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 25 ms\n",
            "23/04/10 03:10:26 INFO PythonRunner: Times: total = 48, boot = -627, init = 671, finish = 4\n",
            "23/04/10 03:10:26 INFO Executor: Finished task 0.0 in stage 2.0 (TID 3). 1627 bytes result sent to driver\n",
            "23/04/10 03:10:26 INFO PythonRunner: Times: total = 59, boot = -677, init = 732, finish = 4\n",
            "23/04/10 03:10:26 INFO Executor: Finished task 1.0 in stage 2.0 (TID 4). 1623 bytes result sent to driver\n",
            "23/04/10 03:10:26 INFO TaskSetManager: Finished task 1.0 in stage 2.0 (TID 4) in 163 ms on e77b3fc47c45 (executor driver) (1/2)\n",
            "23/04/10 03:10:26 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 3) in 182 ms on e77b3fc47c45 (executor driver) (2/2)\n",
            "23/04/10 03:10:26 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
            "23/04/10 03:10:26 INFO DAGScheduler: ResultStage 2 (max at /content/2_group.py:50) finished in 0.211 s\n",
            "23/04/10 03:10:26 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "23/04/10 03:10:26 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished\n",
            "23/04/10 03:10:26 INFO DAGScheduler: Job 1 finished: max at /content/2_group.py:50, took 1.929906 s\n",
            "('rpsabo', 88)\n",
            "23/04/10 03:10:26 INFO deprecation: mapred.output.dir is deprecated. Instead, use mapreduce.output.fileoutputformat.outputdir\n",
            "23/04/10 03:10:26 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
            "23/04/10 03:10:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "23/04/10 03:10:26 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "23/04/10 03:10:26 INFO SparkContext: Starting job: runJob at SparkHadoopWriter.scala:83\n",
            "23/04/10 03:10:26 INFO DAGScheduler: Got job 2 (runJob at SparkHadoopWriter.scala:83) with 2 output partitions\n",
            "23/04/10 03:10:26 INFO DAGScheduler: Final stage: ResultStage 3 (runJob at SparkHadoopWriter.scala:83)\n",
            "23/04/10 03:10:26 INFO DAGScheduler: Parents of final stage: List()\n",
            "23/04/10 03:10:26 INFO DAGScheduler: Missing parents: List()\n",
            "23/04/10 03:10:26 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[11] at saveAsTextFile at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/04/10 03:10:26 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 103.4 KiB, free 365.8 MiB)\n",
            "23/04/10 03:10:26 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 38.1 KiB, free 365.7 MiB)\n",
            "23/04/10 03:10:26 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on e77b3fc47c45:38225 (size: 38.1 KiB, free: 366.2 MiB)\n",
            "23/04/10 03:10:26 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1478\n",
            "23/04/10 03:10:26 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 3 (MapPartitionsRDD[11] at saveAsTextFile at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
            "23/04/10 03:10:26 INFO TaskSchedulerImpl: Adding task set 3.0 with 2 tasks resource profile 0\n",
            "23/04/10 03:10:26 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 5) (e77b3fc47c45, executor driver, partition 0, PROCESS_LOCAL, 4433 bytes) taskResourceAssignments Map()\n",
            "23/04/10 03:10:26 INFO TaskSetManager: Starting task 1.0 in stage 3.0 (TID 6) (e77b3fc47c45, executor driver, partition 1, PROCESS_LOCAL, 4471 bytes) taskResourceAssignments Map()\n",
            "23/04/10 03:10:26 INFO Executor: Running task 0.0 in stage 3.0 (TID 5)\n",
            "23/04/10 03:10:26 INFO Executor: Running task 1.0 in stage 3.0 (TID 6)\n",
            "23/04/10 03:10:26 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
            "23/04/10 03:10:26 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
            "23/04/10 03:10:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "23/04/10 03:10:26 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "23/04/10 03:10:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "23/04/10 03:10:26 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "23/04/10 03:10:26 INFO PythonRunner: Times: total = 43, boot = -295, init = 338, finish = 0\n",
            "23/04/10 03:10:26 INFO FileOutputCommitter: Saved output of task 'attempt_202304100310263385440263815417053_0011_m_000001_0' to file:/content/2_group.out/_temporary/0/task_202304100310263385440263815417053_0011_m_000001\n",
            "23/04/10 03:10:26 INFO SparkHadoopMapRedUtil: attempt_202304100310263385440263815417053_0011_m_000001_0: Committed\n",
            "23/04/10 03:10:26 INFO PythonRunner: Times: total = 47, boot = -310, init = 357, finish = 0\n",
            "23/04/10 03:10:26 INFO FileOutputCommitter: Saved output of task 'attempt_202304100310263385440263815417053_0011_m_000000_0' to file:/content/2_group.out/_temporary/0/task_202304100310263385440263815417053_0011_m_000000\n",
            "23/04/10 03:10:26 INFO SparkHadoopMapRedUtil: attempt_202304100310263385440263815417053_0011_m_000000_0: Committed\n",
            "23/04/10 03:10:26 INFO Executor: Finished task 1.0 in stage 3.0 (TID 6). 1608 bytes result sent to driver\n",
            "23/04/10 03:10:26 INFO Executor: Finished task 0.0 in stage 3.0 (TID 5). 1565 bytes result sent to driver\n",
            "23/04/10 03:10:26 INFO TaskSetManager: Finished task 1.0 in stage 3.0 (TID 6) in 215 ms on e77b3fc47c45 (executor driver) (1/2)\n",
            "23/04/10 03:10:26 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 5) in 223 ms on e77b3fc47c45 (executor driver) (2/2)\n",
            "23/04/10 03:10:26 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
            "23/04/10 03:10:26 INFO DAGScheduler: ResultStage 3 (runJob at SparkHadoopWriter.scala:83) finished in 0.276 s\n",
            "23/04/10 03:10:26 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "23/04/10 03:10:26 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished\n",
            "23/04/10 03:10:26 INFO DAGScheduler: Job 2 finished: runJob at SparkHadoopWriter.scala:83, took 0.284236 s\n",
            "23/04/10 03:10:26 INFO SparkHadoopWriter: Start to commit write Job job_202304100310263385440263815417053_0011.\n",
            "23/04/10 03:10:26 INFO SparkHadoopWriter: Write Job job_202304100310263385440263815417053_0011 committed. Elapsed time: 18 ms.\n",
            "23/04/10 03:10:26 INFO SparkUI: Stopped Spark web UI at http://e77b3fc47c45:4040\n",
            "23/04/10 03:10:26 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "23/04/10 03:10:27 INFO MemoryStore: MemoryStore cleared\n",
            "23/04/10 03:10:27 INFO BlockManager: BlockManager stopped\n",
            "23/04/10 03:10:27 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "23/04/10 03:10:27 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "23/04/10 03:10:27 INFO SparkContext: Successfully stopped SparkContext\n",
            "23/04/10 03:10:27 INFO ShutdownHookManager: Shutdown hook called\n",
            "23/04/10 03:10:27 INFO ShutdownHookManager: Deleting directory /tmp/spark-5d0805fa-95a8-4be5-9cc3-ba34f59b71a8\n",
            "23/04/10 03:10:27 INFO ShutdownHookManager: Deleting directory /tmp/spark-12b0f44e-d31b-48b2-a540-92fc65be8a7c\n",
            "23/04/10 03:10:27 INFO ShutdownHookManager: Deleting directory /tmp/spark-12b0f44e-d31b-48b2-a540-92fc65be8a7c/pyspark-44248f98-3daf-443a-8816-76d6c615c59d\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test2(lines):\n",
        "    assert lines[0].strip() == \"('rpsabo', 88)\"\n",
        "    print(\"passed\")"
      ],
      "metadata": {
        "id": "TjMRs2KYo0Hd"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvpqT474__D0",
        "outputId": "77cc0e13-df51-4977-89d0-5e4d351ef2f8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# test local execution results\n",
        "with open('2_group.out/part-00001') as f:\n",
        "  lines = f.readlines()\n",
        "  test2(lines)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "passed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DeAaRlMR__D1"
      },
      "source": [
        "## Job 3. Count the tweets per day."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fTMTHj9O__D2",
        "outputId": "626abe57-946f-4098-ba8d-c282c3bdf36f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%file 3_days.py\n",
        "\n",
        "'''\n",
        "TODO:\n",
        "Count the tweets per day.\n",
        "\n",
        "See http://mike.teczno.com/notes/streaming-data-from-twitter.html for parsing info.\n",
        "Get the screen name by accessing tweet['user']['screen_name']\n",
        "\n",
        "Look at tweet['created_at'] for datetime of creation. Just use the first word in the date to get the day.\n",
        "\n",
        "'''\n",
        "\n",
        "from pyspark import SparkContext, SparkConf\n",
        "import json\n",
        "from operator import add\n",
        "\n",
        "def checkjson(entry):\n",
        "  try:\n",
        "    tweet = json.loads(entry)\n",
        "    date = tweet['created_at']\n",
        "    day = date.split(' ', 1)[0]\n",
        "    return day, 1   \n",
        "  except:\n",
        "    pass\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  #conf = SparkConf().setAppName('1_count').set('spark.hadoop.validateOutputSpecs', False)\n",
        "  #sc = SparkContext(conf=conf).getOrCreate()\n",
        "  sc = SparkContext(appName=\"3_days\")\n",
        "\n",
        "\n",
        "  try:\n",
        "    tweets=sc.textFile('s3://vandy-bigdata-2/nashville-tweets-2019-01-28')\n",
        "    counts = tweets.map(lambda tweet: checkjson(tweet))\n",
        "    print(counts.take(2))\n",
        "\n",
        "    total = counts.map(lambda x: (x)).reduceByKey(lambda a, b: a + b)\n",
        "    total.repartition(1).saveAsTextFile(\"s3://vandy-bigdata-2/hw6/3_days.out\")\n",
        "\n",
        "  finally:\n",
        "    sc.stop()\n",
        "\n",
        "  pass"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing 3_days.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6phVcUW__D2",
        "outputId": "14889a5f-433a-475a-9b20-1d90aeb215ed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# execute locally\n",
        "!spark-submit --packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.4.7 3_days.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ":: loading settings :: url = jar:file:/content/spark-3.2.3-bin-hadoop3.2/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
            "Ivy Default Cache set to: /root/.ivy2/cache\n",
            "The jars for the packages stored in: /root/.ivy2/jars\n",
            "org.apache.spark#spark-streaming-kafka-0-8_2.11 added as a dependency\n",
            ":: resolving dependencies :: org.apache.spark#spark-submit-parent-c32f8e2d-7f31-4065-92a8-8854bd6765c1;1.0\n",
            "\tconfs: [default]\n",
            "\tfound org.apache.spark#spark-streaming-kafka-0-8_2.11;2.4.7 in central\n",
            "\tfound org.apache.kafka#kafka_2.11;0.8.2.1 in central\n",
            "\tfound org.scala-lang.modules#scala-xml_2.11;1.0.2 in central\n",
            "\tfound com.yammer.metrics#metrics-core;2.2.0 in central\n",
            "\tfound org.slf4j#slf4j-api;1.7.16 in central\n",
            "\tfound org.scala-lang.modules#scala-parser-combinators_2.11;1.1.0 in central\n",
            "\tfound com.101tec#zkclient;0.3 in central\n",
            "\tfound log4j#log4j;1.2.17 in central\n",
            "\tfound org.apache.kafka#kafka-clients;0.8.2.1 in central\n",
            "\tfound net.jpountz.lz4#lz4;1.2.0 in central\n",
            "\tfound org.xerial.snappy#snappy-java;1.1.7.5 in central\n",
            "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
            ":: resolution report :: resolve 808ms :: artifacts dl 22ms\n",
            "\t:: modules in use:\n",
            "\tcom.101tec#zkclient;0.3 from central in [default]\n",
            "\tcom.yammer.metrics#metrics-core;2.2.0 from central in [default]\n",
            "\tlog4j#log4j;1.2.17 from central in [default]\n",
            "\tnet.jpountz.lz4#lz4;1.2.0 from central in [default]\n",
            "\torg.apache.kafka#kafka-clients;0.8.2.1 from central in [default]\n",
            "\torg.apache.kafka#kafka_2.11;0.8.2.1 from central in [default]\n",
            "\torg.apache.spark#spark-streaming-kafka-0-8_2.11;2.4.7 from central in [default]\n",
            "\torg.scala-lang.modules#scala-parser-combinators_2.11;1.1.0 from central in [default]\n",
            "\torg.scala-lang.modules#scala-xml_2.11;1.0.2 from central in [default]\n",
            "\torg.slf4j#slf4j-api;1.7.16 from central in [default]\n",
            "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
            "\torg.xerial.snappy#snappy-java;1.1.7.5 from central in [default]\n",
            "\t---------------------------------------------------------------------\n",
            "\t|                  |            modules            ||   artifacts   |\n",
            "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
            "\t---------------------------------------------------------------------\n",
            "\t|      default     |   12  |   0   |   0   |   0   ||   12  |   0   |\n",
            "\t---------------------------------------------------------------------\n",
            ":: retrieving :: org.apache.spark#spark-submit-parent-c32f8e2d-7f31-4065-92a8-8854bd6765c1\n",
            "\tconfs: [default]\n",
            "\t0 artifacts copied, 12 already retrieved (0kB/14ms)\n",
            "23/04/10 03:17:21 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
            "23/04/10 03:17:22 INFO SparkContext: Running Spark version 3.2.3\n",
            "23/04/10 03:17:22 INFO ResourceUtils: ==============================================================\n",
            "23/04/10 03:17:22 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "23/04/10 03:17:22 INFO ResourceUtils: ==============================================================\n",
            "23/04/10 03:17:22 INFO SparkContext: Submitted application: 1_count\n",
            "23/04/10 03:17:22 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "23/04/10 03:17:22 INFO ResourceProfile: Limiting resource is cpu\n",
            "23/04/10 03:17:22 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "23/04/10 03:17:22 INFO SecurityManager: Changing view acls to: root\n",
            "23/04/10 03:17:22 INFO SecurityManager: Changing modify acls to: root\n",
            "23/04/10 03:17:22 INFO SecurityManager: Changing view acls groups to: \n",
            "23/04/10 03:17:22 INFO SecurityManager: Changing modify acls groups to: \n",
            "23/04/10 03:17:22 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\n",
            "23/04/10 03:17:23 INFO Utils: Successfully started service 'sparkDriver' on port 46737.\n",
            "23/04/10 03:17:23 INFO SparkEnv: Registering MapOutputTracker\n",
            "23/04/10 03:17:23 INFO SparkEnv: Registering BlockManagerMaster\n",
            "23/04/10 03:17:23 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "23/04/10 03:17:23 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "23/04/10 03:17:23 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "23/04/10 03:17:23 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-a5399835-fdb1-4d9d-9569-b1bb33b05dbf\n",
            "23/04/10 03:17:23 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB\n",
            "23/04/10 03:17:23 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "23/04/10 03:17:24 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
            "23/04/10 03:17:24 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://e77b3fc47c45:4040\n",
            "23/04/10 03:17:24 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar at spark://e77b3fc47c45:46737/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1681096642783\n",
            "23/04/10 03:17:24 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar at spark://e77b3fc47c45:46737/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1681096642783\n",
            "23/04/10 03:17:24 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar at spark://e77b3fc47c45:46737/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1681096642783\n",
            "23/04/10 03:17:24 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar at spark://e77b3fc47c45:46737/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1681096642783\n",
            "23/04/10 03:17:24 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar at spark://e77b3fc47c45:46737/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1681096642783\n",
            "23/04/10 03:17:24 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar at spark://e77b3fc47c45:46737/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1681096642783\n",
            "23/04/10 03:17:24 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar at spark://e77b3fc47c45:46737/jars/com.101tec_zkclient-0.3.jar with timestamp 1681096642783\n",
            "23/04/10 03:17:24 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar at spark://e77b3fc47c45:46737/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1681096642783\n",
            "23/04/10 03:17:24 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar at spark://e77b3fc47c45:46737/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1681096642783\n",
            "23/04/10 03:17:24 INFO SparkContext: Added JAR file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar at spark://e77b3fc47c45:46737/jars/log4j_log4j-1.2.17.jar with timestamp 1681096642783\n",
            "23/04/10 03:17:24 INFO SparkContext: Added JAR file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar at spark://e77b3fc47c45:46737/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1681096642783\n",
            "23/04/10 03:17:24 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar at spark://e77b3fc47c45:46737/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1681096642783\n",
            "23/04/10 03:17:24 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar at file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1681096642783\n",
            "23/04/10 03:17:24 INFO Utils: Copying /root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar to /tmp/spark-4c48e978-a241-4af5-bf47-a44f8c9d3b50/userFiles-851bc9e1-2161-4e41-9621-b40a9518c024/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar\n",
            "23/04/10 03:17:24 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar at file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1681096642783\n",
            "23/04/10 03:17:24 INFO Utils: Copying /root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar to /tmp/spark-4c48e978-a241-4af5-bf47-a44f8c9d3b50/userFiles-851bc9e1-2161-4e41-9621-b40a9518c024/org.apache.kafka_kafka_2.11-0.8.2.1.jar\n",
            "23/04/10 03:17:24 INFO SparkContext: Added file file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar at file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1681096642783\n",
            "23/04/10 03:17:24 INFO Utils: Copying /root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-4c48e978-a241-4af5-bf47-a44f8c9d3b50/userFiles-851bc9e1-2161-4e41-9621-b40a9518c024/org.spark-project.spark_unused-1.0.0.jar\n",
            "23/04/10 03:17:24 INFO SparkContext: Added file file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar at file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1681096642783\n",
            "23/04/10 03:17:24 INFO Utils: Copying /root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar to /tmp/spark-4c48e978-a241-4af5-bf47-a44f8c9d3b50/userFiles-851bc9e1-2161-4e41-9621-b40a9518c024/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar\n",
            "23/04/10 03:17:24 INFO SparkContext: Added file file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar at file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1681096642783\n",
            "23/04/10 03:17:24 INFO Utils: Copying /root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar to /tmp/spark-4c48e978-a241-4af5-bf47-a44f8c9d3b50/userFiles-851bc9e1-2161-4e41-9621-b40a9518c024/com.yammer.metrics_metrics-core-2.2.0.jar\n",
            "23/04/10 03:17:24 INFO SparkContext: Added file file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar at file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1681096642783\n",
            "23/04/10 03:17:24 INFO Utils: Copying /root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar to /tmp/spark-4c48e978-a241-4af5-bf47-a44f8c9d3b50/userFiles-851bc9e1-2161-4e41-9621-b40a9518c024/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar\n",
            "23/04/10 03:17:24 INFO SparkContext: Added file file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar at file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar with timestamp 1681096642783\n",
            "23/04/10 03:17:24 INFO Utils: Copying /root/.ivy2/jars/com.101tec_zkclient-0.3.jar to /tmp/spark-4c48e978-a241-4af5-bf47-a44f8c9d3b50/userFiles-851bc9e1-2161-4e41-9621-b40a9518c024/com.101tec_zkclient-0.3.jar\n",
            "23/04/10 03:17:24 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar at file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1681096642783\n",
            "23/04/10 03:17:24 INFO Utils: Copying /root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar to /tmp/spark-4c48e978-a241-4af5-bf47-a44f8c9d3b50/userFiles-851bc9e1-2161-4e41-9621-b40a9518c024/org.apache.kafka_kafka-clients-0.8.2.1.jar\n",
            "23/04/10 03:17:24 INFO SparkContext: Added file file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar at file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1681096642783\n",
            "23/04/10 03:17:24 INFO Utils: Copying /root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar to /tmp/spark-4c48e978-a241-4af5-bf47-a44f8c9d3b50/userFiles-851bc9e1-2161-4e41-9621-b40a9518c024/org.slf4j_slf4j-api-1.7.16.jar\n",
            "23/04/10 03:17:24 INFO SparkContext: Added file file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar at file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar with timestamp 1681096642783\n",
            "23/04/10 03:17:24 INFO Utils: Copying /root/.ivy2/jars/log4j_log4j-1.2.17.jar to /tmp/spark-4c48e978-a241-4af5-bf47-a44f8c9d3b50/userFiles-851bc9e1-2161-4e41-9621-b40a9518c024/log4j_log4j-1.2.17.jar\n",
            "23/04/10 03:17:24 INFO SparkContext: Added file file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar at file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1681096642783\n",
            "23/04/10 03:17:24 INFO Utils: Copying /root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar to /tmp/spark-4c48e978-a241-4af5-bf47-a44f8c9d3b50/userFiles-851bc9e1-2161-4e41-9621-b40a9518c024/net.jpountz.lz4_lz4-1.2.0.jar\n",
            "23/04/10 03:17:24 INFO SparkContext: Added file file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar at file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1681096642783\n",
            "23/04/10 03:17:24 INFO Utils: Copying /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar to /tmp/spark-4c48e978-a241-4af5-bf47-a44f8c9d3b50/userFiles-851bc9e1-2161-4e41-9621-b40a9518c024/org.xerial.snappy_snappy-java-1.1.7.5.jar\n",
            "23/04/10 03:17:24 INFO Executor: Starting executor ID driver on host e77b3fc47c45\n",
            "23/04/10 03:17:24 INFO Executor: Fetching file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar with timestamp 1681096642783\n",
            "23/04/10 03:17:24 INFO Utils: /root/.ivy2/jars/log4j_log4j-1.2.17.jar has been previously copied to /tmp/spark-4c48e978-a241-4af5-bf47-a44f8c9d3b50/userFiles-851bc9e1-2161-4e41-9621-b40a9518c024/log4j_log4j-1.2.17.jar\n",
            "23/04/10 03:17:24 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1681096642783\n",
            "23/04/10 03:17:24 INFO Utils: /root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar has been previously copied to /tmp/spark-4c48e978-a241-4af5-bf47-a44f8c9d3b50/userFiles-851bc9e1-2161-4e41-9621-b40a9518c024/org.apache.kafka_kafka-clients-0.8.2.1.jar\n",
            "23/04/10 03:17:24 INFO Executor: Fetching file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1681096642783\n",
            "23/04/10 03:17:24 INFO Utils: /root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar has been previously copied to /tmp/spark-4c48e978-a241-4af5-bf47-a44f8c9d3b50/userFiles-851bc9e1-2161-4e41-9621-b40a9518c024/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar\n",
            "23/04/10 03:17:24 INFO Executor: Fetching file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1681096642783\n",
            "23/04/10 03:17:24 INFO Utils: /root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar has been previously copied to /tmp/spark-4c48e978-a241-4af5-bf47-a44f8c9d3b50/userFiles-851bc9e1-2161-4e41-9621-b40a9518c024/org.slf4j_slf4j-api-1.7.16.jar\n",
            "23/04/10 03:17:24 INFO Executor: Fetching file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1681096642783\n",
            "23/04/10 03:17:24 INFO Utils: /root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar has been previously copied to /tmp/spark-4c48e978-a241-4af5-bf47-a44f8c9d3b50/userFiles-851bc9e1-2161-4e41-9621-b40a9518c024/net.jpountz.lz4_lz4-1.2.0.jar\n",
            "23/04/10 03:17:24 INFO Executor: Fetching file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1681096642783\n",
            "23/04/10 03:17:24 INFO Utils: /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar has been previously copied to /tmp/spark-4c48e978-a241-4af5-bf47-a44f8c9d3b50/userFiles-851bc9e1-2161-4e41-9621-b40a9518c024/org.xerial.snappy_snappy-java-1.1.7.5.jar\n",
            "23/04/10 03:17:24 INFO Executor: Fetching file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1681096642783\n",
            "23/04/10 03:17:24 INFO Utils: /root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar has been previously copied to /tmp/spark-4c48e978-a241-4af5-bf47-a44f8c9d3b50/userFiles-851bc9e1-2161-4e41-9621-b40a9518c024/org.spark-project.spark_unused-1.0.0.jar\n",
            "23/04/10 03:17:24 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1681096642783\n",
            "23/04/10 03:17:24 INFO Utils: /root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar has been previously copied to /tmp/spark-4c48e978-a241-4af5-bf47-a44f8c9d3b50/userFiles-851bc9e1-2161-4e41-9621-b40a9518c024/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar\n",
            "23/04/10 03:17:24 INFO Executor: Fetching file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1681096642783\n",
            "23/04/10 03:17:24 INFO Utils: /root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar has been previously copied to /tmp/spark-4c48e978-a241-4af5-bf47-a44f8c9d3b50/userFiles-851bc9e1-2161-4e41-9621-b40a9518c024/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar\n",
            "23/04/10 03:17:24 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1681096642783\n",
            "23/04/10 03:17:24 INFO Utils: /root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar has been previously copied to /tmp/spark-4c48e978-a241-4af5-bf47-a44f8c9d3b50/userFiles-851bc9e1-2161-4e41-9621-b40a9518c024/org.apache.kafka_kafka_2.11-0.8.2.1.jar\n",
            "23/04/10 03:17:24 INFO Executor: Fetching file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar with timestamp 1681096642783\n",
            "23/04/10 03:17:24 INFO Utils: /root/.ivy2/jars/com.101tec_zkclient-0.3.jar has been previously copied to /tmp/spark-4c48e978-a241-4af5-bf47-a44f8c9d3b50/userFiles-851bc9e1-2161-4e41-9621-b40a9518c024/com.101tec_zkclient-0.3.jar\n",
            "23/04/10 03:17:24 INFO Executor: Fetching file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1681096642783\n",
            "23/04/10 03:17:24 INFO Utils: /root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar has been previously copied to /tmp/spark-4c48e978-a241-4af5-bf47-a44f8c9d3b50/userFiles-851bc9e1-2161-4e41-9621-b40a9518c024/com.yammer.metrics_metrics-core-2.2.0.jar\n",
            "23/04/10 03:17:24 INFO Executor: Fetching spark://e77b3fc47c45:46737/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1681096642783\n",
            "23/04/10 03:17:24 INFO TransportClientFactory: Successfully created connection to e77b3fc47c45/172.28.0.12:46737 after 46 ms (0 ms spent in bootstraps)\n",
            "23/04/10 03:17:24 INFO Utils: Fetching spark://e77b3fc47c45:46737/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar to /tmp/spark-4c48e978-a241-4af5-bf47-a44f8c9d3b50/userFiles-851bc9e1-2161-4e41-9621-b40a9518c024/fetchFileTemp1508996713494999743.tmp\n",
            "23/04/10 03:17:24 INFO Utils: /tmp/spark-4c48e978-a241-4af5-bf47-a44f8c9d3b50/userFiles-851bc9e1-2161-4e41-9621-b40a9518c024/fetchFileTemp1508996713494999743.tmp has been previously copied to /tmp/spark-4c48e978-a241-4af5-bf47-a44f8c9d3b50/userFiles-851bc9e1-2161-4e41-9621-b40a9518c024/org.apache.kafka_kafka_2.11-0.8.2.1.jar\n",
            "23/04/10 03:17:24 INFO Executor: Adding file:/tmp/spark-4c48e978-a241-4af5-bf47-a44f8c9d3b50/userFiles-851bc9e1-2161-4e41-9621-b40a9518c024/org.apache.kafka_kafka_2.11-0.8.2.1.jar to class loader\n",
            "23/04/10 03:17:24 INFO Executor: Fetching spark://e77b3fc47c45:46737/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1681096642783\n",
            "23/04/10 03:17:24 INFO Utils: Fetching spark://e77b3fc47c45:46737/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-4c48e978-a241-4af5-bf47-a44f8c9d3b50/userFiles-851bc9e1-2161-4e41-9621-b40a9518c024/fetchFileTemp6364252769945245276.tmp\n",
            "23/04/10 03:17:24 INFO Utils: /tmp/spark-4c48e978-a241-4af5-bf47-a44f8c9d3b50/userFiles-851bc9e1-2161-4e41-9621-b40a9518c024/fetchFileTemp6364252769945245276.tmp has been previously copied to /tmp/spark-4c48e978-a241-4af5-bf47-a44f8c9d3b50/userFiles-851bc9e1-2161-4e41-9621-b40a9518c024/org.spark-project.spark_unused-1.0.0.jar\n",
            "23/04/10 03:17:24 INFO Executor: Adding file:/tmp/spark-4c48e978-a241-4af5-bf47-a44f8c9d3b50/userFiles-851bc9e1-2161-4e41-9621-b40a9518c024/org.spark-project.spark_unused-1.0.0.jar to class loader\n",
            "23/04/10 03:17:24 INFO Executor: Fetching spark://e77b3fc47c45:46737/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1681096642783\n",
            "23/04/10 03:17:24 INFO Utils: Fetching spark://e77b3fc47c45:46737/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar to /tmp/spark-4c48e978-a241-4af5-bf47-a44f8c9d3b50/userFiles-851bc9e1-2161-4e41-9621-b40a9518c024/fetchFileTemp4474693085277621462.tmp\n",
            "23/04/10 03:17:24 INFO Utils: /tmp/spark-4c48e978-a241-4af5-bf47-a44f8c9d3b50/userFiles-851bc9e1-2161-4e41-9621-b40a9518c024/fetchFileTemp4474693085277621462.tmp has been previously copied to /tmp/spark-4c48e978-a241-4af5-bf47-a44f8c9d3b50/userFiles-851bc9e1-2161-4e41-9621-b40a9518c024/org.xerial.snappy_snappy-java-1.1.7.5.jar\n",
            "23/04/10 03:17:24 INFO Executor: Adding file:/tmp/spark-4c48e978-a241-4af5-bf47-a44f8c9d3b50/userFiles-851bc9e1-2161-4e41-9621-b40a9518c024/org.xerial.snappy_snappy-java-1.1.7.5.jar to class loader\n",
            "23/04/10 03:17:24 INFO Executor: Fetching spark://e77b3fc47c45:46737/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1681096642783\n",
            "23/04/10 03:17:24 INFO Utils: Fetching spark://e77b3fc47c45:46737/jars/org.slf4j_slf4j-api-1.7.16.jar to /tmp/spark-4c48e978-a241-4af5-bf47-a44f8c9d3b50/userFiles-851bc9e1-2161-4e41-9621-b40a9518c024/fetchFileTemp1816816455500415177.tmp\n",
            "23/04/10 03:17:24 INFO Utils: /tmp/spark-4c48e978-a241-4af5-bf47-a44f8c9d3b50/userFiles-851bc9e1-2161-4e41-9621-b40a9518c024/fetchFileTemp1816816455500415177.tmp has been previously copied to /tmp/spark-4c48e978-a241-4af5-bf47-a44f8c9d3b50/userFiles-851bc9e1-2161-4e41-9621-b40a9518c024/org.slf4j_slf4j-api-1.7.16.jar\n",
            "23/04/10 03:17:24 INFO Executor: Adding file:/tmp/spark-4c48e978-a241-4af5-bf47-a44f8c9d3b50/userFiles-851bc9e1-2161-4e41-9621-b40a9518c024/org.slf4j_slf4j-api-1.7.16.jar to class loader\n",
            "23/04/10 03:17:24 INFO Executor: Fetching spark://e77b3fc47c45:46737/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1681096642783\n",
            "23/04/10 03:17:24 INFO Utils: Fetching spark://e77b3fc47c45:46737/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar to /tmp/spark-4c48e978-a241-4af5-bf47-a44f8c9d3b50/userFiles-851bc9e1-2161-4e41-9621-b40a9518c024/fetchFileTemp8620943391687090338.tmp\n",
            "23/04/10 03:17:24 INFO Utils: /tmp/spark-4c48e978-a241-4af5-bf47-a44f8c9d3b50/userFiles-851bc9e1-2161-4e41-9621-b40a9518c024/fetchFileTemp8620943391687090338.tmp has been previously copied to /tmp/spark-4c48e978-a241-4af5-bf47-a44f8c9d3b50/userFiles-851bc9e1-2161-4e41-9621-b40a9518c024/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar\n",
            "23/04/10 03:17:24 INFO Executor: Adding file:/tmp/spark-4c48e978-a241-4af5-bf47-a44f8c9d3b50/userFiles-851bc9e1-2161-4e41-9621-b40a9518c024/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar to class loader\n",
            "23/04/10 03:17:24 INFO Executor: Fetching spark://e77b3fc47c45:46737/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1681096642783\n",
            "23/04/10 03:17:24 INFO Utils: Fetching spark://e77b3fc47c45:46737/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar to /tmp/spark-4c48e978-a241-4af5-bf47-a44f8c9d3b50/userFiles-851bc9e1-2161-4e41-9621-b40a9518c024/fetchFileTemp2368873691538666841.tmp\n",
            "23/04/10 03:17:24 INFO Utils: /tmp/spark-4c48e978-a241-4af5-bf47-a44f8c9d3b50/userFiles-851bc9e1-2161-4e41-9621-b40a9518c024/fetchFileTemp2368873691538666841.tmp has been previously copied to /tmp/spark-4c48e978-a241-4af5-bf47-a44f8c9d3b50/userFiles-851bc9e1-2161-4e41-9621-b40a9518c024/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar\n",
            "23/04/10 03:17:24 INFO Executor: Adding file:/tmp/spark-4c48e978-a241-4af5-bf47-a44f8c9d3b50/userFiles-851bc9e1-2161-4e41-9621-b40a9518c024/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar to class loader\n",
            "23/04/10 03:17:24 INFO Executor: Fetching spark://e77b3fc47c45:46737/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1681096642783\n",
            "23/04/10 03:17:24 INFO Utils: Fetching spark://e77b3fc47c45:46737/jars/net.jpountz.lz4_lz4-1.2.0.jar to /tmp/spark-4c48e978-a241-4af5-bf47-a44f8c9d3b50/userFiles-851bc9e1-2161-4e41-9621-b40a9518c024/fetchFileTemp4786602023410130633.tmp\n",
            "23/04/10 03:17:24 INFO Utils: /tmp/spark-4c48e978-a241-4af5-bf47-a44f8c9d3b50/userFiles-851bc9e1-2161-4e41-9621-b40a9518c024/fetchFileTemp4786602023410130633.tmp has been previously copied to /tmp/spark-4c48e978-a241-4af5-bf47-a44f8c9d3b50/userFiles-851bc9e1-2161-4e41-9621-b40a9518c024/net.jpountz.lz4_lz4-1.2.0.jar\n",
            "23/04/10 03:17:24 INFO Executor: Adding file:/tmp/spark-4c48e978-a241-4af5-bf47-a44f8c9d3b50/userFiles-851bc9e1-2161-4e41-9621-b40a9518c024/net.jpountz.lz4_lz4-1.2.0.jar to class loader\n",
            "23/04/10 03:17:24 INFO Executor: Fetching spark://e77b3fc47c45:46737/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1681096642783\n",
            "23/04/10 03:17:24 INFO Utils: Fetching spark://e77b3fc47c45:46737/jars/com.yammer.metrics_metrics-core-2.2.0.jar to /tmp/spark-4c48e978-a241-4af5-bf47-a44f8c9d3b50/userFiles-851bc9e1-2161-4e41-9621-b40a9518c024/fetchFileTemp1773570204901822036.tmp\n",
            "23/04/10 03:17:24 INFO Utils: /tmp/spark-4c48e978-a241-4af5-bf47-a44f8c9d3b50/userFiles-851bc9e1-2161-4e41-9621-b40a9518c024/fetchFileTemp1773570204901822036.tmp has been previously copied to /tmp/spark-4c48e978-a241-4af5-bf47-a44f8c9d3b50/userFiles-851bc9e1-2161-4e41-9621-b40a9518c024/com.yammer.metrics_metrics-core-2.2.0.jar\n",
            "23/04/10 03:17:24 INFO Executor: Adding file:/tmp/spark-4c48e978-a241-4af5-bf47-a44f8c9d3b50/userFiles-851bc9e1-2161-4e41-9621-b40a9518c024/com.yammer.metrics_metrics-core-2.2.0.jar to class loader\n",
            "23/04/10 03:17:24 INFO Executor: Fetching spark://e77b3fc47c45:46737/jars/log4j_log4j-1.2.17.jar with timestamp 1681096642783\n",
            "23/04/10 03:17:24 INFO Utils: Fetching spark://e77b3fc47c45:46737/jars/log4j_log4j-1.2.17.jar to /tmp/spark-4c48e978-a241-4af5-bf47-a44f8c9d3b50/userFiles-851bc9e1-2161-4e41-9621-b40a9518c024/fetchFileTemp3713658249280851139.tmp\n",
            "23/04/10 03:17:24 INFO Utils: /tmp/spark-4c48e978-a241-4af5-bf47-a44f8c9d3b50/userFiles-851bc9e1-2161-4e41-9621-b40a9518c024/fetchFileTemp3713658249280851139.tmp has been previously copied to /tmp/spark-4c48e978-a241-4af5-bf47-a44f8c9d3b50/userFiles-851bc9e1-2161-4e41-9621-b40a9518c024/log4j_log4j-1.2.17.jar\n",
            "23/04/10 03:17:24 INFO Executor: Adding file:/tmp/spark-4c48e978-a241-4af5-bf47-a44f8c9d3b50/userFiles-851bc9e1-2161-4e41-9621-b40a9518c024/log4j_log4j-1.2.17.jar to class loader\n",
            "23/04/10 03:17:24 INFO Executor: Fetching spark://e77b3fc47c45:46737/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1681096642783\n",
            "23/04/10 03:17:24 INFO Utils: Fetching spark://e77b3fc47c45:46737/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar to /tmp/spark-4c48e978-a241-4af5-bf47-a44f8c9d3b50/userFiles-851bc9e1-2161-4e41-9621-b40a9518c024/fetchFileTemp8081686956009798410.tmp\n",
            "23/04/10 03:17:24 INFO Utils: /tmp/spark-4c48e978-a241-4af5-bf47-a44f8c9d3b50/userFiles-851bc9e1-2161-4e41-9621-b40a9518c024/fetchFileTemp8081686956009798410.tmp has been previously copied to /tmp/spark-4c48e978-a241-4af5-bf47-a44f8c9d3b50/userFiles-851bc9e1-2161-4e41-9621-b40a9518c024/org.apache.kafka_kafka-clients-0.8.2.1.jar\n",
            "23/04/10 03:17:24 INFO Executor: Adding file:/tmp/spark-4c48e978-a241-4af5-bf47-a44f8c9d3b50/userFiles-851bc9e1-2161-4e41-9621-b40a9518c024/org.apache.kafka_kafka-clients-0.8.2.1.jar to class loader\n",
            "23/04/10 03:17:24 INFO Executor: Fetching spark://e77b3fc47c45:46737/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1681096642783\n",
            "23/04/10 03:17:24 INFO Utils: Fetching spark://e77b3fc47c45:46737/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar to /tmp/spark-4c48e978-a241-4af5-bf47-a44f8c9d3b50/userFiles-851bc9e1-2161-4e41-9621-b40a9518c024/fetchFileTemp7425057473224869810.tmp\n",
            "23/04/10 03:17:24 INFO Utils: /tmp/spark-4c48e978-a241-4af5-bf47-a44f8c9d3b50/userFiles-851bc9e1-2161-4e41-9621-b40a9518c024/fetchFileTemp7425057473224869810.tmp has been previously copied to /tmp/spark-4c48e978-a241-4af5-bf47-a44f8c9d3b50/userFiles-851bc9e1-2161-4e41-9621-b40a9518c024/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar\n",
            "23/04/10 03:17:24 INFO Executor: Adding file:/tmp/spark-4c48e978-a241-4af5-bf47-a44f8c9d3b50/userFiles-851bc9e1-2161-4e41-9621-b40a9518c024/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar to class loader\n",
            "23/04/10 03:17:24 INFO Executor: Fetching spark://e77b3fc47c45:46737/jars/com.101tec_zkclient-0.3.jar with timestamp 1681096642783\n",
            "23/04/10 03:17:24 INFO Utils: Fetching spark://e77b3fc47c45:46737/jars/com.101tec_zkclient-0.3.jar to /tmp/spark-4c48e978-a241-4af5-bf47-a44f8c9d3b50/userFiles-851bc9e1-2161-4e41-9621-b40a9518c024/fetchFileTemp4979859098002979462.tmp\n",
            "23/04/10 03:17:24 INFO Utils: /tmp/spark-4c48e978-a241-4af5-bf47-a44f8c9d3b50/userFiles-851bc9e1-2161-4e41-9621-b40a9518c024/fetchFileTemp4979859098002979462.tmp has been previously copied to /tmp/spark-4c48e978-a241-4af5-bf47-a44f8c9d3b50/userFiles-851bc9e1-2161-4e41-9621-b40a9518c024/com.101tec_zkclient-0.3.jar\n",
            "23/04/10 03:17:24 INFO Executor: Adding file:/tmp/spark-4c48e978-a241-4af5-bf47-a44f8c9d3b50/userFiles-851bc9e1-2161-4e41-9621-b40a9518c024/com.101tec_zkclient-0.3.jar to class loader\n",
            "23/04/10 03:17:24 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46699.\n",
            "23/04/10 03:17:24 INFO NettyBlockTransferService: Server created on e77b3fc47c45:46699\n",
            "23/04/10 03:17:24 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "23/04/10 03:17:24 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, e77b3fc47c45, 46699, None)\n",
            "23/04/10 03:17:25 INFO BlockManagerMasterEndpoint: Registering block manager e77b3fc47c45:46699 with 366.3 MiB RAM, BlockManagerId(driver, e77b3fc47c45, 46699, None)\n",
            "23/04/10 03:17:25 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, e77b3fc47c45, 46699, None)\n",
            "23/04/10 03:17:25 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, e77b3fc47c45, 46699, None)\n",
            "23/04/10 03:17:26 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 354.3 KiB, free 366.0 MiB)\n",
            "23/04/10 03:17:26 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 32.0 KiB, free 365.9 MiB)\n",
            "23/04/10 03:17:26 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on e77b3fc47c45:46699 (size: 32.0 KiB, free: 366.3 MiB)\n",
            "23/04/10 03:17:26 INFO SparkContext: Created broadcast 0 from textFile at NativeMethodAccessorImpl.java:0\n",
            "23/04/10 03:17:26 INFO FileInputFormat: Total input files to process : 1\n",
            "23/04/10 03:17:26 INFO SparkContext: Starting job: runJob at PythonRDD.scala:166\n",
            "23/04/10 03:17:26 INFO DAGScheduler: Got job 0 (runJob at PythonRDD.scala:166) with 1 output partitions\n",
            "23/04/10 03:17:26 INFO DAGScheduler: Final stage: ResultStage 0 (runJob at PythonRDD.scala:166)\n",
            "23/04/10 03:17:26 INFO DAGScheduler: Parents of final stage: List()\n",
            "23/04/10 03:17:26 INFO DAGScheduler: Missing parents: List()\n",
            "23/04/10 03:17:26 INFO DAGScheduler: Submitting ResultStage 0 (PythonRDD[2] at RDD at PythonRDD.scala:53), which has no missing parents\n",
            "23/04/10 03:17:26 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 8.9 KiB, free 365.9 MiB)\n",
            "23/04/10 03:17:26 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.5 KiB, free 365.9 MiB)\n",
            "23/04/10 03:17:26 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on e77b3fc47c45:46699 (size: 5.5 KiB, free: 366.3 MiB)\n",
            "23/04/10 03:17:26 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1478\n",
            "23/04/10 03:17:26 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (PythonRDD[2] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(0))\n",
            "23/04/10 03:17:26 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
            "23/04/10 03:17:27 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (e77b3fc47c45, executor driver, partition 0, PROCESS_LOCAL, 4510 bytes) taskResourceAssignments Map()\n",
            "23/04/10 03:17:27 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
            "23/04/10 03:17:28 INFO HadoopRDD: Input split: file:/content/nashville-tweets-2019-01-28:0+12519625\n",
            "23/04/10 03:17:29 INFO PythonRunner: Times: total = 1003, boot = 948, init = 54, finish = 1\n",
            "23/04/10 03:17:29 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1440 bytes result sent to driver\n",
            "23/04/10 03:17:29 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 2481 ms on e77b3fc47c45 (executor driver) (1/1)\n",
            "23/04/10 03:17:29 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
            "23/04/10 03:17:29 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 56257\n",
            "23/04/10 03:17:29 INFO DAGScheduler: ResultStage 0 (runJob at PythonRDD.scala:166) finished in 3.113 s\n",
            "23/04/10 03:17:29 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "23/04/10 03:17:29 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
            "23/04/10 03:17:29 INFO DAGScheduler: Job 0 finished: runJob at PythonRDD.scala:166, took 3.263446 s\n",
            "[('Sun', 1), ('Sun', 1)]\n",
            "23/04/10 03:17:30 INFO deprecation: mapred.output.dir is deprecated. Instead, use mapreduce.output.fileoutputformat.outputdir\n",
            "23/04/10 03:17:30 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
            "23/04/10 03:17:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "23/04/10 03:17:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "23/04/10 03:17:30 INFO SparkContext: Starting job: runJob at SparkHadoopWriter.scala:83\n",
            "23/04/10 03:17:30 INFO DAGScheduler: Registering RDD 4 (reduceByKey at /content/3_days.py:49) as input to shuffle 1\n",
            "23/04/10 03:17:30 INFO DAGScheduler: Registering RDD 8 (coalesce at NativeMethodAccessorImpl.java:0) as input to shuffle 0\n",
            "23/04/10 03:17:30 INFO DAGScheduler: Got job 1 (runJob at SparkHadoopWriter.scala:83) with 1 output partitions\n",
            "23/04/10 03:17:30 INFO DAGScheduler: Final stage: ResultStage 3 (runJob at SparkHadoopWriter.scala:83)\n",
            "23/04/10 03:17:30 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)\n",
            "23/04/10 03:17:30 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 2)\n",
            "23/04/10 03:17:30 INFO DAGScheduler: Submitting ShuffleMapStage 1 (PairwiseRDD[4] at reduceByKey at /content/3_days.py:49), which has no missing parents\n",
            "23/04/10 03:17:30 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 12.6 KiB, free 365.9 MiB)\n",
            "23/04/10 03:17:30 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 365.9 MiB)\n",
            "23/04/10 03:17:30 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on e77b3fc47c45:46699 (size: 7.7 KiB, free: 366.3 MiB)\n",
            "23/04/10 03:17:30 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1478\n",
            "23/04/10 03:17:30 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 1 (PairwiseRDD[4] at reduceByKey at /content/3_days.py:49) (first 15 tasks are for partitions Vector(0, 1))\n",
            "23/04/10 03:17:30 INFO TaskSchedulerImpl: Adding task set 1.0 with 2 tasks resource profile 0\n",
            "23/04/10 03:17:30 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (e77b3fc47c45, executor driver, partition 0, PROCESS_LOCAL, 4499 bytes) taskResourceAssignments Map()\n",
            "23/04/10 03:17:30 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 2) (e77b3fc47c45, executor driver, partition 1, PROCESS_LOCAL, 4499 bytes) taskResourceAssignments Map()\n",
            "23/04/10 03:17:30 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)\n",
            "23/04/10 03:17:30 INFO Executor: Running task 1.0 in stage 1.0 (TID 2)\n",
            "23/04/10 03:17:30 INFO HadoopRDD: Input split: file:/content/nashville-tweets-2019-01-28:12519625+12519626\n",
            "23/04/10 03:17:30 INFO HadoopRDD: Input split: file:/content/nashville-tweets-2019-01-28:0+12519625\n",
            "23/04/10 03:17:31 INFO PythonRunner: Times: total = 741, boot = 40, init = 62, finish = 639\n",
            "23/04/10 03:17:31 INFO PythonRunner: Times: total = 744, boot = 27, init = 16, finish = 701\n",
            "23/04/10 03:17:31 INFO Executor: Finished task 1.0 in stage 1.0 (TID 2). 1569 bytes result sent to driver\n",
            "23/04/10 03:17:31 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 2) in 1301 ms on e77b3fc47c45 (executor driver) (1/2)\n",
            "23/04/10 03:17:31 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1612 bytes result sent to driver\n",
            "23/04/10 03:17:31 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 1313 ms on e77b3fc47c45 (executor driver) (2/2)\n",
            "23/04/10 03:17:31 INFO DAGScheduler: ShuffleMapStage 1 (reduceByKey at /content/3_days.py:49) finished in 1.388 s\n",
            "23/04/10 03:17:31 INFO DAGScheduler: looking for newly runnable stages\n",
            "23/04/10 03:17:31 INFO DAGScheduler: running: Set()\n",
            "23/04/10 03:17:31 INFO DAGScheduler: waiting: Set(ShuffleMapStage 2, ResultStage 3)\n",
            "23/04/10 03:17:31 INFO DAGScheduler: failed: Set()\n",
            "23/04/10 03:17:31 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
            "23/04/10 03:17:31 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[8] at coalesce at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/04/10 03:17:31 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 10.6 KiB, free 365.9 MiB)\n",
            "23/04/10 03:17:31 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 6.2 KiB, free 365.9 MiB)\n",
            "23/04/10 03:17:31 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on e77b3fc47c45:46699 (size: 6.2 KiB, free: 366.2 MiB)\n",
            "23/04/10 03:17:31 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1478\n",
            "23/04/10 03:17:31 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[8] at coalesce at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
            "23/04/10 03:17:31 INFO TaskSchedulerImpl: Adding task set 2.0 with 2 tasks resource profile 0\n",
            "23/04/10 03:17:31 INFO TaskSetManager: Starting task 1.0 in stage 2.0 (TID 3) (e77b3fc47c45, executor driver, partition 1, NODE_LOCAL, 4260 bytes) taskResourceAssignments Map()\n",
            "23/04/10 03:17:31 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 4) (e77b3fc47c45, executor driver, partition 0, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()\n",
            "23/04/10 03:17:31 INFO Executor: Running task 0.0 in stage 2.0 (TID 4)\n",
            "23/04/10 03:17:31 INFO Executor: Running task 1.0 in stage 2.0 (TID 3)\n",
            "23/04/10 03:17:31 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/10 03:17:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 33 ms\n",
            "23/04/10 03:17:31 INFO ShuffleBlockFetcherIterator: Getting 2 (144.0 B) non-empty blocks including 2 (144.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/10 03:17:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 31 ms\n",
            "23/04/10 03:17:31 INFO PythonRunner: Times: total = 23, boot = -616, init = 639, finish = 0\n",
            "23/04/10 03:17:31 INFO Executor: Finished task 0.0 in stage 2.0 (TID 4). 1654 bytes result sent to driver\n",
            "23/04/10 03:17:32 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 4) in 151 ms on e77b3fc47c45 (executor driver) (1/2)\n",
            "23/04/10 03:17:32 INFO PythonRunner: Times: total = 64, boot = -634, init = 698, finish = 0\n",
            "23/04/10 03:17:32 INFO Executor: Finished task 1.0 in stage 2.0 (TID 3). 1783 bytes result sent to driver\n",
            "23/04/10 03:17:32 INFO TaskSetManager: Finished task 1.0 in stage 2.0 (TID 3) in 189 ms on e77b3fc47c45 (executor driver) (2/2)\n",
            "23/04/10 03:17:32 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
            "23/04/10 03:17:32 INFO DAGScheduler: ShuffleMapStage 2 (coalesce at NativeMethodAccessorImpl.java:0) finished in 0.220 s\n",
            "23/04/10 03:17:32 INFO DAGScheduler: looking for newly runnable stages\n",
            "23/04/10 03:17:32 INFO DAGScheduler: running: Set()\n",
            "23/04/10 03:17:32 INFO DAGScheduler: waiting: Set(ResultStage 3)\n",
            "23/04/10 03:17:32 INFO DAGScheduler: failed: Set()\n",
            "23/04/10 03:17:32 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[14] at saveAsTextFile at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/04/10 03:17:32 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 105.9 KiB, free 365.8 MiB)\n",
            "23/04/10 03:17:32 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 39.6 KiB, free 365.7 MiB)\n",
            "23/04/10 03:17:32 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on e77b3fc47c45:46699 (size: 39.6 KiB, free: 366.2 MiB)\n",
            "23/04/10 03:17:32 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1478\n",
            "23/04/10 03:17:32 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[14] at saveAsTextFile at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/04/10 03:17:32 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0\n",
            "23/04/10 03:17:32 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 5) (e77b3fc47c45, executor driver, partition 0, NODE_LOCAL, 4547 bytes) taskResourceAssignments Map()\n",
            "23/04/10 03:17:32 INFO Executor: Running task 0.0 in stage 3.0 (TID 5)\n",
            "23/04/10 03:17:32 INFO BlockManagerInfo: Removed broadcast_3_piece0 on e77b3fc47c45:46699 in memory (size: 6.2 KiB, free: 366.2 MiB)\n",
            "23/04/10 03:17:32 INFO BlockManagerInfo: Removed broadcast_2_piece0 on e77b3fc47c45:46699 in memory (size: 7.7 KiB, free: 366.2 MiB)\n",
            "23/04/10 03:17:32 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
            "23/04/10 03:17:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "23/04/10 03:17:32 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "23/04/10 03:17:32 INFO ShuffleBlockFetcherIterator: Getting 1 (72.0 B) non-empty blocks including 1 (72.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/10 03:17:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
            "23/04/10 03:17:32 INFO PythonRunner: Times: total = 56, boot = -312, init = 368, finish = 0\n",
            "23/04/10 03:17:32 INFO FileOutputCommitter: Saved output of task 'attempt_20230410031730423074992305391460_0014_m_000000_0' to file:/content/3_days.out/_temporary/0/task_20230410031730423074992305391460_0014_m_000000\n",
            "23/04/10 03:17:32 INFO SparkHadoopMapRedUtil: attempt_20230410031730423074992305391460_0014_m_000000_0: Committed\n",
            "23/04/10 03:17:32 INFO Executor: Finished task 0.0 in stage 3.0 (TID 5). 1995 bytes result sent to driver\n",
            "23/04/10 03:17:32 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 5) in 302 ms on e77b3fc47c45 (executor driver) (1/1)\n",
            "23/04/10 03:17:32 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
            "23/04/10 03:17:32 INFO DAGScheduler: ResultStage 3 (runJob at SparkHadoopWriter.scala:83) finished in 0.360 s\n",
            "23/04/10 03:17:32 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "23/04/10 03:17:32 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished\n",
            "23/04/10 03:17:32 INFO DAGScheduler: Job 1 finished: runJob at SparkHadoopWriter.scala:83, took 2.084660 s\n",
            "23/04/10 03:17:32 INFO SparkHadoopWriter: Start to commit write Job job_20230410031730423074992305391460_0014.\n",
            "23/04/10 03:17:32 INFO SparkHadoopWriter: Write Job job_20230410031730423074992305391460_0014 committed. Elapsed time: 16 ms.\n",
            "23/04/10 03:17:32 INFO SparkUI: Stopped Spark web UI at http://e77b3fc47c45:4040\n",
            "23/04/10 03:17:32 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "23/04/10 03:17:32 INFO MemoryStore: MemoryStore cleared\n",
            "23/04/10 03:17:32 INFO BlockManager: BlockManager stopped\n",
            "23/04/10 03:17:32 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "23/04/10 03:17:32 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "23/04/10 03:17:32 INFO SparkContext: Successfully stopped SparkContext\n",
            "23/04/10 03:17:33 INFO ShutdownHookManager: Shutdown hook called\n",
            "23/04/10 03:17:33 INFO ShutdownHookManager: Deleting directory /tmp/spark-4c48e978-a241-4af5-bf47-a44f8c9d3b50\n",
            "23/04/10 03:17:33 INFO ShutdownHookManager: Deleting directory /tmp/spark-28323b27-dad2-4bfd-b7a8-960f499d2dc1\n",
            "23/04/10 03:17:33 INFO ShutdownHookManager: Deleting directory /tmp/spark-4c48e978-a241-4af5-bf47-a44f8c9d3b50/pyspark-c52237f5-2714-47ca-b927-a52cee0576a9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test3(lines):\n",
        "    if lines[0].strip() == \"('Sun', 6294)\":\n",
        "        print(\"passed\")\n",
        "    else:\n",
        "        assert False"
      ],
      "metadata": {
        "id": "XPRY68m2ptuN"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2h8y_YU2__D2",
        "outputId": "c5b63347-8ccb-4e6f-8fd4-1e0901ed1dd6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# test locall execution results\n",
        "with open('3_days.out/part-00000') as f:\n",
        "  lines = f.readlines()\n",
        "  test3(lines)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "passed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zdiDy-gu__D3"
      },
      "source": [
        "## Job 4. Join the batting and salaries data for Barry Bonds per year."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%file 4_join.py\n",
        "\n",
        "from pyspark import SparkContext, SparkConf\n",
        "\n",
        "def extract_keys(line):\n",
        "    data = line.split(',')\n",
        "    if data[0] == 'playerID' or len(data) < 2:\n",
        "        return None\n",
        "    elif data[0] == 'bondsba01':\n",
        "        # Batting data\n",
        "        key = (data[0], data[1])\n",
        "        return key, 'B,' + line\n",
        "    else:\n",
        "        # Salaries data\n",
        "        key = (data[3], data[0])\n",
        "        return key, 'S,' + line\n",
        "\n",
        "def join_rows(rows):\n",
        "    batting_rows = [r for r in rows if r.startswith('B,')]\n",
        "    salaries_rows = [r for r in rows if r.startswith('S,')]\n",
        "    if len(batting_rows) == 0 or len(salaries_rows) == 0:\n",
        "        return []\n",
        "    result = []\n",
        "    for bat in batting_rows:\n",
        "        for salary in salaries_rows:\n",
        "            final = bat[2:] + salary[2:]\n",
        "            result.append(final)\n",
        "    return result\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    #conf = SparkConf().setAppName('4_join').set('spark.hadoop.validateOutputSpecs', False)\n",
        "    #sc = SparkContext(conf=conf).getOrCreate()\n",
        "    sc = SparkContext(appName=\"4_join\")\n",
        "\n",
        "\n",
        "    try:\n",
        "        batting = sc.textFile('s3://vandy-bigdata-2/Batting.csv')\n",
        "        salaries = sc.textFile('s3://vandy-bigdata-2/Salaries.csv')\n",
        "\n",
        "        # Extract key-value pairs from both RDDs\n",
        "        batting_mapped = batting.map(extract_keys).filter(lambda x: x is not None)\n",
        "        salaries_mapped = salaries.map(extract_keys).filter(lambda x: x is not None)\n",
        "\n",
        "        # Join the two RDDs on the key (player ID and year)\n",
        "        joined_rdd = batting_mapped.union(salaries_mapped).groupByKey().flatMapValues(join_rows)\n",
        "\n",
        "        # Combine the values into a CSV string\n",
        "        combined_csv = joined_rdd.map(lambda x: ''.join(x[1])).collect()\n",
        "\n",
        "        final = []\n",
        "        for line in combined_csv:\n",
        "          final.append( ('join', line) )\n",
        "        \n",
        "        final_rdd = sc.parallelize(final).repartition(1)\n",
        "        final_rdd.saveAsTextFile(\"s3://vandy-bigdata-2/hw6/4_join.out\")\n",
        "\n",
        "\n",
        "    finally:\n",
        "        sc.stop()"
      ],
      "metadata": {
        "id": "pf7QOOGcXsup",
        "outputId": "3ed16d8b-1645-416a-bc5d-f4eed1ae5dc1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing 4_join.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n5_a0GAy__D3",
        "outputId": "c69164c3-05e0-4009-8abe-9ea802754a9e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# execute locally\n",
        "!spark-submit --packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.4.7 4_join.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ":: loading settings :: url = jar:file:/content/spark-3.2.3-bin-hadoop3.2/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
            "Ivy Default Cache set to: /root/.ivy2/cache\n",
            "The jars for the packages stored in: /root/.ivy2/jars\n",
            "org.apache.spark#spark-streaming-kafka-0-8_2.11 added as a dependency\n",
            ":: resolving dependencies :: org.apache.spark#spark-submit-parent-598842d3-26b2-45d7-aed3-94f34f68e369;1.0\n",
            "\tconfs: [default]\n",
            "\tfound org.apache.spark#spark-streaming-kafka-0-8_2.11;2.4.7 in central\n",
            "\tfound org.apache.kafka#kafka_2.11;0.8.2.1 in central\n",
            "\tfound org.scala-lang.modules#scala-xml_2.11;1.0.2 in central\n",
            "\tfound com.yammer.metrics#metrics-core;2.2.0 in central\n",
            "\tfound org.slf4j#slf4j-api;1.7.16 in central\n",
            "\tfound org.scala-lang.modules#scala-parser-combinators_2.11;1.1.0 in central\n",
            "\tfound com.101tec#zkclient;0.3 in central\n",
            "\tfound log4j#log4j;1.2.17 in central\n",
            "\tfound org.apache.kafka#kafka-clients;0.8.2.1 in central\n",
            "\tfound net.jpountz.lz4#lz4;1.2.0 in central\n",
            "\tfound org.xerial.snappy#snappy-java;1.1.7.5 in central\n",
            "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
            ":: resolution report :: resolve 594ms :: artifacts dl 29ms\n",
            "\t:: modules in use:\n",
            "\tcom.101tec#zkclient;0.3 from central in [default]\n",
            "\tcom.yammer.metrics#metrics-core;2.2.0 from central in [default]\n",
            "\tlog4j#log4j;1.2.17 from central in [default]\n",
            "\tnet.jpountz.lz4#lz4;1.2.0 from central in [default]\n",
            "\torg.apache.kafka#kafka-clients;0.8.2.1 from central in [default]\n",
            "\torg.apache.kafka#kafka_2.11;0.8.2.1 from central in [default]\n",
            "\torg.apache.spark#spark-streaming-kafka-0-8_2.11;2.4.7 from central in [default]\n",
            "\torg.scala-lang.modules#scala-parser-combinators_2.11;1.1.0 from central in [default]\n",
            "\torg.scala-lang.modules#scala-xml_2.11;1.0.2 from central in [default]\n",
            "\torg.slf4j#slf4j-api;1.7.16 from central in [default]\n",
            "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
            "\torg.xerial.snappy#snappy-java;1.1.7.5 from central in [default]\n",
            "\t---------------------------------------------------------------------\n",
            "\t|                  |            modules            ||   artifacts   |\n",
            "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
            "\t---------------------------------------------------------------------\n",
            "\t|      default     |   12  |   0   |   0   |   0   ||   12  |   0   |\n",
            "\t---------------------------------------------------------------------\n",
            ":: retrieving :: org.apache.spark#spark-submit-parent-598842d3-26b2-45d7-aed3-94f34f68e369\n",
            "\tconfs: [default]\n",
            "\t0 artifacts copied, 12 already retrieved (0kB/12ms)\n",
            "23/04/12 03:21:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
            "23/04/12 03:21:14 INFO SparkContext: Running Spark version 3.2.3\n",
            "23/04/12 03:21:14 INFO ResourceUtils: ==============================================================\n",
            "23/04/12 03:21:14 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "23/04/12 03:21:14 INFO ResourceUtils: ==============================================================\n",
            "23/04/12 03:21:14 INFO SparkContext: Submitted application: 4_join\n",
            "23/04/12 03:21:14 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "23/04/12 03:21:14 INFO ResourceProfile: Limiting resource is cpu\n",
            "23/04/12 03:21:14 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "23/04/12 03:21:14 INFO SecurityManager: Changing view acls to: root\n",
            "23/04/12 03:21:14 INFO SecurityManager: Changing modify acls to: root\n",
            "23/04/12 03:21:14 INFO SecurityManager: Changing view acls groups to: \n",
            "23/04/12 03:21:14 INFO SecurityManager: Changing modify acls groups to: \n",
            "23/04/12 03:21:14 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\n",
            "23/04/12 03:21:14 INFO Utils: Successfully started service 'sparkDriver' on port 41073.\n",
            "23/04/12 03:21:14 INFO SparkEnv: Registering MapOutputTracker\n",
            "23/04/12 03:21:14 INFO SparkEnv: Registering BlockManagerMaster\n",
            "23/04/12 03:21:14 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "23/04/12 03:21:14 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "23/04/12 03:21:14 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "23/04/12 03:21:15 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-753eff5b-74f2-499b-bb82-569554b0a563\n",
            "23/04/12 03:21:15 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB\n",
            "23/04/12 03:21:15 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "23/04/12 03:21:15 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
            "23/04/12 03:21:15 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://f4c7b61d3b37:4040\n",
            "23/04/12 03:21:15 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar at spark://f4c7b61d3b37:41073/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1681269674469\n",
            "23/04/12 03:21:15 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar at spark://f4c7b61d3b37:41073/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1681269674469\n",
            "23/04/12 03:21:15 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar at spark://f4c7b61d3b37:41073/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1681269674469\n",
            "23/04/12 03:21:15 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar at spark://f4c7b61d3b37:41073/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1681269674469\n",
            "23/04/12 03:21:15 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar at spark://f4c7b61d3b37:41073/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1681269674469\n",
            "23/04/12 03:21:15 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar at spark://f4c7b61d3b37:41073/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1681269674469\n",
            "23/04/12 03:21:15 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar at spark://f4c7b61d3b37:41073/jars/com.101tec_zkclient-0.3.jar with timestamp 1681269674469\n",
            "23/04/12 03:21:15 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar at spark://f4c7b61d3b37:41073/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1681269674469\n",
            "23/04/12 03:21:15 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar at spark://f4c7b61d3b37:41073/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1681269674469\n",
            "23/04/12 03:21:15 INFO SparkContext: Added JAR file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar at spark://f4c7b61d3b37:41073/jars/log4j_log4j-1.2.17.jar with timestamp 1681269674469\n",
            "23/04/12 03:21:15 INFO SparkContext: Added JAR file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar at spark://f4c7b61d3b37:41073/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1681269674469\n",
            "23/04/12 03:21:15 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar at spark://f4c7b61d3b37:41073/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1681269674469\n",
            "23/04/12 03:21:15 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar at file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1681269674469\n",
            "23/04/12 03:21:15 INFO Utils: Copying /root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar to /tmp/spark-2749c8bf-e99b-445b-a265-a9739115088f/userFiles-3e64d614-08c6-4289-a015-11ebc6712678/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar\n",
            "23/04/12 03:21:15 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar at file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1681269674469\n",
            "23/04/12 03:21:15 INFO Utils: Copying /root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar to /tmp/spark-2749c8bf-e99b-445b-a265-a9739115088f/userFiles-3e64d614-08c6-4289-a015-11ebc6712678/org.apache.kafka_kafka_2.11-0.8.2.1.jar\n",
            "23/04/12 03:21:15 INFO SparkContext: Added file file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar at file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1681269674469\n",
            "23/04/12 03:21:15 INFO Utils: Copying /root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-2749c8bf-e99b-445b-a265-a9739115088f/userFiles-3e64d614-08c6-4289-a015-11ebc6712678/org.spark-project.spark_unused-1.0.0.jar\n",
            "23/04/12 03:21:15 INFO SparkContext: Added file file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar at file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1681269674469\n",
            "23/04/12 03:21:15 INFO Utils: Copying /root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar to /tmp/spark-2749c8bf-e99b-445b-a265-a9739115088f/userFiles-3e64d614-08c6-4289-a015-11ebc6712678/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar\n",
            "23/04/12 03:21:15 INFO SparkContext: Added file file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar at file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1681269674469\n",
            "23/04/12 03:21:15 INFO Utils: Copying /root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar to /tmp/spark-2749c8bf-e99b-445b-a265-a9739115088f/userFiles-3e64d614-08c6-4289-a015-11ebc6712678/com.yammer.metrics_metrics-core-2.2.0.jar\n",
            "23/04/12 03:21:15 INFO SparkContext: Added file file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar at file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1681269674469\n",
            "23/04/12 03:21:15 INFO Utils: Copying /root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar to /tmp/spark-2749c8bf-e99b-445b-a265-a9739115088f/userFiles-3e64d614-08c6-4289-a015-11ebc6712678/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar\n",
            "23/04/12 03:21:15 INFO SparkContext: Added file file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar at file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar with timestamp 1681269674469\n",
            "23/04/12 03:21:15 INFO Utils: Copying /root/.ivy2/jars/com.101tec_zkclient-0.3.jar to /tmp/spark-2749c8bf-e99b-445b-a265-a9739115088f/userFiles-3e64d614-08c6-4289-a015-11ebc6712678/com.101tec_zkclient-0.3.jar\n",
            "23/04/12 03:21:15 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar at file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1681269674469\n",
            "23/04/12 03:21:15 INFO Utils: Copying /root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar to /tmp/spark-2749c8bf-e99b-445b-a265-a9739115088f/userFiles-3e64d614-08c6-4289-a015-11ebc6712678/org.apache.kafka_kafka-clients-0.8.2.1.jar\n",
            "23/04/12 03:21:15 INFO SparkContext: Added file file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar at file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1681269674469\n",
            "23/04/12 03:21:15 INFO Utils: Copying /root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar to /tmp/spark-2749c8bf-e99b-445b-a265-a9739115088f/userFiles-3e64d614-08c6-4289-a015-11ebc6712678/org.slf4j_slf4j-api-1.7.16.jar\n",
            "23/04/12 03:21:15 INFO SparkContext: Added file file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar at file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar with timestamp 1681269674469\n",
            "23/04/12 03:21:15 INFO Utils: Copying /root/.ivy2/jars/log4j_log4j-1.2.17.jar to /tmp/spark-2749c8bf-e99b-445b-a265-a9739115088f/userFiles-3e64d614-08c6-4289-a015-11ebc6712678/log4j_log4j-1.2.17.jar\n",
            "23/04/12 03:21:15 INFO SparkContext: Added file file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar at file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1681269674469\n",
            "23/04/12 03:21:15 INFO Utils: Copying /root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar to /tmp/spark-2749c8bf-e99b-445b-a265-a9739115088f/userFiles-3e64d614-08c6-4289-a015-11ebc6712678/net.jpountz.lz4_lz4-1.2.0.jar\n",
            "23/04/12 03:21:15 INFO SparkContext: Added file file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar at file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1681269674469\n",
            "23/04/12 03:21:15 INFO Utils: Copying /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar to /tmp/spark-2749c8bf-e99b-445b-a265-a9739115088f/userFiles-3e64d614-08c6-4289-a015-11ebc6712678/org.xerial.snappy_snappy-java-1.1.7.5.jar\n",
            "23/04/12 03:21:15 INFO Executor: Starting executor ID driver on host f4c7b61d3b37\n",
            "23/04/12 03:21:15 INFO Executor: Fetching file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar with timestamp 1681269674469\n",
            "23/04/12 03:21:15 INFO Utils: /root/.ivy2/jars/log4j_log4j-1.2.17.jar has been previously copied to /tmp/spark-2749c8bf-e99b-445b-a265-a9739115088f/userFiles-3e64d614-08c6-4289-a015-11ebc6712678/log4j_log4j-1.2.17.jar\n",
            "23/04/12 03:21:15 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1681269674469\n",
            "23/04/12 03:21:15 INFO Utils: /root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar has been previously copied to /tmp/spark-2749c8bf-e99b-445b-a265-a9739115088f/userFiles-3e64d614-08c6-4289-a015-11ebc6712678/org.apache.kafka_kafka-clients-0.8.2.1.jar\n",
            "23/04/12 03:21:15 INFO Executor: Fetching file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1681269674469\n",
            "23/04/12 03:21:15 INFO Utils: /root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar has been previously copied to /tmp/spark-2749c8bf-e99b-445b-a265-a9739115088f/userFiles-3e64d614-08c6-4289-a015-11ebc6712678/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar\n",
            "23/04/12 03:21:15 INFO Executor: Fetching file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1681269674469\n",
            "23/04/12 03:21:15 INFO Utils: /root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar has been previously copied to /tmp/spark-2749c8bf-e99b-445b-a265-a9739115088f/userFiles-3e64d614-08c6-4289-a015-11ebc6712678/org.slf4j_slf4j-api-1.7.16.jar\n",
            "23/04/12 03:21:15 INFO Executor: Fetching file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1681269674469\n",
            "23/04/12 03:21:15 INFO Utils: /root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar has been previously copied to /tmp/spark-2749c8bf-e99b-445b-a265-a9739115088f/userFiles-3e64d614-08c6-4289-a015-11ebc6712678/net.jpountz.lz4_lz4-1.2.0.jar\n",
            "23/04/12 03:21:15 INFO Executor: Fetching file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1681269674469\n",
            "23/04/12 03:21:15 INFO Utils: /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar has been previously copied to /tmp/spark-2749c8bf-e99b-445b-a265-a9739115088f/userFiles-3e64d614-08c6-4289-a015-11ebc6712678/org.xerial.snappy_snappy-java-1.1.7.5.jar\n",
            "23/04/12 03:21:15 INFO Executor: Fetching file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1681269674469\n",
            "23/04/12 03:21:15 INFO Utils: /root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar has been previously copied to /tmp/spark-2749c8bf-e99b-445b-a265-a9739115088f/userFiles-3e64d614-08c6-4289-a015-11ebc6712678/org.spark-project.spark_unused-1.0.0.jar\n",
            "23/04/12 03:21:15 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1681269674469\n",
            "23/04/12 03:21:15 INFO Utils: /root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar has been previously copied to /tmp/spark-2749c8bf-e99b-445b-a265-a9739115088f/userFiles-3e64d614-08c6-4289-a015-11ebc6712678/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar\n",
            "23/04/12 03:21:15 INFO Executor: Fetching file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1681269674469\n",
            "23/04/12 03:21:15 INFO Utils: /root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar has been previously copied to /tmp/spark-2749c8bf-e99b-445b-a265-a9739115088f/userFiles-3e64d614-08c6-4289-a015-11ebc6712678/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar\n",
            "23/04/12 03:21:15 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1681269674469\n",
            "23/04/12 03:21:15 INFO Utils: /root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar has been previously copied to /tmp/spark-2749c8bf-e99b-445b-a265-a9739115088f/userFiles-3e64d614-08c6-4289-a015-11ebc6712678/org.apache.kafka_kafka_2.11-0.8.2.1.jar\n",
            "23/04/12 03:21:15 INFO Executor: Fetching file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar with timestamp 1681269674469\n",
            "23/04/12 03:21:15 INFO Utils: /root/.ivy2/jars/com.101tec_zkclient-0.3.jar has been previously copied to /tmp/spark-2749c8bf-e99b-445b-a265-a9739115088f/userFiles-3e64d614-08c6-4289-a015-11ebc6712678/com.101tec_zkclient-0.3.jar\n",
            "23/04/12 03:21:15 INFO Executor: Fetching file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1681269674469\n",
            "23/04/12 03:21:15 INFO Utils: /root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar has been previously copied to /tmp/spark-2749c8bf-e99b-445b-a265-a9739115088f/userFiles-3e64d614-08c6-4289-a015-11ebc6712678/com.yammer.metrics_metrics-core-2.2.0.jar\n",
            "23/04/12 03:21:15 INFO Executor: Fetching spark://f4c7b61d3b37:41073/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1681269674469\n",
            "23/04/12 03:21:15 INFO TransportClientFactory: Successfully created connection to f4c7b61d3b37/172.28.0.12:41073 after 33 ms (0 ms spent in bootstraps)\n",
            "23/04/12 03:21:15 INFO Utils: Fetching spark://f4c7b61d3b37:41073/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar to /tmp/spark-2749c8bf-e99b-445b-a265-a9739115088f/userFiles-3e64d614-08c6-4289-a015-11ebc6712678/fetchFileTemp1561706985930593559.tmp\n",
            "23/04/12 03:21:16 INFO Utils: /tmp/spark-2749c8bf-e99b-445b-a265-a9739115088f/userFiles-3e64d614-08c6-4289-a015-11ebc6712678/fetchFileTemp1561706985930593559.tmp has been previously copied to /tmp/spark-2749c8bf-e99b-445b-a265-a9739115088f/userFiles-3e64d614-08c6-4289-a015-11ebc6712678/org.xerial.snappy_snappy-java-1.1.7.5.jar\n",
            "23/04/12 03:21:16 INFO Executor: Adding file:/tmp/spark-2749c8bf-e99b-445b-a265-a9739115088f/userFiles-3e64d614-08c6-4289-a015-11ebc6712678/org.xerial.snappy_snappy-java-1.1.7.5.jar to class loader\n",
            "23/04/12 03:21:16 INFO Executor: Fetching spark://f4c7b61d3b37:41073/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1681269674469\n",
            "23/04/12 03:21:16 INFO Utils: Fetching spark://f4c7b61d3b37:41073/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar to /tmp/spark-2749c8bf-e99b-445b-a265-a9739115088f/userFiles-3e64d614-08c6-4289-a015-11ebc6712678/fetchFileTemp195581058183146429.tmp\n",
            "23/04/12 03:21:16 INFO Utils: /tmp/spark-2749c8bf-e99b-445b-a265-a9739115088f/userFiles-3e64d614-08c6-4289-a015-11ebc6712678/fetchFileTemp195581058183146429.tmp has been previously copied to /tmp/spark-2749c8bf-e99b-445b-a265-a9739115088f/userFiles-3e64d614-08c6-4289-a015-11ebc6712678/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar\n",
            "23/04/12 03:21:16 INFO Executor: Adding file:/tmp/spark-2749c8bf-e99b-445b-a265-a9739115088f/userFiles-3e64d614-08c6-4289-a015-11ebc6712678/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar to class loader\n",
            "23/04/12 03:21:16 INFO Executor: Fetching spark://f4c7b61d3b37:41073/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1681269674469\n",
            "23/04/12 03:21:16 INFO Utils: Fetching spark://f4c7b61d3b37:41073/jars/org.slf4j_slf4j-api-1.7.16.jar to /tmp/spark-2749c8bf-e99b-445b-a265-a9739115088f/userFiles-3e64d614-08c6-4289-a015-11ebc6712678/fetchFileTemp7967684192008497788.tmp\n",
            "23/04/12 03:21:16 INFO Utils: /tmp/spark-2749c8bf-e99b-445b-a265-a9739115088f/userFiles-3e64d614-08c6-4289-a015-11ebc6712678/fetchFileTemp7967684192008497788.tmp has been previously copied to /tmp/spark-2749c8bf-e99b-445b-a265-a9739115088f/userFiles-3e64d614-08c6-4289-a015-11ebc6712678/org.slf4j_slf4j-api-1.7.16.jar\n",
            "23/04/12 03:21:16 INFO Executor: Adding file:/tmp/spark-2749c8bf-e99b-445b-a265-a9739115088f/userFiles-3e64d614-08c6-4289-a015-11ebc6712678/org.slf4j_slf4j-api-1.7.16.jar to class loader\n",
            "23/04/12 03:21:16 INFO Executor: Fetching spark://f4c7b61d3b37:41073/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1681269674469\n",
            "23/04/12 03:21:16 INFO Utils: Fetching spark://f4c7b61d3b37:41073/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar to /tmp/spark-2749c8bf-e99b-445b-a265-a9739115088f/userFiles-3e64d614-08c6-4289-a015-11ebc6712678/fetchFileTemp1870736678833999409.tmp\n",
            "23/04/12 03:21:16 INFO Utils: /tmp/spark-2749c8bf-e99b-445b-a265-a9739115088f/userFiles-3e64d614-08c6-4289-a015-11ebc6712678/fetchFileTemp1870736678833999409.tmp has been previously copied to /tmp/spark-2749c8bf-e99b-445b-a265-a9739115088f/userFiles-3e64d614-08c6-4289-a015-11ebc6712678/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar\n",
            "23/04/12 03:21:16 INFO Executor: Adding file:/tmp/spark-2749c8bf-e99b-445b-a265-a9739115088f/userFiles-3e64d614-08c6-4289-a015-11ebc6712678/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar to class loader\n",
            "23/04/12 03:21:16 INFO Executor: Fetching spark://f4c7b61d3b37:41073/jars/log4j_log4j-1.2.17.jar with timestamp 1681269674469\n",
            "23/04/12 03:21:16 INFO Utils: Fetching spark://f4c7b61d3b37:41073/jars/log4j_log4j-1.2.17.jar to /tmp/spark-2749c8bf-e99b-445b-a265-a9739115088f/userFiles-3e64d614-08c6-4289-a015-11ebc6712678/fetchFileTemp8353735727409208563.tmp\n",
            "23/04/12 03:21:16 INFO Utils: /tmp/spark-2749c8bf-e99b-445b-a265-a9739115088f/userFiles-3e64d614-08c6-4289-a015-11ebc6712678/fetchFileTemp8353735727409208563.tmp has been previously copied to /tmp/spark-2749c8bf-e99b-445b-a265-a9739115088f/userFiles-3e64d614-08c6-4289-a015-11ebc6712678/log4j_log4j-1.2.17.jar\n",
            "23/04/12 03:21:16 INFO Executor: Adding file:/tmp/spark-2749c8bf-e99b-445b-a265-a9739115088f/userFiles-3e64d614-08c6-4289-a015-11ebc6712678/log4j_log4j-1.2.17.jar to class loader\n",
            "23/04/12 03:21:16 INFO Executor: Fetching spark://f4c7b61d3b37:41073/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1681269674469\n",
            "23/04/12 03:21:16 INFO Utils: Fetching spark://f4c7b61d3b37:41073/jars/com.yammer.metrics_metrics-core-2.2.0.jar to /tmp/spark-2749c8bf-e99b-445b-a265-a9739115088f/userFiles-3e64d614-08c6-4289-a015-11ebc6712678/fetchFileTemp3340282311704288748.tmp\n",
            "23/04/12 03:21:16 INFO Utils: /tmp/spark-2749c8bf-e99b-445b-a265-a9739115088f/userFiles-3e64d614-08c6-4289-a015-11ebc6712678/fetchFileTemp3340282311704288748.tmp has been previously copied to /tmp/spark-2749c8bf-e99b-445b-a265-a9739115088f/userFiles-3e64d614-08c6-4289-a015-11ebc6712678/com.yammer.metrics_metrics-core-2.2.0.jar\n",
            "23/04/12 03:21:16 INFO Executor: Adding file:/tmp/spark-2749c8bf-e99b-445b-a265-a9739115088f/userFiles-3e64d614-08c6-4289-a015-11ebc6712678/com.yammer.metrics_metrics-core-2.2.0.jar to class loader\n",
            "23/04/12 03:21:16 INFO Executor: Fetching spark://f4c7b61d3b37:41073/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1681269674469\n",
            "23/04/12 03:21:16 INFO Utils: Fetching spark://f4c7b61d3b37:41073/jars/net.jpountz.lz4_lz4-1.2.0.jar to /tmp/spark-2749c8bf-e99b-445b-a265-a9739115088f/userFiles-3e64d614-08c6-4289-a015-11ebc6712678/fetchFileTemp4118306892002921133.tmp\n",
            "23/04/12 03:21:16 INFO Utils: /tmp/spark-2749c8bf-e99b-445b-a265-a9739115088f/userFiles-3e64d614-08c6-4289-a015-11ebc6712678/fetchFileTemp4118306892002921133.tmp has been previously copied to /tmp/spark-2749c8bf-e99b-445b-a265-a9739115088f/userFiles-3e64d614-08c6-4289-a015-11ebc6712678/net.jpountz.lz4_lz4-1.2.0.jar\n",
            "23/04/12 03:21:16 INFO Executor: Adding file:/tmp/spark-2749c8bf-e99b-445b-a265-a9739115088f/userFiles-3e64d614-08c6-4289-a015-11ebc6712678/net.jpountz.lz4_lz4-1.2.0.jar to class loader\n",
            "23/04/12 03:21:16 INFO Executor: Fetching spark://f4c7b61d3b37:41073/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1681269674469\n",
            "23/04/12 03:21:16 INFO Utils: Fetching spark://f4c7b61d3b37:41073/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-2749c8bf-e99b-445b-a265-a9739115088f/userFiles-3e64d614-08c6-4289-a015-11ebc6712678/fetchFileTemp6429611600249535555.tmp\n",
            "23/04/12 03:21:16 INFO Utils: /tmp/spark-2749c8bf-e99b-445b-a265-a9739115088f/userFiles-3e64d614-08c6-4289-a015-11ebc6712678/fetchFileTemp6429611600249535555.tmp has been previously copied to /tmp/spark-2749c8bf-e99b-445b-a265-a9739115088f/userFiles-3e64d614-08c6-4289-a015-11ebc6712678/org.spark-project.spark_unused-1.0.0.jar\n",
            "23/04/12 03:21:16 INFO Executor: Adding file:/tmp/spark-2749c8bf-e99b-445b-a265-a9739115088f/userFiles-3e64d614-08c6-4289-a015-11ebc6712678/org.spark-project.spark_unused-1.0.0.jar to class loader\n",
            "23/04/12 03:21:16 INFO Executor: Fetching spark://f4c7b61d3b37:41073/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1681269674469\n",
            "23/04/12 03:21:16 INFO Utils: Fetching spark://f4c7b61d3b37:41073/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar to /tmp/spark-2749c8bf-e99b-445b-a265-a9739115088f/userFiles-3e64d614-08c6-4289-a015-11ebc6712678/fetchFileTemp6755215421403541309.tmp\n",
            "23/04/12 03:21:16 INFO Utils: /tmp/spark-2749c8bf-e99b-445b-a265-a9739115088f/userFiles-3e64d614-08c6-4289-a015-11ebc6712678/fetchFileTemp6755215421403541309.tmp has been previously copied to /tmp/spark-2749c8bf-e99b-445b-a265-a9739115088f/userFiles-3e64d614-08c6-4289-a015-11ebc6712678/org.apache.kafka_kafka_2.11-0.8.2.1.jar\n",
            "23/04/12 03:21:16 INFO Executor: Adding file:/tmp/spark-2749c8bf-e99b-445b-a265-a9739115088f/userFiles-3e64d614-08c6-4289-a015-11ebc6712678/org.apache.kafka_kafka_2.11-0.8.2.1.jar to class loader\n",
            "23/04/12 03:21:16 INFO Executor: Fetching spark://f4c7b61d3b37:41073/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1681269674469\n",
            "23/04/12 03:21:16 INFO Utils: Fetching spark://f4c7b61d3b37:41073/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar to /tmp/spark-2749c8bf-e99b-445b-a265-a9739115088f/userFiles-3e64d614-08c6-4289-a015-11ebc6712678/fetchFileTemp2586582865511217388.tmp\n",
            "23/04/12 03:21:16 INFO Utils: /tmp/spark-2749c8bf-e99b-445b-a265-a9739115088f/userFiles-3e64d614-08c6-4289-a015-11ebc6712678/fetchFileTemp2586582865511217388.tmp has been previously copied to /tmp/spark-2749c8bf-e99b-445b-a265-a9739115088f/userFiles-3e64d614-08c6-4289-a015-11ebc6712678/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar\n",
            "23/04/12 03:21:16 INFO Executor: Adding file:/tmp/spark-2749c8bf-e99b-445b-a265-a9739115088f/userFiles-3e64d614-08c6-4289-a015-11ebc6712678/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar to class loader\n",
            "23/04/12 03:21:16 INFO Executor: Fetching spark://f4c7b61d3b37:41073/jars/com.101tec_zkclient-0.3.jar with timestamp 1681269674469\n",
            "23/04/12 03:21:16 INFO Utils: Fetching spark://f4c7b61d3b37:41073/jars/com.101tec_zkclient-0.3.jar to /tmp/spark-2749c8bf-e99b-445b-a265-a9739115088f/userFiles-3e64d614-08c6-4289-a015-11ebc6712678/fetchFileTemp8156655626584284378.tmp\n",
            "23/04/12 03:21:16 INFO Utils: /tmp/spark-2749c8bf-e99b-445b-a265-a9739115088f/userFiles-3e64d614-08c6-4289-a015-11ebc6712678/fetchFileTemp8156655626584284378.tmp has been previously copied to /tmp/spark-2749c8bf-e99b-445b-a265-a9739115088f/userFiles-3e64d614-08c6-4289-a015-11ebc6712678/com.101tec_zkclient-0.3.jar\n",
            "23/04/12 03:21:16 INFO Executor: Adding file:/tmp/spark-2749c8bf-e99b-445b-a265-a9739115088f/userFiles-3e64d614-08c6-4289-a015-11ebc6712678/com.101tec_zkclient-0.3.jar to class loader\n",
            "23/04/12 03:21:16 INFO Executor: Fetching spark://f4c7b61d3b37:41073/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1681269674469\n",
            "23/04/12 03:21:16 INFO Utils: Fetching spark://f4c7b61d3b37:41073/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar to /tmp/spark-2749c8bf-e99b-445b-a265-a9739115088f/userFiles-3e64d614-08c6-4289-a015-11ebc6712678/fetchFileTemp3406291036439387038.tmp\n",
            "23/04/12 03:21:16 INFO Utils: /tmp/spark-2749c8bf-e99b-445b-a265-a9739115088f/userFiles-3e64d614-08c6-4289-a015-11ebc6712678/fetchFileTemp3406291036439387038.tmp has been previously copied to /tmp/spark-2749c8bf-e99b-445b-a265-a9739115088f/userFiles-3e64d614-08c6-4289-a015-11ebc6712678/org.apache.kafka_kafka-clients-0.8.2.1.jar\n",
            "23/04/12 03:21:16 INFO Executor: Adding file:/tmp/spark-2749c8bf-e99b-445b-a265-a9739115088f/userFiles-3e64d614-08c6-4289-a015-11ebc6712678/org.apache.kafka_kafka-clients-0.8.2.1.jar to class loader\n",
            "23/04/12 03:21:16 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39635.\n",
            "23/04/12 03:21:16 INFO NettyBlockTransferService: Server created on f4c7b61d3b37:39635\n",
            "23/04/12 03:21:16 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "23/04/12 03:21:16 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, f4c7b61d3b37, 39635, None)\n",
            "23/04/12 03:21:16 INFO BlockManagerMasterEndpoint: Registering block manager f4c7b61d3b37:39635 with 366.3 MiB RAM, BlockManagerId(driver, f4c7b61d3b37, 39635, None)\n",
            "23/04/12 03:21:16 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, f4c7b61d3b37, 39635, None)\n",
            "23/04/12 03:21:16 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, f4c7b61d3b37, 39635, None)\n",
            "23/04/12 03:21:16 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 354.3 KiB, free 366.0 MiB)\n",
            "23/04/12 03:21:16 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 32.0 KiB, free 365.9 MiB)\n",
            "23/04/12 03:21:16 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on f4c7b61d3b37:39635 (size: 32.0 KiB, free: 366.3 MiB)\n",
            "23/04/12 03:21:16 INFO SparkContext: Created broadcast 0 from textFile at NativeMethodAccessorImpl.java:0\n",
            "23/04/12 03:21:17 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 354.3 KiB, free 365.6 MiB)\n",
            "23/04/12 03:21:17 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 32.0 KiB, free 365.5 MiB)\n",
            "23/04/12 03:21:17 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on f4c7b61d3b37:39635 (size: 32.0 KiB, free: 366.2 MiB)\n",
            "23/04/12 03:21:17 INFO SparkContext: Created broadcast 1 from textFile at NativeMethodAccessorImpl.java:0\n",
            "23/04/12 03:21:17 INFO FileInputFormat: Total input files to process : 1\n",
            "23/04/12 03:21:17 INFO FileInputFormat: Total input files to process : 1\n",
            "23/04/12 03:21:17 INFO SparkContext: Starting job: collect at /content/4_join.py:46\n",
            "23/04/12 03:21:17 INFO DAGScheduler: Registering RDD 8 (groupByKey at /content/4_join.py:43) as input to shuffle 0\n",
            "23/04/12 03:21:17 INFO DAGScheduler: Got job 0 (collect at /content/4_join.py:46) with 4 output partitions\n",
            "23/04/12 03:21:17 INFO DAGScheduler: Final stage: ResultStage 1 (collect at /content/4_join.py:46)\n",
            "23/04/12 03:21:17 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)\n",
            "23/04/12 03:21:17 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 0)\n",
            "23/04/12 03:21:17 INFO DAGScheduler: Submitting ShuffleMapStage 0 (PairwiseRDD[8] at groupByKey at /content/4_join.py:43), which has no missing parents\n",
            "23/04/12 03:21:17 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 17.6 KiB, free 365.5 MiB)\n",
            "23/04/12 03:21:17 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 8.3 KiB, free 365.5 MiB)\n",
            "23/04/12 03:21:17 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on f4c7b61d3b37:39635 (size: 8.3 KiB, free: 366.2 MiB)\n",
            "23/04/12 03:21:17 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1478\n",
            "23/04/12 03:21:17 INFO DAGScheduler: Submitting 4 missing tasks from ShuffleMapStage 0 (PairwiseRDD[8] at groupByKey at /content/4_join.py:43) (first 15 tasks are for partitions Vector(0, 1, 2, 3))\n",
            "23/04/12 03:21:17 INFO TaskSchedulerImpl: Adding task set 0.0 with 4 tasks resource profile 0\n",
            "23/04/12 03:21:17 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (f4c7b61d3b37, executor driver, partition 0, PROCESS_LOCAL, 4592 bytes) taskResourceAssignments Map()\n",
            "23/04/12 03:21:17 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1) (f4c7b61d3b37, executor driver, partition 1, PROCESS_LOCAL, 4592 bytes) taskResourceAssignments Map()\n",
            "23/04/12 03:21:17 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)\n",
            "23/04/12 03:21:17 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
            "23/04/12 03:21:18 INFO HadoopRDD: Input split: file:/content/Batting.csv:3244373+3244374\n",
            "23/04/12 03:21:18 INFO HadoopRDD: Input split: file:/content/Batting.csv:0+3244373\n",
            "23/04/12 03:21:19 INFO PythonRunner: Times: total = 949, boot = 422, init = 37, finish = 490\n",
            "23/04/12 03:21:19 INFO PythonRunner: Times: total = 957, boot = 430, init = 45, finish = 482\n",
            "23/04/12 03:21:19 INFO PythonRunner: Times: total = 1074, boot = 4, init = 76, finish = 994\n",
            "23/04/12 03:21:19 INFO PythonRunner: Times: total = 1094, boot = 17, init = 39, finish = 1038\n",
            "23/04/12 03:21:19 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1657 bytes result sent to driver\n",
            "23/04/12 03:21:19 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 1614 bytes result sent to driver\n",
            "23/04/12 03:21:19 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2) (f4c7b61d3b37, executor driver, partition 2, PROCESS_LOCAL, 4593 bytes) taskResourceAssignments Map()\n",
            "23/04/12 03:21:19 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)\n",
            "23/04/12 03:21:19 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 2085 ms on f4c7b61d3b37 (executor driver) (1/4)\n",
            "23/04/12 03:21:19 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3) (f4c7b61d3b37, executor driver, partition 3, PROCESS_LOCAL, 4593 bytes) taskResourceAssignments Map()\n",
            "23/04/12 03:21:19 INFO Executor: Running task 3.0 in stage 0.0 (TID 3)\n",
            "23/04/12 03:21:19 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 2077 ms on f4c7b61d3b37 (executor driver) (2/4)\n",
            "23/04/12 03:21:19 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 54713\n",
            "23/04/12 03:21:19 INFO HadoopRDD: Input split: file:/content/Salaries.csv:350012+350012\n",
            "23/04/12 03:21:19 INFO HadoopRDD: Input split: file:/content/Salaries.csv:0+350012\n",
            "23/04/12 03:21:20 INFO PythonRunner: Times: total = 125, boot = -686, init = 695, finish = 116\n",
            "23/04/12 03:21:20 INFO PythonRunner: Times: total = 151, boot = -686, init = 721, finish = 116\n",
            "23/04/12 03:21:20 INFO PythonRunner: Times: total = 286, boot = -142, init = 178, finish = 250\n",
            "23/04/12 03:21:20 INFO Executor: Finished task 3.0 in stage 0.0 (TID 3). 1571 bytes result sent to driver\n",
            "23/04/12 03:21:20 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 358 ms on f4c7b61d3b37 (executor driver) (3/4)\n",
            "23/04/12 03:21:20 INFO PythonRunner: Times: total = 340, boot = -112, init = 169, finish = 283\n",
            "23/04/12 03:21:20 INFO Executor: Finished task 2.0 in stage 0.0 (TID 2). 1571 bytes result sent to driver\n",
            "23/04/12 03:21:20 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 440 ms on f4c7b61d3b37 (executor driver) (4/4)\n",
            "23/04/12 03:21:20 INFO DAGScheduler: ShuffleMapStage 0 (groupByKey at /content/4_join.py:43) finished in 2.742 s\n",
            "23/04/12 03:21:20 INFO DAGScheduler: looking for newly runnable stages\n",
            "23/04/12 03:21:20 INFO DAGScheduler: running: Set()\n",
            "23/04/12 03:21:20 INFO DAGScheduler: waiting: Set(ResultStage 1)\n",
            "23/04/12 03:21:20 INFO DAGScheduler: failed: Set()\n",
            "23/04/12 03:21:20 INFO DAGScheduler: Submitting ResultStage 1 (PythonRDD[11] at collect at /content/4_join.py:46), which has no missing parents\n",
            "23/04/12 03:21:20 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
            "23/04/12 03:21:20 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 12.3 KiB, free 365.5 MiB)\n",
            "23/04/12 03:21:20 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 7.1 KiB, free 365.5 MiB)\n",
            "23/04/12 03:21:20 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on f4c7b61d3b37:39635 (size: 7.1 KiB, free: 366.2 MiB)\n",
            "23/04/12 03:21:20 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1478\n",
            "23/04/12 03:21:20 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 1 (PythonRDD[11] at collect at /content/4_join.py:46) (first 15 tasks are for partitions Vector(0, 1, 2, 3))\n",
            "23/04/12 03:21:20 INFO TaskSchedulerImpl: Adding task set 1.0 with 4 tasks resource profile 0\n",
            "23/04/12 03:21:20 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 4) (f4c7b61d3b37, executor driver, partition 0, NODE_LOCAL, 4271 bytes) taskResourceAssignments Map()\n",
            "23/04/12 03:21:20 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 5) (f4c7b61d3b37, executor driver, partition 1, NODE_LOCAL, 4271 bytes) taskResourceAssignments Map()\n",
            "23/04/12 03:21:20 INFO Executor: Running task 0.0 in stage 1.0 (TID 4)\n",
            "23/04/12 03:21:20 INFO Executor: Running task 1.0 in stage 1.0 (TID 5)\n",
            "23/04/12 03:21:20 INFO ShuffleBlockFetcherIterator: Getting 4 (1231.1 KiB) non-empty blocks including 4 (1231.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/12 03:21:20 INFO ShuffleBlockFetcherIterator: Getting 4 (1231.1 KiB) non-empty blocks including 4 (1231.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/12 03:21:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 19 ms\n",
            "23/04/12 03:21:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 23 ms\n",
            "23/04/12 03:21:20 INFO PythonRunner: Times: total = 108, boot = -370, init = 379, finish = 99\n",
            "23/04/12 03:21:20 INFO Executor: Finished task 0.0 in stage 1.0 (TID 4). 2124 bytes result sent to driver\n",
            "23/04/12 03:21:20 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 6) (f4c7b61d3b37, executor driver, partition 2, NODE_LOCAL, 4271 bytes) taskResourceAssignments Map()\n",
            "23/04/12 03:21:20 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 4) in 215 ms on f4c7b61d3b37 (executor driver) (1/4)\n",
            "23/04/12 03:21:20 INFO Executor: Running task 2.0 in stage 1.0 (TID 6)\n",
            "23/04/12 03:21:20 INFO ShuffleBlockFetcherIterator: Getting 4 (1231.1 KiB) non-empty blocks including 4 (1231.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/12 03:21:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 7 ms\n",
            "23/04/12 03:21:20 INFO PythonRunner: Times: total = 151, boot = -385, init = 393, finish = 143\n",
            "23/04/12 03:21:20 INFO Executor: Finished task 1.0 in stage 1.0 (TID 5). 2697 bytes result sent to driver\n",
            "23/04/12 03:21:20 INFO TaskSetManager: Starting task 3.0 in stage 1.0 (TID 7) (f4c7b61d3b37, executor driver, partition 3, NODE_LOCAL, 4271 bytes) taskResourceAssignments Map()\n",
            "23/04/12 03:21:20 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 5) in 269 ms on f4c7b61d3b37 (executor driver) (2/4)\n",
            "23/04/12 03:21:20 INFO Executor: Running task 3.0 in stage 1.0 (TID 7)\n",
            "23/04/12 03:21:20 INFO ShuffleBlockFetcherIterator: Getting 4 (1180.8 KiB) non-empty blocks including 4 (1180.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/12 03:21:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 6 ms\n",
            "23/04/12 03:21:20 INFO PythonRunner: Times: total = 98, boot = -355, init = 364, finish = 89\n",
            "23/04/12 03:21:20 INFO Executor: Finished task 2.0 in stage 1.0 (TID 6). 2231 bytes result sent to driver\n",
            "23/04/12 03:21:20 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 6) in 133 ms on f4c7b61d3b37 (executor driver) (3/4)\n",
            "23/04/12 03:21:20 INFO PythonRunner: Times: total = 87, boot = -369, init = 372, finish = 84\n",
            "23/04/12 03:21:20 INFO Executor: Finished task 3.0 in stage 1.0 (TID 7). 2154 bytes result sent to driver\n",
            "23/04/12 03:21:20 INFO TaskSetManager: Finished task 3.0 in stage 1.0 (TID 7) in 135 ms on f4c7b61d3b37 (executor driver) (4/4)\n",
            "23/04/12 03:21:20 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
            "23/04/12 03:21:20 INFO DAGScheduler: ResultStage 1 (collect at /content/4_join.py:46) finished in 0.426 s\n",
            "23/04/12 03:21:20 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "23/04/12 03:21:20 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n",
            "23/04/12 03:21:20 INFO DAGScheduler: Job 0 finished: collect at /content/4_join.py:46, took 3.310107 s\n",
            "23/04/12 03:21:20 INFO deprecation: mapred.output.dir is deprecated. Instead, use mapreduce.output.fileoutputformat.outputdir\n",
            "23/04/12 03:21:20 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
            "23/04/12 03:21:20 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "23/04/12 03:21:20 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "23/04/12 03:21:20 INFO SparkContext: Starting job: runJob at SparkHadoopWriter.scala:83\n",
            "23/04/12 03:21:20 INFO DAGScheduler: Registering RDD 14 (coalesce at NativeMethodAccessorImpl.java:0) as input to shuffle 1\n",
            "23/04/12 03:21:20 INFO DAGScheduler: Got job 1 (runJob at SparkHadoopWriter.scala:83) with 1 output partitions\n",
            "23/04/12 03:21:20 INFO DAGScheduler: Final stage: ResultStage 3 (runJob at SparkHadoopWriter.scala:83)\n",
            "23/04/12 03:21:20 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)\n",
            "23/04/12 03:21:20 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 2)\n",
            "23/04/12 03:21:21 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[14] at coalesce at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/04/12 03:21:21 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 7.8 KiB, free 365.5 MiB)\n",
            "23/04/12 03:21:21 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 4.8 KiB, free 365.5 MiB)\n",
            "23/04/12 03:21:21 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on f4c7b61d3b37:39635 (size: 4.8 KiB, free: 366.2 MiB)\n",
            "23/04/12 03:21:21 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1478\n",
            "23/04/12 03:21:21 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[14] at coalesce at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
            "23/04/12 03:21:21 INFO TaskSchedulerImpl: Adding task set 2.0 with 2 tasks resource profile 0\n",
            "23/04/12 03:21:21 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 8) (f4c7b61d3b37, executor driver, partition 0, PROCESS_LOCAL, 5749 bytes) taskResourceAssignments Map()\n",
            "23/04/12 03:21:21 INFO TaskSetManager: Starting task 1.0 in stage 2.0 (TID 9) (f4c7b61d3b37, executor driver, partition 1, PROCESS_LOCAL, 5722 bytes) taskResourceAssignments Map()\n",
            "23/04/12 03:21:21 INFO Executor: Running task 0.0 in stage 2.0 (TID 8)\n",
            "23/04/12 03:21:21 INFO Executor: Running task 1.0 in stage 2.0 (TID 9)\n",
            "23/04/12 03:21:21 INFO PythonRunner: Times: total = 7, boot = -454, init = 461, finish = 0\n",
            "23/04/12 03:21:21 INFO Executor: Finished task 1.0 in stage 2.0 (TID 9). 1482 bytes result sent to driver\n",
            "23/04/12 03:21:21 INFO PythonRunner: Times: total = 3, boot = -466, init = 469, finish = 0\n",
            "23/04/12 03:21:21 INFO Executor: Finished task 0.0 in stage 2.0 (TID 8). 1482 bytes result sent to driver\n",
            "23/04/12 03:21:21 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 8) in 85 ms on f4c7b61d3b37 (executor driver) (1/2)\n",
            "23/04/12 03:21:21 INFO TaskSetManager: Finished task 1.0 in stage 2.0 (TID 9) in 81 ms on f4c7b61d3b37 (executor driver) (2/2)\n",
            "23/04/12 03:21:21 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
            "23/04/12 03:21:21 INFO DAGScheduler: ShuffleMapStage 2 (coalesce at NativeMethodAccessorImpl.java:0) finished in 0.110 s\n",
            "23/04/12 03:21:21 INFO DAGScheduler: looking for newly runnable stages\n",
            "23/04/12 03:21:21 INFO DAGScheduler: running: Set()\n",
            "23/04/12 03:21:21 INFO DAGScheduler: waiting: Set(ResultStage 3)\n",
            "23/04/12 03:21:21 INFO DAGScheduler: failed: Set()\n",
            "23/04/12 03:21:21 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[20] at saveAsTextFile at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/04/12 03:21:21 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 105.9 KiB, free 365.4 MiB)\n",
            "23/04/12 03:21:21 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 39.6 KiB, free 365.3 MiB)\n",
            "23/04/12 03:21:21 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on f4c7b61d3b37:39635 (size: 39.6 KiB, free: 366.2 MiB)\n",
            "23/04/12 03:21:21 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1478\n",
            "23/04/12 03:21:21 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[20] at saveAsTextFile at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/04/12 03:21:21 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0\n",
            "23/04/12 03:21:21 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 10) (f4c7b61d3b37, executor driver, partition 0, NODE_LOCAL, 4547 bytes) taskResourceAssignments Map()\n",
            "23/04/12 03:21:21 INFO Executor: Running task 0.0 in stage 3.0 (TID 10)\n",
            "23/04/12 03:21:21 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
            "23/04/12 03:21:21 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "23/04/12 03:21:21 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "23/04/12 03:21:21 INFO ShuffleBlockFetcherIterator: Getting 2 (1910.0 B) non-empty blocks including 2 (1910.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/12 03:21:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms\n",
            "23/04/12 03:21:21 INFO PythonRunner: Times: total = 36, boot = -627, init = 663, finish = 0\n",
            "23/04/12 03:21:21 INFO FileOutputCommitter: Saved output of task 'attempt_202304120321205482930662298370719_0020_m_000000_0' to file:/content/4_join.out/_temporary/0/task_202304120321205482930662298370719_0020_m_000000\n",
            "23/04/12 03:21:21 INFO SparkHadoopMapRedUtil: attempt_202304120321205482930662298370719_0020_m_000000_0: Committed\n",
            "23/04/12 03:21:21 INFO Executor: Finished task 0.0 in stage 3.0 (TID 10). 1952 bytes result sent to driver\n",
            "23/04/12 03:21:21 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 10) in 318 ms on f4c7b61d3b37 (executor driver) (1/1)\n",
            "23/04/12 03:21:21 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
            "23/04/12 03:21:21 INFO DAGScheduler: ResultStage 3 (runJob at SparkHadoopWriter.scala:83) finished in 0.375 s\n",
            "23/04/12 03:21:21 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "23/04/12 03:21:21 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished\n",
            "23/04/12 03:21:21 INFO DAGScheduler: Job 1 finished: runJob at SparkHadoopWriter.scala:83, took 0.513066 s\n",
            "23/04/12 03:21:21 INFO SparkHadoopWriter: Start to commit write Job job_202304120321205482930662298370719_0020.\n",
            "23/04/12 03:21:21 INFO SparkHadoopWriter: Write Job job_202304120321205482930662298370719_0020 committed. Elapsed time: 38 ms.\n",
            "23/04/12 03:21:21 INFO SparkUI: Stopped Spark web UI at http://f4c7b61d3b37:4040\n",
            "23/04/12 03:21:21 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "23/04/12 03:21:21 INFO MemoryStore: MemoryStore cleared\n",
            "23/04/12 03:21:21 INFO BlockManager: BlockManager stopped\n",
            "23/04/12 03:21:21 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "23/04/12 03:21:21 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "23/04/12 03:21:21 INFO SparkContext: Successfully stopped SparkContext\n",
            "23/04/12 03:21:22 INFO ShutdownHookManager: Shutdown hook called\n",
            "23/04/12 03:21:22 INFO ShutdownHookManager: Deleting directory /tmp/spark-2749c8bf-e99b-445b-a265-a9739115088f/pyspark-26cb3468-a830-49e7-be24-e74e00369316\n",
            "23/04/12 03:21:22 INFO ShutdownHookManager: Deleting directory /tmp/spark-2749c8bf-e99b-445b-a265-a9739115088f\n",
            "23/04/12 03:21:22 INFO ShutdownHookManager: Deleting directory /tmp/spark-7dc9a8df-044c-439c-b530-cccdd85783de\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test locally\n",
        "results = [('join', 'bondsba01,1991,1,PIT,NL,153,153,510,95,149,28,5,25,116,43,13,107,73,25,4,0,13,8,1531991,PIT,NL,bondsba01,2300000'),\n",
        "('join', 'bondsba01,1993,1,SFN,NL,159,159,539,129,181,38,4,46,123,29,12,126,79,43,2,0,7,11,1591993,SFN,NL,bondsba01,4516666'),\n",
        "('join', 'bondsba01,2002,1,SFN,NL,143,143,403,117,149,31,2,46,110,9,2,198,47,68,9,0,2,4,1432002,SFN,NL,bondsba01,15000000'),\n",
        "('join', 'bondsba01,2004,1,SFN,NL,147,147,373,129,135,27,3,45,101,6,1,232,41,120,9,0,3,5,1472004,SFN,NL,bondsba01,18000000'),\n",
        "('join', 'bondsba01,1986,1,PIT,NL,113,113,413,72,92,26,3,16,48,36,7,65,102,2,2,2,2,4,1131986,PIT,NL,bondsba01,60000'),\n",
        "('join', 'bondsba01,1996,1,SFN,NL,158,158,517,122,159,27,3,42,129,40,7,151,76,30,1,0,6,11,1581996,SFN,NL,bondsba01,8416667'),\n",
        "('join', 'bondsba01,1997,1,SFN,NL,159,159,532,123,155,26,5,40,101,37,8,145,87,34,8,0,5,13,1591997,SFN,NL,bondsba01,8666667'),\n",
        "('join', 'bondsba01,1999,1,SFN,NL,102,102,355,91,93,20,2,34,83,15,2,73,62,9,3,0,3,6,1021999,SFN,NL,bondsba01,9381057'),\n",
        "('join', 'bondsba01,1990,1,PIT,NL,151,151,519,104,156,32,3,33,114,52,13,93,83,15,3,0,6,8,1511990,PIT,NL,bondsba01,850000'),\n",
        "('join', 'bondsba01,1994,1,SFN,NL,112,112,391,89,122,18,1,37,81,29,9,74,43,18,6,0,3,3,1121994,SFN,NL,bondsba01,5166666'),\n",
        "('join', 'bondsba01,1995,1,SFN,NL,144,144,506,109,149,30,7,33,104,31,10,120,83,22,5,0,4,12,1441995,SFN,NL,bondsba01,8166666'),\n",
        "('join', 'bondsba01,2003,1,SFN,NL,130,130,390,111,133,22,1,45,90,7,0,148,58,61,10,0,2,7,1302003,SFN,NL,bondsba01,15500000'),\n",
        "('join', 'bondsba01,2007,1,SFN,NL,126,126,340,75,94,14,0,28,66,5,0,132,54,43,3,0,2,13,1262007,SFN,NL,bondsba01,15533970'),\n",
        "('join', 'bondsba01,1987,1,PIT,NL,150,150,551,99,144,34,9,25,59,32,10,54,88,3,3,0,3,4,1501987,PIT,NL,bondsba01,100000'),\n",
        "('join', 'bondsba01,1988,1,PIT,NL,144,144,538,97,152,30,5,24,58,17,11,72,82,14,2,0,2,3,1441988,PIT,NL,bondsba01,220000'),\n",
        "('join', 'bondsba01,1989,1,PIT,NL,159,159,580,96,144,34,6,19,58,32,10,93,93,22,1,1,4,9,1591989,PIT,NL,bondsba01,360000'),\n",
        "('join', 'bondsba01,1992,1,PIT,NL,140,140,473,109,147,36,5,34,103,39,8,127,69,32,5,0,7,9,1401992,PIT,NL,bondsba01,4800000'),\n",
        "('join', 'bondsba01,1998,1,SFN,NL,156,156,552,120,167,44,7,37,122,28,12,130,92,29,8,1,6,15,1561998,SFN,NL,bondsba01,8916667'),\n",
        "('join', 'bondsba01,2000,1,SFN,NL,143,143,480,129,147,28,4,49,106,11,3,117,77,22,3,0,7,6,1432000,SFN,NL,bondsba01,10658826'),\n",
        "('join', 'bondsba01,2001,1,SFN,NL,153,153,476,129,156,32,2,73,137,13,3,177,93,35,9,0,2,5,1532001,SFN,NL,bondsba01,10300000'),\n",
        "('join', 'bondsba01,2005,1,SFN,NL,14,14,42,8,12,1,0,5,10,0,0,9,6,3,0,0,1,0,142005,SFN,NL,bondsba01,22000000'),\n",
        "('join', 'bondsba01,2006,1,SFN,NL,130,130,367,74,99,23,0,26,77,3,0,115,51,38,10,0,1,9,1302006,SFN,NL,bondsba01,19331470')]\n",
        "def test4(lines):\n",
        "  global results\n",
        "  results = [str(x) for x in results]\n",
        "  find_lines = 0\n",
        "  for  line in lines:\n",
        "    if line.strip() in results:\n",
        "        find_lines += 1\n",
        "  if find_lines != 22:\n",
        "      assert False\n",
        "  print('test passed')"
      ],
      "metadata": {
        "id": "DrHOt9VOsRne"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XpkDNQCb__D3",
        "outputId": "c391b290-68b1-46c4-eae2-f44f5dc99580",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "with open('4_join.out/part-00000') as f:\n",
        "    lines = f.readlines()\n",
        "    test4(lines)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test passed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3 Execute the same scripts on EMR.\n",
        "\n",
        "* Make sure that you have created an EMR cluster using the instructions in the main readme.\n",
        "* upload the main data to s3"
      ],
      "metadata": {
        "id": "Ed2gw0o387bf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup AWS Credentials and fill them here. Make sure you do not save this information back to github"
      ],
      "metadata": {
        "id": "dyeTsQLV-B-u"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bMEsJXTU__Dx"
      },
      "source": [
        "# Please fill your aws credential information here\n",
        "credentials = {\n",
        "    'region_name': 'us-east-1',\n",
        "    'aws_access_key_id': 'ASIA5D6DSQAUH2Y53ITH',\n",
        "    'aws_secret_access_key': 'hIdLVHrNBjXfFtS6Id49gu7Vr8DlrSAok/j0fNi5',\n",
        "    'aws_session_token': 'FwoGZXIvYXdzEO///////////wEaDDJl6Ua5q+p/XJh5rSLKAR4vU6L+OAtIV0gQdjxJ9phA8etJdhD8Bn9Ze8HQvtxYathGMp6Yan7viYKYa2Gtdc8l01iy3+nozLEBPTuep705KNQmccS1jKo80HcriJe0vOB6SuFpuCtouMfTz0mQKZVC6gsm/jWiXp0YO8+w6DXLCikCKyiFlf2znrIrh7mefFjt3MXv0C7/wbZO4XhuwJfZJW5XamLZBI2losoiwsG0LUqdyfDgS1315aKNnxTFzNyBTn2AuZwqFcM5+fTJqoIVpE1RRj13LQIo2tHcoQYyLcyHRWD6ZXlBGzweTzFO/8fo41jMkz4/fJIM2BMMxB2qZCb/moAQZlsT4wPsHA=='\n",
        "}"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yRJjVwtO__Dx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54841394-653a-45c1-c839-d8e7f2f49c1d"
      },
      "source": [
        "!pip install boto3\n",
        "import boto3, json\n",
        "\n",
        "session = boto3.session.Session(**credentials)\n",
        "s3 = session.client('s3')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting boto3\n",
            "  Downloading boto3-1.26.112-py3-none-any.whl (135 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.6/135.6 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting botocore<1.30.0,>=1.29.112\n",
            "  Downloading botocore-1.29.112-py3-none-any.whl (10.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.6/10.6 MB\u001b[0m \u001b[31m77.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting s3transfer<0.7.0,>=0.6.0\n",
            "  Downloading s3transfer-0.6.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.6/79.6 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.9/dist-packages (from botocore<1.30.0,>=1.29.112->boto3) (2.8.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /usr/local/lib/python3.9/dist-packages (from botocore<1.30.0,>=1.29.112->boto3) (1.26.15)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.30.0,>=1.29.112->boto3) (1.16.0)\n",
            "Installing collected packages: jmespath, botocore, s3transfer, boto3\n",
            "Successfully installed boto3-1.26.112 botocore-1.29.112 jmespath-1.0.1 s3transfer-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Upload Data to S3"
      ],
      "metadata": {
        "id": "Mss_SpU89_eo"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8MzsIwij__Dy"
      },
      "source": [
        "# upload tweets dataset to S3, please replace the bucket name and object keys with yours\n",
        "s3.upload_file(Filename='nashville-tweets-2019-01-28', Bucket='vandy-bigdata-2', Key='nashville-tweets-2019-01-28')\n",
        "s3.upload_file(Filename='Batting.csv', Bucket='vandy-bigdata-2', Key='Batting.csv')\n",
        "s3.upload_file(Filename='Salaries.csv', Bucket='vandy-bigdata-2', Key='Salaries.csv')"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lljwHyOy__D2"
      },
      "source": [
        "# replae with your EMR cluster ID\n",
        "CLUSTER_ID = 'j-3JBU686HGGWY1'\n",
        "\n",
        "def submit_job(app_name, pyfile_uri):\n",
        "    emr = session.client('emr')\n",
        "    emr.add_job_flow_steps(JobFlowId=CLUSTER_ID, Steps=[{\n",
        "        'Name': app_name,\n",
        "        'ActionOnFailure': 'CANCEL_AND_WAIT',\n",
        "        'HadoopJarStep': {\n",
        "            'Args': ['spark-submit',\n",
        "                     '--master', 'yarn',\n",
        "                     '--deploy-mode', 'cluster',\n",
        "                     pyfile_uri],\n",
        "            'Jar': 'command-runner.jar'\n",
        "        }}])"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# upload script to S3\n",
        "s3.upload_file(Filename='1_count.py', Bucket='vandy-bigdata-2', Key='hw6/1_count.py')"
      ],
      "metadata": {
        "id": "PrBnuVrdl-nb"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# submit spark job to emr\n",
        "submit_job(app_name='1_count', pyfile_uri='s3://vandy-bigdata-2/hw6/1_count.py')"
      ],
      "metadata": {
        "id": "3JGJ6K0Cl_Ph"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test emr execution results\n",
        "output_key = \"hw6/1_count.out/part-00000\"\n",
        "lines = s3.get_object(Bucket='vandy-bigdata-2', Key=output_key)['Body'].read().decode().splitlines()\n",
        "test1(lines)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fdQzRsqama14",
        "outputId": "be3931b0-5704-4788-dfe3-0356841978b3"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "passed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pCzOGzTj__D1"
      },
      "source": [
        "# upload script to S3\n",
        "s3.upload_file(Filename='2_group.py', Bucket='vandy-bigdata-2', Key='hw6/2_group.py')"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eoqkfcEn__D1"
      },
      "source": [
        "# submit spark job to emr\n",
        "submit_job(app_name='2_group', pyfile_uri='s3://vandy-bigdata-2/hw6/2_group.py')"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gm7iAdlL__D1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6981dff-9f16-4ea5-957f-2dd49e55fb5b"
      },
      "source": [
        "# test emr execution results\n",
        "output_key = \"hw6/2_group.out/part-00003\"\n",
        "lines = s3.get_object(Bucket='vandy-bigdata-2', Key=output_key)['Body'].read().decode().splitlines()\n",
        "test2(lines)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "passed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4VUV5ZIp__D2"
      },
      "source": [
        "# upload script to S3 - Make sure that the S3 bucket name is changed to your own bucket\n",
        "s3.upload_file(Filename='3_days.py', Bucket='vandy-bigdata-2', Key='hw6/3_days.py')"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YFWrjexF__D2"
      },
      "source": [
        "# submit spark job to emr\n",
        "submit_job(app_name='3_days', pyfile_uri='s3://vandy-bigdata-2/hw6/3_days.py')"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V1eFnxXZ__D3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28ed25ae-b573-46c2-eb73-174b5969714c"
      },
      "source": [
        "# test emr execution results\n",
        "output_key = \"hw6/3_days.out/part-00000\"\n",
        "lines = s3.get_object(Bucket='vandy-bigdata-2', Key=output_key)['Body'].read().decode().splitlines()\n",
        "test3(lines)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "passed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# upload script to S3 - Make sure that the S3 bucket name is changed to your own bucket\n",
        "s3.upload_file(Filename='4_join.py', Bucket='vandy-bigdata-2', Key='hw6/4_join.py')"
      ],
      "metadata": {
        "id": "3Dm0hI6OrpOT"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OkQiNiXW__D4"
      },
      "source": [
        "# submit spark job to emr\n",
        "submit_job(app_name='4_join', pyfile_uri='s3://vandy-bigdata-2/hw6/4_join.py')"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VqyNRexw__D4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39109a6f-dc8a-4dde-d0b1-b75919faaec9"
      },
      "source": [
        "# test emr execution results\n",
        "output_key = \"hw6/4_join.out/part-00000\"\n",
        "lines = s3.get_object(Bucket='vandy-bigdata-2', Key=output_key)['Body'].read().decode().splitlines()\n",
        "test4(lines)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test passed\n"
          ]
        }
      ]
    }
  ]
}
