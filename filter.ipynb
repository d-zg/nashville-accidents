{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7-final"
    },
    "orig_nbformat": 2,
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "colab": {
      "name": "spark_jobs.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vu-topics-in-big-data-2023/Team05/blob/main/filter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rRZIkSqT__Dv"
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Cf9znGuYBTM",
        "outputId": "ddfb10e6-515a-4adc-f20e-9b9242bd9647"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#install spark. we are using the one that uses hadoop as the underlying scheduler.\n",
        "!wget -q https://downloads.apache.org/spark/spark-3.2.4/spark-3.2.4-bin-hadoop3.2.tgz\n",
        "!tar xf  spark-3.2.4-bin-hadoop3.2.tgz\n",
        "!ls -l\n",
        "\n",
        "#Provides findspark.init() to make pyspark importable as a regular library.\n",
        "os.environ[\"SPARK_HOME\"] = \"spark-3.2.4-bin-hadoop3.2\"\n",
        "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.4.7 pyspark-shell'"
      ],
      "metadata": {
        "id": "_BcbpaYeKNlG",
        "outputId": "e1233c13-9ddf-4612-adbe-4f0d44f59c45",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 294140\n",
            "drwx------  6 root root      4096 May  1 18:23 drive\n",
            "drwxr-xr-x  1 root root      4096 Apr 28 13:35 sample_data\n",
            "drwxr-xr-x 13 1000 1000      4096 Apr  9 21:17 spark-3.2.4-bin-hadoop3.2\n",
            "-rw-r--r--  1 root root 301183180 Apr  9 21:35 spark-3.2.4-bin-hadoop3.2.tgz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q findspark pyspark\n",
        "import findspark\n",
        "findspark.init()"
      ],
      "metadata": {
        "id": "xB4TPZH16jmR",
        "outputId": "f38d31c4-84de-4d9f-fa93-51bcd8edda4b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install geopandas "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YHM9yyXtT6rG",
        "outputId": "dabd2a88-f9ec-4167-b172-6b58b3bdf1c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: geopandas in /usr/local/lib/python3.10/dist-packages (0.12.2)\n",
            "Requirement already satisfied: pandas>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from geopandas) (1.5.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from geopandas) (23.1)\n",
            "Requirement already satisfied: shapely>=1.7 in /usr/local/lib/python3.10/dist-packages (from geopandas) (2.0.1)\n",
            "Requirement already satisfied: fiona>=1.8 in /usr/local/lib/python3.10/dist-packages (from geopandas) (1.9.3)\n",
            "Requirement already satisfied: pyproj>=2.6.1.post1 in /usr/local/lib/python3.10/dist-packages (from geopandas) (3.5.0)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.10/dist-packages (from fiona>=1.8->geopandas) (23.1.0)\n",
            "Requirement already satisfied: munch>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from fiona>=1.8->geopandas) (2.5.0)\n",
            "Requirement already satisfied: click~=8.0 in /usr/local/lib/python3.10/dist-packages (from fiona>=1.8->geopandas) (8.1.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from fiona>=1.8->geopandas) (2022.12.7)\n",
            "Requirement already satisfied: cligj>=0.5 in /usr/local/lib/python3.10/dist-packages (from fiona>=1.8->geopandas) (0.7.2)\n",
            "Requirement already satisfied: click-plugins>=1.0 in /usr/local/lib/python3.10/dist-packages (from fiona>=1.8->geopandas) (1.1.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.0->geopandas) (1.22.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.0->geopandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.0->geopandas) (2022.7.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from munch>=2.3.2->fiona>=1.8->geopandas) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install geospark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gUcTxDt2tLKH",
        "outputId": "73428cee-834a-4759-e589-acbb4b2b7e9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: geospark in /usr/local/lib/python3.10/dist-packages (1.3.1)\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (from geospark) (3.4.0)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.10/dist-packages (from geospark) (23.1.0)\n",
            "Requirement already satisfied: shapely in /usr/local/lib/python3.10/dist-packages (from geospark) (2.0.1)\n",
            "Requirement already satisfied: findspark in /usr/local/lib/python3.10/dist-packages (from geospark) (2.0.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark->geospark) (0.10.9.7)\n",
            "Requirement already satisfied: numpy>=1.14 in /usr/local/lib/python3.10/dist-packages (from shapely->geospark) (1.22.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import date_format\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import DoubleType\n",
        "from pyspark.sql.functions import struct\n",
        "from pyspark.sql.functions import col, min\n",
        "from pyspark.sql.functions import pow, sqrt\n",
        "from pyspark.sql.functions import count\n",
        "\n",
        "spark = SparkSession.builder.appName(\"ParquetReader\").getOrCreate()\n",
        "weather_df = spark.read.parquet(\"/content/drive/MyDrive/big-data-final/joined_weather.parquet\")\n",
        "\n",
        "weather_sampled_df = weather_df.sample(fraction=0.001, seed=42)\n",
        "\n",
        "df = weather_sampled_df\n",
        "# # Step 1: Define UDF for Euclidean distance calculation\n",
        "# euclidean_distance_udf = udf(lambda lon1, lat1, lon2, lat2: ((lon1 - lon2)**2 + (lat1 - lat2)**2)**0.5, DoubleType())\n",
        "\n",
        "# # Step 2: Group DataFrame by Incident_ID\n",
        "\n",
        "def euclidean_distance(lat1, long1, lat2, long2):\n",
        "    return sqrt(pow(lat1 - lat2, 2) + pow(long1 - long2, 2))\n",
        "\n",
        "df_with_distance = df.withColumn('distance', euclidean_distance(col('latitude'), col('longitude'), col('gps_coordinate_latitude'), col('gps_coordinate_longitude')))\n",
        "\n",
        "df_grouped = df_with_distance.groupBy('Incident_ID') \\\n",
        "                             .agg(min('distance').alias('min_distance'))\n",
        "\n",
        "df_result = df_with_distance.join(df_grouped, 'Incident_ID') \\\n",
        "                             .filter(col('distance') == col('min_distance')) \\\n",
        "                             .drop('min_distance', 'distance')\n",
        "\n",
        "df_result = df_result.dropDuplicates([\"Incident_ID\"])\n"
      ],
      "metadata": {
        "id": "v1eiidhaSFbD"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_result.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E_Vhz1aJlHF2",
        "outputId": "ca19c4b6-d3f6-4eef-ac64-648fef3e296b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5217"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pdf = df_result.toPandas()\n",
        "print(pdf.Incident_ID.nunique())"
      ],
      "metadata": {
        "id": "95_r8oPNXoF_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ddd9c57e-f060-4dec-da1b-a0ac6c0a4daf"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/spark-3.2.4-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:194: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
            "  series = series.astype(t, copy=False)\n",
            "/content/spark-3.2.4-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:194: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
            "  series = series.astype(t, copy=False)\n",
            "/content/spark-3.2.4-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:194: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
            "  series = series.astype(t, copy=False)\n",
            "/content/spark-3.2.4-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:194: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
            "  series = series.astype(t, copy=False)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5217\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "weather_pdf = weather_sampled_df.toPandas()\n",
        "weather_pdf.columns"
      ],
      "metadata": {
        "id": "XcqqwKMDSrSb",
        "outputId": "7568ea45-46d8-4671-881d-22c2e71a5048",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/spark-3.2.4-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:194: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
            "  series = series.astype(t, copy=False)\n",
            "/content/spark-3.2.4-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:194: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
            "  series = series.astype(t, copy=False)\n",
            "/content/spark-3.2.4-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:194: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
            "  series = series.astype(t, copy=False)\n",
            "/content/spark-3.2.4-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:194: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
            "  series = series.astype(t, copy=False)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['ID_Original', 'latitude', 'longitude', 'emdCardNumber', 'time_utc',\n",
              "       'time_local', 'response_time_sec', 'day_of_week', 'weekend_or_not',\n",
              "       'geometry', 'Incident_ID', 'Dist_to_Seg', 'XDSegID', 'index_right',\n",
              "       'STATEFP', 'COUNTYFP', 'TRACTCE', 'GEOID', 'NAME', 'NAMELSAD', 'MTFCC',\n",
              "       'FUNCSTAT', 'ALAND', 'AWATER', 'INTPTLAT', 'INTPTLON',\n",
              "       '__index_level_0__', 'station_id', 'start_date_st', 'end_date_st',\n",
              "       'timestamp_local', 'rh', 'wind_spd', 'timestamp_utc', 'pod', 'slp',\n",
              "       'app_temp', 'elev_angle', 'solar_rad', 'pres', 'h_angle', 'dewpt',\n",
              "       'snow', 'uv', 'azimuth', 'wind_dir', 'ghi', 'dhi', 'vis', 'dni', 'temp',\n",
              "       'precip', 'clouds', 'ts', 'icon', 'code', 'description',\n",
              "       'gps_coordinate_latitude', 'gps_coordinate_longitude', 'spatial_id',\n",
              "       'days'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wUc1E5zdS7BG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GWmuMCcjTpI5",
        "outputId": "e47b6da5-631d-4374-e65a-d0c0638e5967",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 781
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "PythonException",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPythonException\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-444b6fd776e8>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmin_distance_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/spark-3.2.4-bin-hadoop3.2/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcount\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    678\u001b[0m         \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m         \"\"\"\n\u001b[0;32m--> 680\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    681\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mspark-3.2.4-bin-hadoop3.2/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.2.4-bin-hadoop3.2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"<ipython-input-12-74ec8f514f91>\", line 18, in get_min_distance_row\n  File \"/content/spark-3.2.4-bin-hadoop3.2/python/pyspark/sql/udf.py\", line 430, in wrapper\n    [Row(name='b', avg=102.0), Row(name='a', avg=102.0)]\n  File \"/content/spark-3.2.4-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/sql/udf.py\", line 177, in __call__\n    judf = self._judf\n  File \"/content/spark-3.2.4-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/sql/udf.py\", line 161, in _judf\n    self._judf_placeholder = self._create_judf()\n  File \"/content/spark-3.2.4-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/sql/udf.py\", line 167, in _create_judf\n    spark = SparkSession.builder.getOrCreate()\n  File \"/content/spark-3.2.4-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/sql/session.py\", line 228, in getOrCreate\n    sc = SparkContext.getOrCreate(sparkConf)\n  File \"/content/spark-3.2.4-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/context.py\", line 392, in getOrCreate\n    SparkContext(conf=conf or SparkConf())\n  File \"/content/spark-3.2.4-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/context.py\", line 136, in __init__\n    SparkContext._assert_on_driver()\n  File \"/content/spark-3.2.4-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/context.py\", line 1271, in _assert_on_driver\n    raise RuntimeError(\"SparkContext should only be created and accessed on the driver.\")\nRuntimeError: SparkContext should only be created and accessed on the driver.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import date_format\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "spark = SparkSession.builder.appName(\"ParquetReader\").getOrCreate()\n",
        "weather_df = spark.read.option(\"recursiveFileLookup\", \"true\").parquet(\"/content/drive/MyDrive/big-data-final/weather_tn.parquet\")\n",
        "\n",
        "# read a single parquet file called incidents\n",
        "incidents_df = spark.read.parquet(\"/content/drive/MyDrive/big-data-final/tracts_joined_with_points.parquet\")\n",
        "\n",
        "weather_sampled_df = weather_df.sample(fraction=0.0001, seed=42)\n",
        "\n",
        "# select a random subset of rows from the weather_sampled_df dataframe\n",
        "random_rows = weather_sampled_df.sample(fraction=0.01, seed=42)\n",
        "\n",
        "# select the datetime column and show some random values\n",
        "random_rows.select(\"datetime\").show(10)\n",
        "\n",
        "incidents_df.select(\"time_local\").show(1, 1000)\n",
        "\n",
        "df_with_new_column = incidents_df.withColumn(\n",
        "    \"date_local\",\n",
        "    date_format(\"time_local\", \"yyyy-MM-dd:HH\")\n",
        "        .cast(\"string\")\n",
        ")\n",
        "\n",
        "df_with_new_column = df_with_new_column.withColumnRenamed(\"date_local\", \"datetime\")\n",
        "\n",
        "df_with_new_column = df_with_new_column.repartition(col(\"datetime\"))\n",
        "\n",
        "weather_sampled_df = weather_sampled_df.repartition(col(\"datetime\"))\n",
        "\n",
        "# join the two dataframes on the date_local and datetime columns\n",
        "joined_df = df_with_new_column.join(\n",
        "    weather_sampled_df,\n",
        "    df_with_new_column[\"datetime\"] == weather_sampled_df[\"datetime\"],\n",
        "    \"inner\"\n",
        ")\n",
        "joined_df = joined_df.drop('datetime')\n",
        "joined_df.write.parquet(\"/content/joinedtest.parquet\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3lRu2xbTI46P",
        "outputId": "f774751a-d99a-4c6c-9cfd-dfd78aadc119"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+\n",
            "|     datetime|\n",
            "+-------------+\n",
            "|2013-07-24:09|\n",
            "|2016-07-18:20|\n",
            "|2012-07-24:23|\n",
            "|2012-05-29:02|\n",
            "|2015-07-12:23|\n",
            "|2017-08-23:00|\n",
            "|2011-08-23:15|\n",
            "|2020-04-01:06|\n",
            "|2020-04-05:07|\n",
            "|2018-04-15:03|\n",
            "+-------------+\n",
            "only showing top 10 rows\n",
            "\n",
            "+-----------------------+\n",
            "|             time_local|\n",
            "+-----------------------+\n",
            "|2017-01-01 01:59:29.507|\n",
            "+-----------------------+\n",
            "only showing top 1 row\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pdf.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 716
        },
        "id": "bui44qPMNgJG",
        "outputId": "3fca50bd-5046-4ae1-9ee5-b6bd17238749"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                          ID_Original   latitude  longitude emdCardNumber  \\\n",
              "0  ObjectId(5c607bae95fad01b9df02c82)  35.999451 -86.693431          29B1   \n",
              "1  ObjectId(5c60761d95fad01b9df02c79)  36.238125 -86.783028         29D2P   \n",
              "2  ObjectId(5cf1bb8195fad0113178cbc8)  36.119166 -86.728632         29D2P   \n",
              "3  ObjectId(59d3a99008f47311c8922ceb)  36.263097 -86.690496          29B1   \n",
              "4  ObjectId(59d3a99008f47311c8922ce8)  36.151630 -86.768547          29A2   \n",
              "\n",
              "                 time_utc              time_local  response_time_sec  \\\n",
              "0 2019-02-10 19:25:17.730 2019-02-10 13:25:17.730              707.0   \n",
              "1 2019-02-10 19:02:53.547 2019-02-10 13:02:53.547              323.0   \n",
              "2 2019-05-31 23:38:11.593 2019-05-31 18:38:11.593              359.0   \n",
              "3 2017-02-25 17:10:38.000 2017-02-25 11:10:38.000              258.0   \n",
              "4 2017-02-25 17:00:09.000 2017-02-25 11:00:09.000              275.0   \n",
              "\n",
              "   day_of_week  weekend_or_not  \\\n",
              "0            6               1   \n",
              "1            6               1   \n",
              "2            4               0   \n",
              "3            5               1   \n",
              "4            5               1   \n",
              "\n",
              "                                            geometry  ...  precip  clouds  \\\n",
              "0  [1, 1, 0, 0, 0, 123, 39, 110, 46, 97, 172, 85,...  ...     0.5   100.0   \n",
              "1  [1, 1, 0, 0, 0, 185, 24, 135, 35, 29, 178, 85,...  ...     0.5   100.0   \n",
              "2  [1, 1, 0, 0, 0, 48, 206, 198, 231, 161, 174, 8...  ...     0.0    68.0   \n",
              "3  [1, 1, 0, 0, 0, 151, 67, 197, 23, 49, 172, 85,...  ...     3.0   100.0   \n",
              "4  [1, 1, 0, 0, 0, 86, 37, 120, 224, 47, 177, 85,...  ...     3.0   100.0   \n",
              "\n",
              "             ts  icon   code      description gps_coordinate_latitude  \\\n",
              "0  1.549804e+09  c04n  804.0  Overcast clouds                  36.617   \n",
              "1  1.549804e+09  c04n  804.0  Overcast clouds                  36.617   \n",
              "2  1.559326e+09  c03d  803.0    Broken clouds                  35.030   \n",
              "3  1.488020e+09  r01n  500.0       Light rain                  36.859   \n",
              "4  1.488020e+09  r01n  500.0       Light rain                  36.859   \n",
              "\n",
              "  gps_coordinate_longitude spatial_id        days  \n",
              "0                  -87.417  Oak Grove  2019-02-10  \n",
              "1                  -87.417  Oak Grove  2019-02-10  \n",
              "2                  -85.200  Ridgeside  2019-05-31  \n",
              "3                  -83.358     Loyall  2017-02-25  \n",
              "4                  -83.358     Loyall  2017-02-25  \n",
              "\n",
              "[5 rows x 63 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-174717a0-d0ce-409d-9148-af27a5d292e9\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID_Original</th>\n",
              "      <th>latitude</th>\n",
              "      <th>longitude</th>\n",
              "      <th>emdCardNumber</th>\n",
              "      <th>time_utc</th>\n",
              "      <th>time_local</th>\n",
              "      <th>response_time_sec</th>\n",
              "      <th>day_of_week</th>\n",
              "      <th>weekend_or_not</th>\n",
              "      <th>geometry</th>\n",
              "      <th>...</th>\n",
              "      <th>precip</th>\n",
              "      <th>clouds</th>\n",
              "      <th>ts</th>\n",
              "      <th>icon</th>\n",
              "      <th>code</th>\n",
              "      <th>description</th>\n",
              "      <th>gps_coordinate_latitude</th>\n",
              "      <th>gps_coordinate_longitude</th>\n",
              "      <th>spatial_id</th>\n",
              "      <th>days</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ObjectId(5c607bae95fad01b9df02c82)</td>\n",
              "      <td>35.999451</td>\n",
              "      <td>-86.693431</td>\n",
              "      <td>29B1</td>\n",
              "      <td>2019-02-10 19:25:17.730</td>\n",
              "      <td>2019-02-10 13:25:17.730</td>\n",
              "      <td>707.0</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>[1, 1, 0, 0, 0, 123, 39, 110, 46, 97, 172, 85,...</td>\n",
              "      <td>...</td>\n",
              "      <td>0.5</td>\n",
              "      <td>100.0</td>\n",
              "      <td>1.549804e+09</td>\n",
              "      <td>c04n</td>\n",
              "      <td>804.0</td>\n",
              "      <td>Overcast clouds</td>\n",
              "      <td>36.617</td>\n",
              "      <td>-87.417</td>\n",
              "      <td>Oak Grove</td>\n",
              "      <td>2019-02-10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ObjectId(5c60761d95fad01b9df02c79)</td>\n",
              "      <td>36.238125</td>\n",
              "      <td>-86.783028</td>\n",
              "      <td>29D2P</td>\n",
              "      <td>2019-02-10 19:02:53.547</td>\n",
              "      <td>2019-02-10 13:02:53.547</td>\n",
              "      <td>323.0</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>[1, 1, 0, 0, 0, 185, 24, 135, 35, 29, 178, 85,...</td>\n",
              "      <td>...</td>\n",
              "      <td>0.5</td>\n",
              "      <td>100.0</td>\n",
              "      <td>1.549804e+09</td>\n",
              "      <td>c04n</td>\n",
              "      <td>804.0</td>\n",
              "      <td>Overcast clouds</td>\n",
              "      <td>36.617</td>\n",
              "      <td>-87.417</td>\n",
              "      <td>Oak Grove</td>\n",
              "      <td>2019-02-10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ObjectId(5cf1bb8195fad0113178cbc8)</td>\n",
              "      <td>36.119166</td>\n",
              "      <td>-86.728632</td>\n",
              "      <td>29D2P</td>\n",
              "      <td>2019-05-31 23:38:11.593</td>\n",
              "      <td>2019-05-31 18:38:11.593</td>\n",
              "      <td>359.0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>[1, 1, 0, 0, 0, 48, 206, 198, 231, 161, 174, 8...</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>68.0</td>\n",
              "      <td>1.559326e+09</td>\n",
              "      <td>c03d</td>\n",
              "      <td>803.0</td>\n",
              "      <td>Broken clouds</td>\n",
              "      <td>35.030</td>\n",
              "      <td>-85.200</td>\n",
              "      <td>Ridgeside</td>\n",
              "      <td>2019-05-31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ObjectId(59d3a99008f47311c8922ceb)</td>\n",
              "      <td>36.263097</td>\n",
              "      <td>-86.690496</td>\n",
              "      <td>29B1</td>\n",
              "      <td>2017-02-25 17:10:38.000</td>\n",
              "      <td>2017-02-25 11:10:38.000</td>\n",
              "      <td>258.0</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>[1, 1, 0, 0, 0, 151, 67, 197, 23, 49, 172, 85,...</td>\n",
              "      <td>...</td>\n",
              "      <td>3.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>1.488020e+09</td>\n",
              "      <td>r01n</td>\n",
              "      <td>500.0</td>\n",
              "      <td>Light rain</td>\n",
              "      <td>36.859</td>\n",
              "      <td>-83.358</td>\n",
              "      <td>Loyall</td>\n",
              "      <td>2017-02-25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ObjectId(59d3a99008f47311c8922ce8)</td>\n",
              "      <td>36.151630</td>\n",
              "      <td>-86.768547</td>\n",
              "      <td>29A2</td>\n",
              "      <td>2017-02-25 17:00:09.000</td>\n",
              "      <td>2017-02-25 11:00:09.000</td>\n",
              "      <td>275.0</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>[1, 1, 0, 0, 0, 86, 37, 120, 224, 47, 177, 85,...</td>\n",
              "      <td>...</td>\n",
              "      <td>3.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>1.488020e+09</td>\n",
              "      <td>r01n</td>\n",
              "      <td>500.0</td>\n",
              "      <td>Light rain</td>\n",
              "      <td>36.859</td>\n",
              "      <td>-83.358</td>\n",
              "      <td>Loyall</td>\n",
              "      <td>2017-02-25</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 63 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-174717a0-d0ce-409d-9148-af27a5d292e9')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-174717a0-d0ce-409d-9148-af27a5d292e9 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-174717a0-d0ce-409d-9148-af27a5d292e9');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup AWS Credentials and fill them here. Make sure you do not save this information back to github"
      ],
      "metadata": {
        "id": "dyeTsQLV-B-u"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YDL78heZ__Dz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb4fdf17-62e3-4c15-d62d-1c3660cf6628"
      },
      "source": [
        "%%file filter_weather.py\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import date_format\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import DoubleType\n",
        "from pyspark.sql.functions import struct\n",
        "from pyspark.sql.functions import col, min\n",
        "from pyspark.sql.functions import pow, sqrt\n",
        "from pyspark.sql.functions import count\n",
        "\n",
        "#create spark context. This is very important. Do this similarly for the other parts\n",
        "# Note to read a file directly from s3 into an rdd you may have to do something like this\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  # replace this line with the s3 pass when testing over EMR (? check proj)\n",
        "  spark = SparkSession.builder.appName(\"filter_weather\").getOrCreate()\n",
        "\n",
        "  try:\n",
        "    weather_df = spark.read.parquet(\"s3://bd-final/outputs/joined_weather.parquet/\")\n",
        "\n",
        "    df = weather_df.repartition(col(\"Incident_ID\"))\n",
        "    # # Step 1: Define UDF for Euclidean distance calculation\n",
        "    # euclidean_distance_udf = udf(lambda lon1, lat1, lon2, lat2: ((lon1 - lon2)**2 + (lat1 - lat2)**2)**0.5, DoubleType())\n",
        "\n",
        "    # # Step 2: Group DataFrame by Incident_ID\n",
        "\n",
        "    def euclidean_distance(lat1, long1, lat2, long2):\n",
        "        return sqrt(pow(lat1 - lat2, 2) + pow(long1 - long2, 2))\n",
        "\n",
        "    df_with_distance = df.withColumn('distance', euclidean_distance(col('latitude'), col('longitude'), col('gps_coordinate_latitude'), col('gps_coordinate_longitude')))\n",
        "\n",
        "    df_grouped = df_with_distance.groupBy('Incident_ID') \\\n",
        "                                .agg(min('distance').alias('min_distance'))\n",
        "\n",
        "    df_result = df_with_distance.join(df_grouped, 'Incident_ID') \\\n",
        "                                .filter(col('distance') == col('min_distance')) \\\n",
        "                                .drop('min_distance', 'distance')\n",
        "\n",
        "    df_result = df_result.dropDuplicates([\"Incident_ID\"])\n",
        "\n",
        "    df_result.coalesce(1).write.parquet(\"s3://bd-final/outputs/filtered_weather.parquet\")\n",
        "  finally:\n",
        "    # very important: stop the context. Otherwise you may get an error that context is still alive. if you are on colab just restart the runtime if you face problem\n",
        "    #finally is used to make sure the context is stopped even with errors\n",
        "    spark.stop()\n",
        "  \n",
        "\n",
        " \n",
        "  \n",
        "  pass"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting filter_weather.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3y2A34y__Dz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d76af75-7856-4149-c102-c039c94431f8"
      },
      "source": [
        "# execute locally and ensure everything works. If it works you should get the 1_count.out/part-00000 file. \n",
        "!spark-submit --packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.4.7 filter_weather.py\n",
        "# note the cell magic command %%file 1_count.py is used to create a local copy of the content of cell as a file 1_count.py on colab"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ":: loading settings :: url = jar:file:/content/spark-3.2.4-bin-hadoop3.2/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
            "Ivy Default Cache set to: /root/.ivy2/cache\n",
            "The jars for the packages stored in: /root/.ivy2/jars\n",
            "org.apache.spark#spark-streaming-kafka-0-8_2.11 added as a dependency\n",
            ":: resolving dependencies :: org.apache.spark#spark-submit-parent-68ff2be5-1742-4a56-895a-e8de0d2c8599;1.0\n",
            "\tconfs: [default]\n",
            "\tfound org.apache.spark#spark-streaming-kafka-0-8_2.11;2.4.7 in central\n",
            "\tfound org.apache.kafka#kafka_2.11;0.8.2.1 in central\n",
            "\tfound org.scala-lang.modules#scala-xml_2.11;1.0.2 in central\n",
            "\tfound com.yammer.metrics#metrics-core;2.2.0 in central\n",
            "\tfound org.slf4j#slf4j-api;1.7.16 in central\n",
            "\tfound org.scala-lang.modules#scala-parser-combinators_2.11;1.1.0 in central\n",
            "\tfound com.101tec#zkclient;0.3 in central\n",
            "\tfound log4j#log4j;1.2.17 in central\n",
            "\tfound org.apache.kafka#kafka-clients;0.8.2.1 in central\n",
            "\tfound net.jpountz.lz4#lz4;1.2.0 in central\n",
            "\tfound org.xerial.snappy#snappy-java;1.1.7.5 in central\n",
            "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
            ":: resolution report :: resolve 990ms :: artifacts dl 52ms\n",
            "\t:: modules in use:\n",
            "\tcom.101tec#zkclient;0.3 from central in [default]\n",
            "\tcom.yammer.metrics#metrics-core;2.2.0 from central in [default]\n",
            "\tlog4j#log4j;1.2.17 from central in [default]\n",
            "\tnet.jpountz.lz4#lz4;1.2.0 from central in [default]\n",
            "\torg.apache.kafka#kafka-clients;0.8.2.1 from central in [default]\n",
            "\torg.apache.kafka#kafka_2.11;0.8.2.1 from central in [default]\n",
            "\torg.apache.spark#spark-streaming-kafka-0-8_2.11;2.4.7 from central in [default]\n",
            "\torg.scala-lang.modules#scala-parser-combinators_2.11;1.1.0 from central in [default]\n",
            "\torg.scala-lang.modules#scala-xml_2.11;1.0.2 from central in [default]\n",
            "\torg.slf4j#slf4j-api;1.7.16 from central in [default]\n",
            "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
            "\torg.xerial.snappy#snappy-java;1.1.7.5 from central in [default]\n",
            "\t---------------------------------------------------------------------\n",
            "\t|                  |            modules            ||   artifacts   |\n",
            "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
            "\t---------------------------------------------------------------------\n",
            "\t|      default     |   12  |   0   |   0   |   0   ||   12  |   0   |\n",
            "\t---------------------------------------------------------------------\n",
            ":: retrieving :: org.apache.spark#spark-submit-parent-68ff2be5-1742-4a56-895a-e8de0d2c8599\n",
            "\tconfs: [default]\n",
            "\t0 artifacts copied, 12 already retrieved (0kB/22ms)\n",
            "23/05/01 18:48:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
            "23/05/01 18:48:36 INFO SparkContext: Running Spark version 3.2.4\n",
            "23/05/01 18:48:36 INFO ResourceUtils: ==============================================================\n",
            "23/05/01 18:48:36 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "23/05/01 18:48:36 INFO ResourceUtils: ==============================================================\n",
            "23/05/01 18:48:36 INFO SparkContext: Submitted application: filter_weather\n",
            "23/05/01 18:48:36 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "23/05/01 18:48:37 INFO ResourceProfile: Limiting resource is cpu\n",
            "23/05/01 18:48:37 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "23/05/01 18:48:37 INFO SecurityManager: Changing view acls to: root\n",
            "23/05/01 18:48:37 INFO SecurityManager: Changing modify acls to: root\n",
            "23/05/01 18:48:37 INFO SecurityManager: Changing view acls groups to: \n",
            "23/05/01 18:48:37 INFO SecurityManager: Changing modify acls groups to: \n",
            "23/05/01 18:48:37 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\n",
            "23/05/01 18:48:37 INFO Utils: Successfully started service 'sparkDriver' on port 33109.\n",
            "23/05/01 18:48:37 INFO SparkEnv: Registering MapOutputTracker\n",
            "23/05/01 18:48:37 INFO SparkEnv: Registering BlockManagerMaster\n",
            "23/05/01 18:48:37 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "23/05/01 18:48:37 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "23/05/01 18:48:37 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "23/05/01 18:48:38 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-4ff7b458-4740-410e-8190-68607f324608\n",
            "23/05/01 18:48:38 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB\n",
            "23/05/01 18:48:38 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "23/05/01 18:48:38 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
            "23/05/01 18:48:38 INFO Utils: Successfully started service 'SparkUI' on port 4041.\n",
            "23/05/01 18:48:38 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://d6cf72fcf488:4041\n",
            "23/05/01 18:48:38 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar at spark://d6cf72fcf488:33109/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1682966916850\n",
            "23/05/01 18:48:38 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar at spark://d6cf72fcf488:33109/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1682966916850\n",
            "23/05/01 18:48:38 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar at spark://d6cf72fcf488:33109/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1682966916850\n",
            "23/05/01 18:48:38 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar at spark://d6cf72fcf488:33109/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1682966916850\n",
            "23/05/01 18:48:38 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar at spark://d6cf72fcf488:33109/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1682966916850\n",
            "23/05/01 18:48:38 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar at spark://d6cf72fcf488:33109/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1682966916850\n",
            "23/05/01 18:48:38 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar at spark://d6cf72fcf488:33109/jars/com.101tec_zkclient-0.3.jar with timestamp 1682966916850\n",
            "23/05/01 18:48:38 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar at spark://d6cf72fcf488:33109/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1682966916850\n",
            "23/05/01 18:48:38 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar at spark://d6cf72fcf488:33109/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1682966916850\n",
            "23/05/01 18:48:38 INFO SparkContext: Added JAR file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar at spark://d6cf72fcf488:33109/jars/log4j_log4j-1.2.17.jar with timestamp 1682966916850\n",
            "23/05/01 18:48:38 INFO SparkContext: Added JAR file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar at spark://d6cf72fcf488:33109/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1682966916850\n",
            "23/05/01 18:48:38 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar at spark://d6cf72fcf488:33109/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1682966916850\n",
            "23/05/01 18:48:38 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar at file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1682966916850\n",
            "23/05/01 18:48:38 INFO Utils: Copying /root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar to /tmp/spark-2cc600ff-7644-4e15-905b-35875dcdd421/userFiles-b0d3cc74-c87a-46af-b943-8c71cc7e7ba9/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar\n",
            "23/05/01 18:48:38 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar at file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1682966916850\n",
            "23/05/01 18:48:38 INFO Utils: Copying /root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar to /tmp/spark-2cc600ff-7644-4e15-905b-35875dcdd421/userFiles-b0d3cc74-c87a-46af-b943-8c71cc7e7ba9/org.apache.kafka_kafka_2.11-0.8.2.1.jar\n",
            "23/05/01 18:48:38 INFO SparkContext: Added file file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar at file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1682966916850\n",
            "23/05/01 18:48:38 INFO Utils: Copying /root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-2cc600ff-7644-4e15-905b-35875dcdd421/userFiles-b0d3cc74-c87a-46af-b943-8c71cc7e7ba9/org.spark-project.spark_unused-1.0.0.jar\n",
            "23/05/01 18:48:38 INFO SparkContext: Added file file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar at file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1682966916850\n",
            "23/05/01 18:48:38 INFO Utils: Copying /root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar to /tmp/spark-2cc600ff-7644-4e15-905b-35875dcdd421/userFiles-b0d3cc74-c87a-46af-b943-8c71cc7e7ba9/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar\n",
            "23/05/01 18:48:38 INFO SparkContext: Added file file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar at file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1682966916850\n",
            "23/05/01 18:48:38 INFO Utils: Copying /root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar to /tmp/spark-2cc600ff-7644-4e15-905b-35875dcdd421/userFiles-b0d3cc74-c87a-46af-b943-8c71cc7e7ba9/com.yammer.metrics_metrics-core-2.2.0.jar\n",
            "23/05/01 18:48:38 INFO SparkContext: Added file file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar at file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1682966916850\n",
            "23/05/01 18:48:38 INFO Utils: Copying /root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar to /tmp/spark-2cc600ff-7644-4e15-905b-35875dcdd421/userFiles-b0d3cc74-c87a-46af-b943-8c71cc7e7ba9/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar\n",
            "23/05/01 18:48:38 INFO SparkContext: Added file file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar at file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar with timestamp 1682966916850\n",
            "23/05/01 18:48:38 INFO Utils: Copying /root/.ivy2/jars/com.101tec_zkclient-0.3.jar to /tmp/spark-2cc600ff-7644-4e15-905b-35875dcdd421/userFiles-b0d3cc74-c87a-46af-b943-8c71cc7e7ba9/com.101tec_zkclient-0.3.jar\n",
            "23/05/01 18:48:38 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar at file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1682966916850\n",
            "23/05/01 18:48:38 INFO Utils: Copying /root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar to /tmp/spark-2cc600ff-7644-4e15-905b-35875dcdd421/userFiles-b0d3cc74-c87a-46af-b943-8c71cc7e7ba9/org.apache.kafka_kafka-clients-0.8.2.1.jar\n",
            "23/05/01 18:48:39 INFO SparkContext: Added file file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar at file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1682966916850\n",
            "23/05/01 18:48:39 INFO Utils: Copying /root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar to /tmp/spark-2cc600ff-7644-4e15-905b-35875dcdd421/userFiles-b0d3cc74-c87a-46af-b943-8c71cc7e7ba9/org.slf4j_slf4j-api-1.7.16.jar\n",
            "23/05/01 18:48:39 INFO SparkContext: Added file file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar at file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar with timestamp 1682966916850\n",
            "23/05/01 18:48:39 INFO Utils: Copying /root/.ivy2/jars/log4j_log4j-1.2.17.jar to /tmp/spark-2cc600ff-7644-4e15-905b-35875dcdd421/userFiles-b0d3cc74-c87a-46af-b943-8c71cc7e7ba9/log4j_log4j-1.2.17.jar\n",
            "23/05/01 18:48:39 INFO SparkContext: Added file file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar at file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1682966916850\n",
            "23/05/01 18:48:39 INFO Utils: Copying /root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar to /tmp/spark-2cc600ff-7644-4e15-905b-35875dcdd421/userFiles-b0d3cc74-c87a-46af-b943-8c71cc7e7ba9/net.jpountz.lz4_lz4-1.2.0.jar\n",
            "23/05/01 18:48:39 INFO SparkContext: Added file file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar at file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1682966916850\n",
            "23/05/01 18:48:39 INFO Utils: Copying /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar to /tmp/spark-2cc600ff-7644-4e15-905b-35875dcdd421/userFiles-b0d3cc74-c87a-46af-b943-8c71cc7e7ba9/org.xerial.snappy_snappy-java-1.1.7.5.jar\n",
            "23/05/01 18:48:39 INFO Executor: Starting executor ID driver on host d6cf72fcf488\n",
            "23/05/01 18:48:39 INFO Executor: Fetching file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar with timestamp 1682966916850\n",
            "23/05/01 18:48:39 INFO Utils: /root/.ivy2/jars/log4j_log4j-1.2.17.jar has been previously copied to /tmp/spark-2cc600ff-7644-4e15-905b-35875dcdd421/userFiles-b0d3cc74-c87a-46af-b943-8c71cc7e7ba9/log4j_log4j-1.2.17.jar\n",
            "23/05/01 18:48:39 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1682966916850\n",
            "23/05/01 18:48:39 INFO Utils: /root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar has been previously copied to /tmp/spark-2cc600ff-7644-4e15-905b-35875dcdd421/userFiles-b0d3cc74-c87a-46af-b943-8c71cc7e7ba9/org.apache.kafka_kafka-clients-0.8.2.1.jar\n",
            "23/05/01 18:48:39 INFO Executor: Fetching file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1682966916850\n",
            "23/05/01 18:48:39 INFO Utils: /root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar has been previously copied to /tmp/spark-2cc600ff-7644-4e15-905b-35875dcdd421/userFiles-b0d3cc74-c87a-46af-b943-8c71cc7e7ba9/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar\n",
            "23/05/01 18:48:39 INFO Executor: Fetching file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1682966916850\n",
            "23/05/01 18:48:39 INFO Utils: /root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar has been previously copied to /tmp/spark-2cc600ff-7644-4e15-905b-35875dcdd421/userFiles-b0d3cc74-c87a-46af-b943-8c71cc7e7ba9/org.slf4j_slf4j-api-1.7.16.jar\n",
            "23/05/01 18:48:39 INFO Executor: Fetching file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1682966916850\n",
            "23/05/01 18:48:39 INFO Utils: /root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar has been previously copied to /tmp/spark-2cc600ff-7644-4e15-905b-35875dcdd421/userFiles-b0d3cc74-c87a-46af-b943-8c71cc7e7ba9/net.jpountz.lz4_lz4-1.2.0.jar\n",
            "23/05/01 18:48:39 INFO Executor: Fetching file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1682966916850\n",
            "23/05/01 18:48:39 INFO Utils: /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar has been previously copied to /tmp/spark-2cc600ff-7644-4e15-905b-35875dcdd421/userFiles-b0d3cc74-c87a-46af-b943-8c71cc7e7ba9/org.xerial.snappy_snappy-java-1.1.7.5.jar\n",
            "23/05/01 18:48:39 INFO Executor: Fetching file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1682966916850\n",
            "23/05/01 18:48:39 INFO Utils: /root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar has been previously copied to /tmp/spark-2cc600ff-7644-4e15-905b-35875dcdd421/userFiles-b0d3cc74-c87a-46af-b943-8c71cc7e7ba9/org.spark-project.spark_unused-1.0.0.jar\n",
            "23/05/01 18:48:39 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1682966916850\n",
            "23/05/01 18:48:39 INFO Utils: /root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar has been previously copied to /tmp/spark-2cc600ff-7644-4e15-905b-35875dcdd421/userFiles-b0d3cc74-c87a-46af-b943-8c71cc7e7ba9/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar\n",
            "23/05/01 18:48:39 INFO Executor: Fetching file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1682966916850\n",
            "23/05/01 18:48:39 INFO Utils: /root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar has been previously copied to /tmp/spark-2cc600ff-7644-4e15-905b-35875dcdd421/userFiles-b0d3cc74-c87a-46af-b943-8c71cc7e7ba9/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar\n",
            "23/05/01 18:48:39 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1682966916850\n",
            "23/05/01 18:48:39 INFO Utils: /root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar has been previously copied to /tmp/spark-2cc600ff-7644-4e15-905b-35875dcdd421/userFiles-b0d3cc74-c87a-46af-b943-8c71cc7e7ba9/org.apache.kafka_kafka_2.11-0.8.2.1.jar\n",
            "23/05/01 18:48:39 INFO Executor: Fetching file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar with timestamp 1682966916850\n",
            "23/05/01 18:48:39 INFO Utils: /root/.ivy2/jars/com.101tec_zkclient-0.3.jar has been previously copied to /tmp/spark-2cc600ff-7644-4e15-905b-35875dcdd421/userFiles-b0d3cc74-c87a-46af-b943-8c71cc7e7ba9/com.101tec_zkclient-0.3.jar\n",
            "23/05/01 18:48:39 INFO Executor: Fetching file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1682966916850\n",
            "23/05/01 18:48:39 INFO Utils: /root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar has been previously copied to /tmp/spark-2cc600ff-7644-4e15-905b-35875dcdd421/userFiles-b0d3cc74-c87a-46af-b943-8c71cc7e7ba9/com.yammer.metrics_metrics-core-2.2.0.jar\n",
            "23/05/01 18:48:39 INFO Executor: Fetching spark://d6cf72fcf488:33109/jars/log4j_log4j-1.2.17.jar with timestamp 1682966916850\n",
            "23/05/01 18:48:39 INFO TransportClientFactory: Successfully created connection to d6cf72fcf488/172.28.0.12:33109 after 78 ms (0 ms spent in bootstraps)\n",
            "23/05/01 18:48:40 INFO Utils: Fetching spark://d6cf72fcf488:33109/jars/log4j_log4j-1.2.17.jar to /tmp/spark-2cc600ff-7644-4e15-905b-35875dcdd421/userFiles-b0d3cc74-c87a-46af-b943-8c71cc7e7ba9/fetchFileTemp6637267442837799700.tmp\n",
            "23/05/01 18:48:40 INFO Utils: /tmp/spark-2cc600ff-7644-4e15-905b-35875dcdd421/userFiles-b0d3cc74-c87a-46af-b943-8c71cc7e7ba9/fetchFileTemp6637267442837799700.tmp has been previously copied to /tmp/spark-2cc600ff-7644-4e15-905b-35875dcdd421/userFiles-b0d3cc74-c87a-46af-b943-8c71cc7e7ba9/log4j_log4j-1.2.17.jar\n",
            "23/05/01 18:48:40 INFO Executor: Adding file:/tmp/spark-2cc600ff-7644-4e15-905b-35875dcdd421/userFiles-b0d3cc74-c87a-46af-b943-8c71cc7e7ba9/log4j_log4j-1.2.17.jar to class loader\n",
            "23/05/01 18:48:40 INFO Executor: Fetching spark://d6cf72fcf488:33109/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1682966916850\n",
            "23/05/01 18:48:40 INFO Utils: Fetching spark://d6cf72fcf488:33109/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar to /tmp/spark-2cc600ff-7644-4e15-905b-35875dcdd421/userFiles-b0d3cc74-c87a-46af-b943-8c71cc7e7ba9/fetchFileTemp1988593509695471406.tmp\n",
            "23/05/01 18:48:40 INFO Utils: /tmp/spark-2cc600ff-7644-4e15-905b-35875dcdd421/userFiles-b0d3cc74-c87a-46af-b943-8c71cc7e7ba9/fetchFileTemp1988593509695471406.tmp has been previously copied to /tmp/spark-2cc600ff-7644-4e15-905b-35875dcdd421/userFiles-b0d3cc74-c87a-46af-b943-8c71cc7e7ba9/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar\n",
            "23/05/01 18:48:40 INFO Executor: Adding file:/tmp/spark-2cc600ff-7644-4e15-905b-35875dcdd421/userFiles-b0d3cc74-c87a-46af-b943-8c71cc7e7ba9/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar to class loader\n",
            "23/05/01 18:48:40 INFO Executor: Fetching spark://d6cf72fcf488:33109/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1682966916850\n",
            "23/05/01 18:48:40 INFO Utils: Fetching spark://d6cf72fcf488:33109/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-2cc600ff-7644-4e15-905b-35875dcdd421/userFiles-b0d3cc74-c87a-46af-b943-8c71cc7e7ba9/fetchFileTemp1461733102215062158.tmp\n",
            "23/05/01 18:48:40 INFO Utils: /tmp/spark-2cc600ff-7644-4e15-905b-35875dcdd421/userFiles-b0d3cc74-c87a-46af-b943-8c71cc7e7ba9/fetchFileTemp1461733102215062158.tmp has been previously copied to /tmp/spark-2cc600ff-7644-4e15-905b-35875dcdd421/userFiles-b0d3cc74-c87a-46af-b943-8c71cc7e7ba9/org.spark-project.spark_unused-1.0.0.jar\n",
            "23/05/01 18:48:40 INFO Executor: Adding file:/tmp/spark-2cc600ff-7644-4e15-905b-35875dcdd421/userFiles-b0d3cc74-c87a-46af-b943-8c71cc7e7ba9/org.spark-project.spark_unused-1.0.0.jar to class loader\n",
            "23/05/01 18:48:40 INFO Executor: Fetching spark://d6cf72fcf488:33109/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1682966916850\n",
            "23/05/01 18:48:40 INFO Utils: Fetching spark://d6cf72fcf488:33109/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar to /tmp/spark-2cc600ff-7644-4e15-905b-35875dcdd421/userFiles-b0d3cc74-c87a-46af-b943-8c71cc7e7ba9/fetchFileTemp5409505707819376630.tmp\n",
            "23/05/01 18:48:40 INFO Utils: /tmp/spark-2cc600ff-7644-4e15-905b-35875dcdd421/userFiles-b0d3cc74-c87a-46af-b943-8c71cc7e7ba9/fetchFileTemp5409505707819376630.tmp has been previously copied to /tmp/spark-2cc600ff-7644-4e15-905b-35875dcdd421/userFiles-b0d3cc74-c87a-46af-b943-8c71cc7e7ba9/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar\n",
            "23/05/01 18:48:40 INFO Executor: Adding file:/tmp/spark-2cc600ff-7644-4e15-905b-35875dcdd421/userFiles-b0d3cc74-c87a-46af-b943-8c71cc7e7ba9/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar to class loader\n",
            "23/05/01 18:48:40 INFO Executor: Fetching spark://d6cf72fcf488:33109/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1682966916850\n",
            "23/05/01 18:48:40 INFO Utils: Fetching spark://d6cf72fcf488:33109/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar to /tmp/spark-2cc600ff-7644-4e15-905b-35875dcdd421/userFiles-b0d3cc74-c87a-46af-b943-8c71cc7e7ba9/fetchFileTemp7205474536608049573.tmp\n",
            "23/05/01 18:48:40 INFO Utils: /tmp/spark-2cc600ff-7644-4e15-905b-35875dcdd421/userFiles-b0d3cc74-c87a-46af-b943-8c71cc7e7ba9/fetchFileTemp7205474536608049573.tmp has been previously copied to /tmp/spark-2cc600ff-7644-4e15-905b-35875dcdd421/userFiles-b0d3cc74-c87a-46af-b943-8c71cc7e7ba9/org.xerial.snappy_snappy-java-1.1.7.5.jar\n",
            "23/05/01 18:48:40 INFO Executor: Adding file:/tmp/spark-2cc600ff-7644-4e15-905b-35875dcdd421/userFiles-b0d3cc74-c87a-46af-b943-8c71cc7e7ba9/org.xerial.snappy_snappy-java-1.1.7.5.jar to class loader\n",
            "23/05/01 18:48:40 INFO Executor: Fetching spark://d6cf72fcf488:33109/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1682966916850\n",
            "23/05/01 18:48:40 INFO Utils: Fetching spark://d6cf72fcf488:33109/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar to /tmp/spark-2cc600ff-7644-4e15-905b-35875dcdd421/userFiles-b0d3cc74-c87a-46af-b943-8c71cc7e7ba9/fetchFileTemp1278127094693925307.tmp\n",
            "23/05/01 18:48:40 INFO Utils: /tmp/spark-2cc600ff-7644-4e15-905b-35875dcdd421/userFiles-b0d3cc74-c87a-46af-b943-8c71cc7e7ba9/fetchFileTemp1278127094693925307.tmp has been previously copied to /tmp/spark-2cc600ff-7644-4e15-905b-35875dcdd421/userFiles-b0d3cc74-c87a-46af-b943-8c71cc7e7ba9/org.apache.kafka_kafka_2.11-0.8.2.1.jar\n",
            "23/05/01 18:48:40 INFO Executor: Adding file:/tmp/spark-2cc600ff-7644-4e15-905b-35875dcdd421/userFiles-b0d3cc74-c87a-46af-b943-8c71cc7e7ba9/org.apache.kafka_kafka_2.11-0.8.2.1.jar to class loader\n",
            "23/05/01 18:48:40 INFO Executor: Fetching spark://d6cf72fcf488:33109/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1682966916850\n",
            "23/05/01 18:48:40 INFO Utils: Fetching spark://d6cf72fcf488:33109/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar to /tmp/spark-2cc600ff-7644-4e15-905b-35875dcdd421/userFiles-b0d3cc74-c87a-46af-b943-8c71cc7e7ba9/fetchFileTemp3120609218804056408.tmp\n",
            "23/05/01 18:48:40 INFO Utils: /tmp/spark-2cc600ff-7644-4e15-905b-35875dcdd421/userFiles-b0d3cc74-c87a-46af-b943-8c71cc7e7ba9/fetchFileTemp3120609218804056408.tmp has been previously copied to /tmp/spark-2cc600ff-7644-4e15-905b-35875dcdd421/userFiles-b0d3cc74-c87a-46af-b943-8c71cc7e7ba9/org.apache.kafka_kafka-clients-0.8.2.1.jar\n",
            "23/05/01 18:48:40 INFO Executor: Adding file:/tmp/spark-2cc600ff-7644-4e15-905b-35875dcdd421/userFiles-b0d3cc74-c87a-46af-b943-8c71cc7e7ba9/org.apache.kafka_kafka-clients-0.8.2.1.jar to class loader\n",
            "23/05/01 18:48:40 INFO Executor: Fetching spark://d6cf72fcf488:33109/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1682966916850\n",
            "23/05/01 18:48:40 INFO Utils: Fetching spark://d6cf72fcf488:33109/jars/net.jpountz.lz4_lz4-1.2.0.jar to /tmp/spark-2cc600ff-7644-4e15-905b-35875dcdd421/userFiles-b0d3cc74-c87a-46af-b943-8c71cc7e7ba9/fetchFileTemp196853220992355093.tmp\n",
            "23/05/01 18:48:40 INFO Utils: /tmp/spark-2cc600ff-7644-4e15-905b-35875dcdd421/userFiles-b0d3cc74-c87a-46af-b943-8c71cc7e7ba9/fetchFileTemp196853220992355093.tmp has been previously copied to /tmp/spark-2cc600ff-7644-4e15-905b-35875dcdd421/userFiles-b0d3cc74-c87a-46af-b943-8c71cc7e7ba9/net.jpountz.lz4_lz4-1.2.0.jar\n",
            "23/05/01 18:48:40 INFO Executor: Adding file:/tmp/spark-2cc600ff-7644-4e15-905b-35875dcdd421/userFiles-b0d3cc74-c87a-46af-b943-8c71cc7e7ba9/net.jpountz.lz4_lz4-1.2.0.jar to class loader\n",
            "23/05/01 18:48:40 INFO Executor: Fetching spark://d6cf72fcf488:33109/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1682966916850\n",
            "23/05/01 18:48:40 INFO Utils: Fetching spark://d6cf72fcf488:33109/jars/com.yammer.metrics_metrics-core-2.2.0.jar to /tmp/spark-2cc600ff-7644-4e15-905b-35875dcdd421/userFiles-b0d3cc74-c87a-46af-b943-8c71cc7e7ba9/fetchFileTemp3981694635196108634.tmp\n",
            "23/05/01 18:48:40 INFO Utils: /tmp/spark-2cc600ff-7644-4e15-905b-35875dcdd421/userFiles-b0d3cc74-c87a-46af-b943-8c71cc7e7ba9/fetchFileTemp3981694635196108634.tmp has been previously copied to /tmp/spark-2cc600ff-7644-4e15-905b-35875dcdd421/userFiles-b0d3cc74-c87a-46af-b943-8c71cc7e7ba9/com.yammer.metrics_metrics-core-2.2.0.jar\n",
            "23/05/01 18:48:40 INFO Executor: Adding file:/tmp/spark-2cc600ff-7644-4e15-905b-35875dcdd421/userFiles-b0d3cc74-c87a-46af-b943-8c71cc7e7ba9/com.yammer.metrics_metrics-core-2.2.0.jar to class loader\n",
            "23/05/01 18:48:40 INFO Executor: Fetching spark://d6cf72fcf488:33109/jars/com.101tec_zkclient-0.3.jar with timestamp 1682966916850\n",
            "23/05/01 18:48:40 INFO Utils: Fetching spark://d6cf72fcf488:33109/jars/com.101tec_zkclient-0.3.jar to /tmp/spark-2cc600ff-7644-4e15-905b-35875dcdd421/userFiles-b0d3cc74-c87a-46af-b943-8c71cc7e7ba9/fetchFileTemp7655092546295968448.tmp\n",
            "23/05/01 18:48:40 INFO Utils: /tmp/spark-2cc600ff-7644-4e15-905b-35875dcdd421/userFiles-b0d3cc74-c87a-46af-b943-8c71cc7e7ba9/fetchFileTemp7655092546295968448.tmp has been previously copied to /tmp/spark-2cc600ff-7644-4e15-905b-35875dcdd421/userFiles-b0d3cc74-c87a-46af-b943-8c71cc7e7ba9/com.101tec_zkclient-0.3.jar\n",
            "23/05/01 18:48:40 INFO Executor: Adding file:/tmp/spark-2cc600ff-7644-4e15-905b-35875dcdd421/userFiles-b0d3cc74-c87a-46af-b943-8c71cc7e7ba9/com.101tec_zkclient-0.3.jar to class loader\n",
            "23/05/01 18:48:40 INFO Executor: Fetching spark://d6cf72fcf488:33109/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1682966916850\n",
            "23/05/01 18:48:40 INFO Utils: Fetching spark://d6cf72fcf488:33109/jars/org.slf4j_slf4j-api-1.7.16.jar to /tmp/spark-2cc600ff-7644-4e15-905b-35875dcdd421/userFiles-b0d3cc74-c87a-46af-b943-8c71cc7e7ba9/fetchFileTemp2597079508181714093.tmp\n",
            "23/05/01 18:48:40 INFO Utils: /tmp/spark-2cc600ff-7644-4e15-905b-35875dcdd421/userFiles-b0d3cc74-c87a-46af-b943-8c71cc7e7ba9/fetchFileTemp2597079508181714093.tmp has been previously copied to /tmp/spark-2cc600ff-7644-4e15-905b-35875dcdd421/userFiles-b0d3cc74-c87a-46af-b943-8c71cc7e7ba9/org.slf4j_slf4j-api-1.7.16.jar\n",
            "23/05/01 18:48:40 INFO Executor: Adding file:/tmp/spark-2cc600ff-7644-4e15-905b-35875dcdd421/userFiles-b0d3cc74-c87a-46af-b943-8c71cc7e7ba9/org.slf4j_slf4j-api-1.7.16.jar to class loader\n",
            "23/05/01 18:48:40 INFO Executor: Fetching spark://d6cf72fcf488:33109/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1682966916850\n",
            "23/05/01 18:48:40 INFO Utils: Fetching spark://d6cf72fcf488:33109/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar to /tmp/spark-2cc600ff-7644-4e15-905b-35875dcdd421/userFiles-b0d3cc74-c87a-46af-b943-8c71cc7e7ba9/fetchFileTemp6310046613563346137.tmp\n",
            "23/05/01 18:48:40 INFO Utils: /tmp/spark-2cc600ff-7644-4e15-905b-35875dcdd421/userFiles-b0d3cc74-c87a-46af-b943-8c71cc7e7ba9/fetchFileTemp6310046613563346137.tmp has been previously copied to /tmp/spark-2cc600ff-7644-4e15-905b-35875dcdd421/userFiles-b0d3cc74-c87a-46af-b943-8c71cc7e7ba9/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar\n",
            "23/05/01 18:48:40 INFO Executor: Adding file:/tmp/spark-2cc600ff-7644-4e15-905b-35875dcdd421/userFiles-b0d3cc74-c87a-46af-b943-8c71cc7e7ba9/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar to class loader\n",
            "23/05/01 18:48:40 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 41509.\n",
            "23/05/01 18:48:40 INFO NettyBlockTransferService: Server created on d6cf72fcf488:41509\n",
            "23/05/01 18:48:40 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "23/05/01 18:48:40 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, d6cf72fcf488, 41509, None)\n",
            "23/05/01 18:48:40 INFO BlockManagerMasterEndpoint: Registering block manager d6cf72fcf488:41509 with 366.3 MiB RAM, BlockManagerId(driver, d6cf72fcf488, 41509, None)\n",
            "23/05/01 18:48:40 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, d6cf72fcf488, 41509, None)\n",
            "23/05/01 18:48:40 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, d6cf72fcf488, 41509, None)\n",
            "23/05/01 18:48:41 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
            "23/05/01 18:48:41 INFO SharedState: Warehouse path is 'file:/content/spark-warehouse'.\n",
            "23/05/01 18:48:44 INFO InMemoryFileIndex: It took 122 ms to list leaf files for 1 paths.\n",
            "23/05/01 18:48:45 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0\n",
            "23/05/01 18:48:45 INFO DAGScheduler: Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "23/05/01 18:48:45 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)\n",
            "23/05/01 18:48:45 INFO DAGScheduler: Parents of final stage: List()\n",
            "23/05/01 18:48:45 INFO DAGScheduler: Missing parents: List()\n",
            "23/05/01 18:48:45 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/05/01 18:48:45 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 104.9 KiB, free 366.2 MiB)\n",
            "23/05/01 18:48:45 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 37.6 KiB, free 366.2 MiB)\n",
            "23/05/01 18:48:45 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on d6cf72fcf488:41509 (size: 37.6 KiB, free: 366.3 MiB)\n",
            "23/05/01 18:48:45 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1474\n",
            "23/05/01 18:48:46 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/05/01 18:48:46 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
            "23/05/01 18:48:46 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (d6cf72fcf488, executor driver, partition 0, PROCESS_LOCAL, 4681 bytes) taskResourceAssignments Map()\n",
            "23/05/01 18:48:46 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
            "23/05/01 18:48:47 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 4361 bytes result sent to driver\n",
            "23/05/01 18:48:47 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1754 ms on d6cf72fcf488 (executor driver) (1/1)\n",
            "23/05/01 18:48:47 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
            "23/05/01 18:48:47 INFO DAGScheduler: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 2.445 s\n",
            "23/05/01 18:48:48 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "23/05/01 18:48:48 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
            "23/05/01 18:48:48 INFO DAGScheduler: Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 2.599052 s\n",
            "23/05/01 18:48:49 INFO BlockManagerInfo: Removed broadcast_0_piece0 on d6cf72fcf488:41509 in memory (size: 37.6 KiB, free: 366.3 MiB)\n",
            "23/05/01 18:48:53 INFO FileSourceStrategy: Pushed Filters: \n",
            "23/05/01 18:48:53 INFO FileSourceStrategy: Post-Scan Filters: \n",
            "23/05/01 18:48:53 INFO FileSourceStrategy: Output Data Schema: struct<ID_Original: string, latitude: double, longitude: double, emdCardNumber: string, time_utc: timestamp ... 59 more fields>\n",
            "23/05/01 18:48:53 INFO FileSourceStrategy: Pushed Filters: \n",
            "23/05/01 18:48:53 INFO FileSourceStrategy: Post-Scan Filters: \n",
            "23/05/01 18:48:53 INFO FileSourceStrategy: Output Data Schema: struct<latitude: double, longitude: double, Incident_ID: int, gps_coordinate_latitude: double, gps_coordinate_longitude: double ... 3 more fields>\n",
            "23/05/01 18:48:53 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
            "23/05/01 18:48:53 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "23/05/01 18:48:54 INFO CodeGenerator: Code generated in 507.885242 ms\n",
            "23/05/01 18:48:54 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 353.4 KiB, free 366.0 MiB)\n",
            "23/05/01 18:48:54 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 35.8 KiB, free 365.9 MiB)\n",
            "23/05/01 18:48:54 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on d6cf72fcf488:41509 (size: 35.8 KiB, free: 366.3 MiB)\n",
            "23/05/01 18:48:54 INFO SparkContext: Created broadcast 1 from parquet at NativeMethodAccessorImpl.java:0\n",
            "23/05/01 18:48:54 INFO FileSourceScanExec: Planning scan with bin packing, max size: 71942091 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "23/05/01 18:48:54 INFO DAGScheduler: Registering RDD 5 (parquet at NativeMethodAccessorImpl.java:0) as input to shuffle 0\n",
            "23/05/01 18:48:54 INFO DAGScheduler: Got map stage job 1 (parquet at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
            "23/05/01 18:48:54 INFO DAGScheduler: Final stage: ShuffleMapStage 1 (parquet at NativeMethodAccessorImpl.java:0)\n",
            "23/05/01 18:48:54 INFO DAGScheduler: Parents of final stage: List()\n",
            "23/05/01 18:48:54 INFO DAGScheduler: Missing parents: List()\n",
            "23/05/01 18:48:54 INFO DAGScheduler: Submitting ShuffleMapStage 1 (MapPartitionsRDD[5] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/05/01 18:48:54 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 42.4 KiB, free 365.9 MiB)\n",
            "23/05/01 18:48:54 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 18.5 KiB, free 365.9 MiB)\n",
            "23/05/01 18:48:54 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on d6cf72fcf488:41509 (size: 18.5 KiB, free: 366.2 MiB)\n",
            "23/05/01 18:48:54 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1474\n",
            "23/05/01 18:48:54 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[5] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
            "23/05/01 18:48:54 INFO TaskSchedulerImpl: Adding task set 1.0 with 2 tasks resource profile 0\n",
            "23/05/01 18:48:54 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (d6cf72fcf488, executor driver, partition 0, PROCESS_LOCAL, 4949 bytes) taskResourceAssignments Map()\n",
            "23/05/01 18:48:54 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 2) (d6cf72fcf488, executor driver, partition 1, PROCESS_LOCAL, 4949 bytes) taskResourceAssignments Map()\n",
            "23/05/01 18:48:54 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)\n",
            "23/05/01 18:48:54 INFO Executor: Running task 1.0 in stage 1.0 (TID 2)\n",
            "23/05/01 18:48:54 INFO CodeGenerator: Code generated in 46.902336 ms\n",
            "23/05/01 18:48:55 INFO CodeGenerator: Code generated in 22.900322 ms\n",
            "23/05/01 18:48:55 INFO CodeGenerator: Code generated in 25.565072 ms\n",
            "23/05/01 18:48:55 INFO CodeGenerator: Code generated in 32.125683 ms\n",
            "23/05/01 18:48:55 INFO FileScanRDD: Reading File path: file:///content/drive/MyDrive/big-data-final/joined_weather.parquet/part-00000-69e31bd4-f6bb-42ab-bf13-ad6f2f55a1f5-c000.snappy.parquet, range: 0-71942091, partition values: [empty row]\n",
            "23/05/01 18:48:55 INFO FileScanRDD: Reading File path: file:///content/drive/MyDrive/big-data-final/joined_weather.parquet/part-00000-69e31bd4-f6bb-42ab-bf13-ad6f2f55a1f5-c000.snappy.parquet, range: 71942091-139689879, partition values: [empty row]\n",
            "23/05/01 18:48:55 INFO CodecPool: Got brand-new decompressor [.snappy]\n",
            "23/05/01 18:48:55 INFO CodecPool: Got brand-new decompressor [.snappy]\n",
            "23/05/01 18:48:56 INFO Executor: Finished task 1.0 in stage 1.0 (TID 2). 3082 bytes result sent to driver\n",
            "23/05/01 18:48:56 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 2) in 1388 ms on d6cf72fcf488 (executor driver) (1/2)\n",
            "23/05/01 18:48:56 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 3082 bytes result sent to driver\n",
            "23/05/01 18:48:56 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 1784 ms on d6cf72fcf488 (executor driver) (2/2)\n",
            "23/05/01 18:48:56 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
            "23/05/01 18:48:56 INFO DAGScheduler: ShuffleMapStage 1 (parquet at NativeMethodAccessorImpl.java:0) finished in 1.827 s\n",
            "23/05/01 18:48:56 INFO DAGScheduler: looking for newly runnable stages\n",
            "23/05/01 18:48:56 INFO DAGScheduler: running: Set()\n",
            "23/05/01 18:48:56 INFO DAGScheduler: waiting: Set()\n",
            "23/05/01 18:48:56 INFO DAGScheduler: failed: Set()\n",
            "23/05/01 18:48:56 INFO FileSourceStrategy: Pushed Filters: \n",
            "23/05/01 18:48:56 INFO FileSourceStrategy: Post-Scan Filters: \n",
            "23/05/01 18:48:56 INFO FileSourceStrategy: Output Data Schema: struct<ID_Original: string, latitude: double, longitude: double, emdCardNumber: string, time_utc: timestamp ... 59 more fields>\n",
            "23/05/01 18:48:56 INFO ShufflePartitionsUtil: For shuffle(0), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
            "23/05/01 18:48:56 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
            "23/05/01 18:48:56 INFO CodeGenerator: Code generated in 46.655724 ms\n",
            "23/05/01 18:48:56 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266\n",
            "23/05/01 18:48:56 INFO DAGScheduler: Got job 2 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) with 1 output partitions\n",
            "23/05/01 18:48:56 INFO DAGScheduler: Final stage: ResultStage 3 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266)\n",
            "23/05/01 18:48:56 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)\n",
            "23/05/01 18:48:56 INFO DAGScheduler: Missing parents: List()\n",
            "23/05/01 18:48:56 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[8] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266), which has no missing parents\n",
            "23/05/01 18:48:56 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 38.1 KiB, free 365.8 MiB)\n",
            "23/05/01 18:48:56 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 17.8 KiB, free 365.8 MiB)\n",
            "23/05/01 18:48:56 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on d6cf72fcf488:41509 (size: 17.8 KiB, free: 366.2 MiB)\n",
            "23/05/01 18:48:56 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1474\n",
            "23/05/01 18:48:56 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[8] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) (first 15 tasks are for partitions Vector(0))\n",
            "23/05/01 18:48:56 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0\n",
            "23/05/01 18:48:56 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (d6cf72fcf488, executor driver, partition 0, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()\n",
            "23/05/01 18:48:56 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)\n",
            "23/05/01 18:48:57 INFO ShuffleBlockFetcherIterator: Getting 2 (95.2 KiB) non-empty blocks including 2 (95.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/05/01 18:48:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 47 ms\n",
            "23/05/01 18:48:57 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 75047 bytes result sent to driver\n",
            "23/05/01 18:48:57 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 259 ms on d6cf72fcf488 (executor driver) (1/1)\n",
            "23/05/01 18:48:57 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
            "23/05/01 18:48:57 INFO DAGScheduler: ResultStage 3 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) finished in 0.279 s\n",
            "23/05/01 18:48:57 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "23/05/01 18:48:57 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished\n",
            "23/05/01 18:48:57 INFO DAGScheduler: Job 2 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266, took 0.321788 s\n",
            "23/05/01 18:48:57 INFO CodeGenerator: Code generated in 37.848823 ms\n",
            "23/05/01 18:48:57 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 8.3 MiB, free 357.6 MiB)\n",
            "23/05/01 18:48:57 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 114.8 KiB, free 357.4 MiB)\n",
            "23/05/01 18:48:57 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on d6cf72fcf488:41509 (size: 114.8 KiB, free: 366.1 MiB)\n",
            "23/05/01 18:48:57 INFO SparkContext: Created broadcast 4 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266\n",
            "23/05/01 18:48:57 INFO FileSourceStrategy: Pushed Filters: \n",
            "23/05/01 18:48:57 INFO FileSourceStrategy: Post-Scan Filters: \n",
            "23/05/01 18:48:57 INFO FileSourceStrategy: Output Data Schema: struct<ID_Original: string, latitude: double, longitude: double, emdCardNumber: string, time_utc: timestamp ... 59 more fields>\n",
            "23/05/01 18:48:58 INFO CodeGenerator: Code generated in 331.913107 ms\n",
            "23/05/01 18:48:58 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 367.7 KiB, free 357.1 MiB)\n",
            "23/05/01 18:48:58 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 37.5 KiB, free 357.0 MiB)\n",
            "23/05/01 18:48:58 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on d6cf72fcf488:41509 (size: 37.5 KiB, free: 366.1 MiB)\n",
            "23/05/01 18:48:58 INFO SparkContext: Created broadcast 5 from parquet at NativeMethodAccessorImpl.java:0\n",
            "23/05/01 18:48:58 INFO FileSourceScanExec: Planning scan with bin packing, max size: 71942091 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "23/05/01 18:48:58 INFO DAGScheduler: Registering RDD 13 (parquet at NativeMethodAccessorImpl.java:0) as input to shuffle 1\n",
            "23/05/01 18:48:58 INFO DAGScheduler: Got map stage job 3 (parquet at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
            "23/05/01 18:48:58 INFO DAGScheduler: Final stage: ShuffleMapStage 4 (parquet at NativeMethodAccessorImpl.java:0)\n",
            "23/05/01 18:48:58 INFO DAGScheduler: Parents of final stage: List()\n",
            "23/05/01 18:48:58 INFO DAGScheduler: Missing parents: List()\n",
            "23/05/01 18:48:58 INFO DAGScheduler: Submitting ShuffleMapStage 4 (MapPartitionsRDD[13] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/05/01 18:48:58 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 124.0 KiB, free 356.9 MiB)\n",
            "23/05/01 18:48:58 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 38.1 KiB, free 356.9 MiB)\n",
            "23/05/01 18:48:58 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on d6cf72fcf488:41509 (size: 38.1 KiB, free: 366.0 MiB)\n",
            "23/05/01 18:48:58 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1474\n",
            "23/05/01 18:48:58 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 4 (MapPartitionsRDD[13] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
            "23/05/01 18:48:58 INFO TaskSchedulerImpl: Adding task set 4.0 with 2 tasks resource profile 0\n",
            "23/05/01 18:48:58 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (d6cf72fcf488, executor driver, partition 0, PROCESS_LOCAL, 4949 bytes) taskResourceAssignments Map()\n",
            "23/05/01 18:48:58 INFO TaskSetManager: Starting task 1.0 in stage 4.0 (TID 5) (d6cf72fcf488, executor driver, partition 1, PROCESS_LOCAL, 4949 bytes) taskResourceAssignments Map()\n",
            "23/05/01 18:48:58 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)\n",
            "23/05/01 18:48:58 INFO Executor: Running task 1.0 in stage 4.0 (TID 5)\n",
            "23/05/01 18:48:58 INFO CodeGenerator: Code generated in 47.081994 ms\n",
            "23/05/01 18:48:58 INFO CodeGenerator: Code generated in 28.312868 ms\n",
            "23/05/01 18:48:58 INFO FileScanRDD: Reading File path: file:///content/drive/MyDrive/big-data-final/joined_weather.parquet/part-00000-69e31bd4-f6bb-42ab-bf13-ad6f2f55a1f5-c000.snappy.parquet, range: 71942091-139689879, partition values: [empty row]\n",
            "23/05/01 18:48:58 INFO FileScanRDD: Reading File path: file:///content/drive/MyDrive/big-data-final/joined_weather.parquet/part-00000-69e31bd4-f6bb-42ab-bf13-ad6f2f55a1f5-c000.snappy.parquet, range: 0-71942091, partition values: [empty row]\n",
            "23/05/01 18:48:59 INFO BlockManagerInfo: Removed broadcast_3_piece0 on d6cf72fcf488:41509 in memory (size: 17.8 KiB, free: 366.1 MiB)\n",
            "23/05/01 18:49:00 INFO CodeGenerator: Code generated in 160.525633 ms\n",
            "23/05/01 18:49:01 INFO CodeGenerator: Code generated in 311.481584 ms\n",
            "23/05/01 18:49:01 INFO CodeGenerator: Code generated in 131.646292 ms\n",
            "23/05/01 18:49:02 INFO BlockManagerInfo: Removed broadcast_2_piece0 on d6cf72fcf488:41509 in memory (size: 18.5 KiB, free: 366.1 MiB)\n",
            "23/05/01 18:49:03 INFO Executor: Finished task 1.0 in stage 4.0 (TID 5). 5564 bytes result sent to driver\n",
            "23/05/01 18:49:03 INFO TaskSetManager: Finished task 1.0 in stage 4.0 (TID 5) in 4760 ms on d6cf72fcf488 (executor driver) (1/2)\n",
            "23/05/01 18:49:09 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 5564 bytes result sent to driver\n",
            "23/05/01 18:49:09 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 11075 ms on d6cf72fcf488 (executor driver) (2/2)\n",
            "23/05/01 18:49:09 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool \n",
            "23/05/01 18:49:09 INFO DAGScheduler: ShuffleMapStage 4 (parquet at NativeMethodAccessorImpl.java:0) finished in 11.178 s\n",
            "23/05/01 18:49:09 INFO DAGScheduler: looking for newly runnable stages\n",
            "23/05/01 18:49:09 INFO DAGScheduler: running: Set()\n",
            "23/05/01 18:49:09 INFO DAGScheduler: waiting: Set()\n",
            "23/05/01 18:49:09 INFO DAGScheduler: failed: Set()\n",
            "23/05/01 18:49:09 INFO ShufflePartitionsUtil: For shuffle(1), advisory target size: 67108864, actual target size 1225900, minimum partition size: 1048576\n",
            "23/05/01 18:49:09 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "23/05/01 18:49:09 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "23/05/01 18:49:09 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "23/05/01 18:49:09 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "23/05/01 18:49:09 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "23/05/01 18:49:09 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "23/05/01 18:49:09 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0\n",
            "23/05/01 18:49:09 INFO DAGScheduler: Got job 4 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "23/05/01 18:49:09 INFO DAGScheduler: Final stage: ResultStage 6 (parquet at NativeMethodAccessorImpl.java:0)\n",
            "23/05/01 18:49:09 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 5)\n",
            "23/05/01 18:49:09 INFO DAGScheduler: Missing parents: List()\n",
            "23/05/01 18:49:09 INFO DAGScheduler: Submitting ResultStage 6 (CoalescedRDD[17] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/05/01 18:49:09 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 325.0 KiB, free 356.7 MiB)\n",
            "23/05/01 18:49:09 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 112.8 KiB, free 356.6 MiB)\n",
            "23/05/01 18:49:09 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on d6cf72fcf488:41509 (size: 112.8 KiB, free: 366.0 MiB)\n",
            "23/05/01 18:49:09 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1474\n",
            "23/05/01 18:49:09 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (CoalescedRDD[17] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/05/01 18:49:09 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0\n",
            "23/05/01 18:49:09 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6) (d6cf72fcf488, executor driver, partition 0, NODE_LOCAL, 4757 bytes) taskResourceAssignments Map()\n",
            "23/05/01 18:49:09 INFO Executor: Running task 0.0 in stage 6.0 (TID 6)\n",
            "23/05/01 18:49:09 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "23/05/01 18:49:09 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "23/05/01 18:49:09 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "23/05/01 18:49:09 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "23/05/01 18:49:09 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "23/05/01 18:49:09 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "23/05/01 18:49:09 INFO CodecConfig: Compression: SNAPPY\n",
            "23/05/01 18:49:09 INFO CodecConfig: Compression: SNAPPY\n",
            "23/05/01 18:49:09 INFO ParquetOutputFormat: Parquet block size to 134217728\n",
            "23/05/01 18:49:09 INFO ParquetOutputFormat: Validation is off\n",
            "23/05/01 18:49:09 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes\n",
            "23/05/01 18:49:09 INFO ParquetOutputFormat: Parquet properties are:\n",
            "Parquet page size to 1048576\n",
            "Parquet dictionary page size to 1048576\n",
            "Dictionary is true\n",
            "Writer version is: PARQUET_1_0\n",
            "Page size checking is: estimated\n",
            "Min row count for page size check is: 100\n",
            "Max row count for page size check is: 10000\n",
            "Truncate length for column indexes is: 64\n",
            "Truncate length for statistics min/max  is: 2147483647\n",
            "Bloom filter enabled: false\n",
            "Max Bloom filter size for a column is 1048576\n",
            "Bloom filter expected number of distinct values are: null\n",
            "Page row count limit to 20000\n",
            "Writing page checksums is: on\n",
            "23/05/01 18:49:10 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
            "{\n",
            "  \"type\" : \"struct\",\n",
            "  \"fields\" : [ {\n",
            "    \"name\" : \"Incident_ID\",\n",
            "    \"type\" : \"integer\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"ID_Original\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"latitude\",\n",
            "    \"type\" : \"double\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"longitude\",\n",
            "    \"type\" : \"double\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"emdCardNumber\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"time_utc\",\n",
            "    \"type\" : \"timestamp\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"time_local\",\n",
            "    \"type\" : \"timestamp\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"response_time_sec\",\n",
            "    \"type\" : \"double\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"day_of_week\",\n",
            "    \"type\" : \"long\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"weekend_or_not\",\n",
            "    \"type\" : \"integer\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"geometry\",\n",
            "    \"type\" : \"binary\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"Dist_to_Seg\",\n",
            "    \"type\" : \"double\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"XDSegID\",\n",
            "    \"type\" : \"double\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"index_right\",\n",
            "    \"type\" : \"long\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"STATEFP\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"COUNTYFP\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"TRACTCE\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"GEOID\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"NAME\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"NAMELSAD\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"MTFCC\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"FUNCSTAT\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"ALAND\",\n",
            "    \"type\" : \"long\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"AWATER\",\n",
            "    \"type\" : \"long\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"INTPTLAT\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"INTPTLON\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"__index_level_0__\",\n",
            "    \"type\" : \"long\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"station_id\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"start_date_st\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"end_date_st\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"timestamp_local\",\n",
            "    \"type\" : \"timestamp\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"rh\",\n",
            "    \"type\" : \"double\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"wind_spd\",\n",
            "    \"type\" : \"double\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"timestamp_utc\",\n",
            "    \"type\" : \"timestamp\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"pod\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"slp\",\n",
            "    \"type\" : \"double\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"app_temp\",\n",
            "    \"type\" : \"double\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"elev_angle\",\n",
            "    \"type\" : \"double\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"solar_rad\",\n",
            "    \"type\" : \"double\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"pres\",\n",
            "    \"type\" : \"double\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"h_angle\",\n",
            "    \"type\" : \"double\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"dewpt\",\n",
            "    \"type\" : \"double\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"snow\",\n",
            "    \"type\" : \"double\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"uv\",\n",
            "    \"type\" : \"double\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"azimuth\",\n",
            "    \"type\" : \"double\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"wind_dir\",\n",
            "    \"type\" : \"double\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"ghi\",\n",
            "    \"type\" : \"double\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"dhi\",\n",
            "    \"type\" : \"double\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"vis\",\n",
            "    \"type\" : \"double\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"dni\",\n",
            "    \"type\" : \"double\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"temp\",\n",
            "    \"type\" : \"double\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"precip\",\n",
            "    \"type\" : \"double\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"clouds\",\n",
            "    \"type\" : \"double\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"ts\",\n",
            "    \"type\" : \"double\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"icon\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"code\",\n",
            "    \"type\" : \"double\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"description\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"gps_coordinate_latitude\",\n",
            "    \"type\" : \"double\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"gps_coordinate_longitude\",\n",
            "    \"type\" : \"double\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"spatial_id\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"days\",\n",
            "    \"type\" : \"date\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  } ]\n",
            "}\n",
            "and corresponding Parquet message type:\n",
            "message spark_schema {\n",
            "  optional int32 Incident_ID;\n",
            "  optional binary ID_Original (STRING);\n",
            "  optional double latitude;\n",
            "  optional double longitude;\n",
            "  optional binary emdCardNumber (STRING);\n",
            "  optional int96 time_utc;\n",
            "  optional int96 time_local;\n",
            "  optional double response_time_sec;\n",
            "  optional int64 day_of_week;\n",
            "  optional int32 weekend_or_not;\n",
            "  optional binary geometry;\n",
            "  optional double Dist_to_Seg;\n",
            "  optional double XDSegID;\n",
            "  optional int64 index_right;\n",
            "  optional binary STATEFP (STRING);\n",
            "  optional binary COUNTYFP (STRING);\n",
            "  optional binary TRACTCE (STRING);\n",
            "  optional binary GEOID (STRING);\n",
            "  optional binary NAME (STRING);\n",
            "  optional binary NAMELSAD (STRING);\n",
            "  optional binary MTFCC (STRING);\n",
            "  optional binary FUNCSTAT (STRING);\n",
            "  optional int64 ALAND;\n",
            "  optional int64 AWATER;\n",
            "  optional binary INTPTLAT (STRING);\n",
            "  optional binary INTPTLON (STRING);\n",
            "  optional int64 __index_level_0__;\n",
            "  optional binary station_id (STRING);\n",
            "  optional binary start_date_st (STRING);\n",
            "  optional binary end_date_st (STRING);\n",
            "  optional int96 timestamp_local;\n",
            "  optional double rh;\n",
            "  optional double wind_spd;\n",
            "  optional int96 timestamp_utc;\n",
            "  optional binary pod (STRING);\n",
            "  optional double slp;\n",
            "  optional double app_temp;\n",
            "  optional double elev_angle;\n",
            "  optional double solar_rad;\n",
            "  optional double pres;\n",
            "  optional double h_angle;\n",
            "  optional double dewpt;\n",
            "  optional double snow;\n",
            "  optional double uv;\n",
            "  optional double azimuth;\n",
            "  optional double wind_dir;\n",
            "  optional double ghi;\n",
            "  optional double dhi;\n",
            "  optional double vis;\n",
            "  optional double dni;\n",
            "  optional double temp;\n",
            "  optional double precip;\n",
            "  optional double clouds;\n",
            "  optional double ts;\n",
            "  optional binary icon (STRING);\n",
            "  optional double code;\n",
            "  optional binary description (STRING);\n",
            "  optional double gps_coordinate_latitude;\n",
            "  optional double gps_coordinate_longitude;\n",
            "  optional binary spatial_id (STRING);\n",
            "  optional int32 days (DATE);\n",
            "}\n",
            "\n",
            "       \n",
            "23/05/01 18:49:10 INFO CodecPool: Got brand-new compressor [.snappy]\n",
            "23/05/01 18:49:10 INFO ShuffleBlockFetcherIterator: Getting 2 (1196.7 KiB) non-empty blocks including 2 (1196.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/05/01 18:49:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
            "23/05/01 18:49:10 INFO CodeGenerator: Code generated in 153.922261 ms\n",
            "23/05/01 18:49:10 INFO CodeGenerator: Code generated in 27.804295 ms\n",
            "23/05/01 18:49:10 INFO CodeGenerator: Code generated in 24.690436 ms\n",
            "23/05/01 18:49:11 INFO ShuffleBlockFetcherIterator: Getting 2 (1197.6 KiB) non-empty blocks including 2 (1197.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/05/01 18:49:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
            "23/05/01 18:49:12 INFO FileOutputCommitter: Saved output of task 'attempt_202305011849096021220740939365927_0006_m_000000_6' to file:/content/drive/MyDrive/big-data-final/jointest.parquet/_temporary/0/task_202305011849096021220740939365927_0006_m_000000\n",
            "23/05/01 18:49:12 INFO SparkHadoopMapRedUtil: attempt_202305011849096021220740939365927_0006_m_000000_6: Committed\n",
            "23/05/01 18:49:12 INFO Executor: Finished task 0.0 in stage 6.0 (TID 6). 7351 bytes result sent to driver\n",
            "23/05/01 18:49:12 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 2610 ms on d6cf72fcf488 (executor driver) (1/1)\n",
            "23/05/01 18:49:12 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool \n",
            "23/05/01 18:49:12 INFO DAGScheduler: ResultStage 6 (parquet at NativeMethodAccessorImpl.java:0) finished in 2.695 s\n",
            "23/05/01 18:49:12 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "23/05/01 18:49:12 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished\n",
            "23/05/01 18:49:12 INFO DAGScheduler: Job 4 finished: parquet at NativeMethodAccessorImpl.java:0, took 2.711482 s\n",
            "23/05/01 18:49:12 INFO FileFormatWriter: Start to commit write Job d7ec8664-1b4f-4b3c-ab28-99983b2d4ddf.\n",
            "23/05/01 18:49:12 INFO FileFormatWriter: Write Job d7ec8664-1b4f-4b3c-ab28-99983b2d4ddf committed. Elapsed time: 66 ms.\n",
            "23/05/01 18:49:12 INFO FileFormatWriter: Finished processing stats for write job d7ec8664-1b4f-4b3c-ab28-99983b2d4ddf.\n",
            "23/05/01 18:49:12 INFO SparkUI: Stopped Spark web UI at http://d6cf72fcf488:4041\n",
            "23/05/01 18:49:12 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "23/05/01 18:49:12 INFO MemoryStore: MemoryStore cleared\n",
            "23/05/01 18:49:12 INFO BlockManager: BlockManager stopped\n",
            "23/05/01 18:49:12 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "23/05/01 18:49:12 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "23/05/01 18:49:12 INFO SparkContext: Successfully stopped SparkContext\n",
            "23/05/01 18:49:12 INFO ShutdownHookManager: Shutdown hook called\n",
            "23/05/01 18:49:12 INFO ShutdownHookManager: Deleting directory /tmp/spark-96b3b921-f2ee-40ec-b0cc-7c63876bd5fc\n",
            "23/05/01 18:49:12 INFO ShutdownHookManager: Deleting directory /tmp/spark-2cc600ff-7644-4e15-905b-35875dcdd421\n",
            "23/05/01 18:49:12 INFO ShutdownHookManager: Deleting directory /tmp/spark-2cc600ff-7644-4e15-905b-35875dcdd421/pyspark-4112cf85-a2dc-4a87-9b5c-eac44a8097c5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bMEsJXTU__Dx"
      },
      "source": [
        "# Please fill your aws credential information here\n",
        "credentials = {\n",
        "    'region_name': 'us-east-1',\n",
        "    'aws_access_key_id': 'ASIA6CQF7SPNEUNHNTUW',\n",
        "    'aws_secret_access_key': 'GeLiKk+CHrTQsin8JiFqcUOFrtyuYdhJ1h9IW0UA',\n",
        "    'aws_session_token': 'FwoGZXIvYXdzELT//////////wEaDI8Iq3KJB2RHbjFKlSLOAbxoPHo+BTSxY0GIRMcElCMjo0zrv+BgU3M224csY5zDPoMwu5EbF0T6b2/mJ5uoAArZTEzy/XDWtYuqUhhMTLMixJoZVBH4Ck5ygHVdjZ6OfuvnbJtz8O3zsaWFE4gVm6DhF3X7geJaKmNzbMvfr6FdqWyJJehXDbQ1Qp1Wzr2pDZvwUsCWnoV96d89tidmlAkYReBQad9CxyBrurL4WM1l4CvRDhM6EEKnErxzZaQzCyrHIWkYT1irXh2NsGRS/WZS3S/1ZejZjvbgZK78KO2UwKIGMi0gRP6K9alKiHlzjsuNLylI/fYEUq0bwGnHJecIzO/VgsIEJYtzTyhIM5w0sus='\n",
        "}"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yRJjVwtO__Dx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c52d34ae-86f8-4473-cb5f-44785c7529b8"
      },
      "source": [
        "!pip install boto3\n",
        "import boto3, json\n",
        "\n",
        "session = boto3.session.Session(**credentials)\n",
        "s3 = session.client('s3')"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting boto3\n",
            "  Downloading boto3-1.26.123-py3-none-any.whl (135 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.6/135.6 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting botocore<1.30.0,>=1.29.123 (from boto3)\n",
            "  Downloading botocore-1.29.123-py3-none-any.whl (10.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.7/10.7 MB\u001b[0m \u001b[31m41.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1 (from boto3)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting s3transfer<0.7.0,>=0.6.0 (from boto3)\n",
            "  Downloading s3transfer-0.6.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.6/79.6 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore<1.30.0,>=1.29.123->boto3) (2.8.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /usr/local/lib/python3.10/dist-packages (from botocore<1.30.0,>=1.29.123->boto3) (1.26.15)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.30.0,>=1.29.123->boto3) (1.16.0)\n",
            "Installing collected packages: jmespath, botocore, s3transfer, boto3\n",
            "Successfully installed boto3-1.26.123 botocore-1.29.123 jmespath-1.0.1 s3transfer-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Upload Data to S3"
      ],
      "metadata": {
        "id": "Mss_SpU89_eo"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lljwHyOy__D2"
      },
      "source": [
        "# replae with your EMR cluster ID\n",
        "CLUSTER_ID = 'j-3UUU6CMFFEWGH'\n",
        "\n",
        "def submit_job(app_name, pyfile_uri):\n",
        "    emr = session.client('emr')\n",
        "    emr.add_job_flow_steps(JobFlowId=CLUSTER_ID, Steps=[{\n",
        "        'Name': app_name,\n",
        "        'ActionOnFailure': 'CANCEL_AND_WAIT',\n",
        "        'HadoopJarStep': {\n",
        "            'Args': ['spark-submit',\n",
        "                     '--master', 'yarn',\n",
        "                     '--deploy-mode', 'cluster',\n",
        "                     pyfile_uri],\n",
        "            'Jar': 'command-runner.jar'\n",
        "        }}])"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pCzOGzTj__D1"
      },
      "source": [
        "# upload script to S3\n",
        "s3.upload_file(Filename='filter_weather.py', Bucket='bd-final', Key='scripts/filter_weather.py')"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eoqkfcEn__D1"
      },
      "source": [
        "# submit spark job to emr\n",
        "submit_job(app_name='filter_weather', pyfile_uri='s3://bd-final/scripts/filter_weather.py')"
      ],
      "execution_count": 31,
      "outputs": []
    }
  ]
}