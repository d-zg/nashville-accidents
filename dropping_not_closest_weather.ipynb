{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7-final"
    },
    "orig_nbformat": 2,
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "colab": {
      "name": "spark_jobs.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vu-topics-in-big-data-2023/Team05/blob/main/dropping_not_closest_weather.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rRZIkSqT__Dv"
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Cf9znGuYBTM",
        "outputId": "e8b3f2a6-8dc3-4349-b229-db2506f6dab0"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#install spark. we are using the one that uses hadoop as the underlying scheduler.\n",
        "!wget -q https://downloads.apache.org/spark/spark-3.2.4/spark-3.2.4-bin-hadoop3.2.tgz\n",
        "!tar xf  spark-3.2.4-bin-hadoop3.2.tgz\n",
        "!ls -l\n",
        "\n",
        "#Provides findspark.init() to make pyspark importable as a regular library.\n",
        "os.environ[\"SPARK_HOME\"] = \"spark-3.2.4-bin-hadoop3.2\"\n",
        "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.4.7 pyspark-shell'"
      ],
      "metadata": {
        "id": "_BcbpaYeKNlG",
        "outputId": "3764809f-771d-4c81-bcfe-0bfa57648b52",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 882416\n",
            "drwx------  6 root root      4096 May  1 01:34 drive\n",
            "-rw-r--r--  1 root root      1175 May  1 02:02 join.py\n",
            "drwxr-xr-x  1 root root      4096 Apr 27 13:35 sample_data\n",
            "-rw-r--r--  1 root root      1701 May  1 02:01 schema.py\n",
            "drwxr-xr-x 13 1000 1000      4096 Apr  9 21:17 spark-3.2.4-bin-hadoop3.2\n",
            "-rw-r--r--  1 root root 301183180 Apr  9 21:35 spark-3.2.4-bin-hadoop3.2.tgz\n",
            "-rw-r--r--  1 root root 301183180 Apr  9 21:35 spark-3.2.4-bin-hadoop3.2.tgz.1\n",
            "-rw-r--r--  1 root root 301183180 Apr  9 21:35 spark-3.2.4-bin-hadoop3.2.tgz.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q findspark pyspark\n",
        "import findspark\n",
        "findspark.init()"
      ],
      "metadata": {
        "id": "xB4TPZH16jmR"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install geopandas "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YHM9yyXtT6rG",
        "outputId": "dabd2a88-f9ec-4167-b172-6b58b3bdf1c3"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: geopandas in /usr/local/lib/python3.10/dist-packages (0.12.2)\n",
            "Requirement already satisfied: pandas>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from geopandas) (1.5.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from geopandas) (23.1)\n",
            "Requirement already satisfied: shapely>=1.7 in /usr/local/lib/python3.10/dist-packages (from geopandas) (2.0.1)\n",
            "Requirement already satisfied: fiona>=1.8 in /usr/local/lib/python3.10/dist-packages (from geopandas) (1.9.3)\n",
            "Requirement already satisfied: pyproj>=2.6.1.post1 in /usr/local/lib/python3.10/dist-packages (from geopandas) (3.5.0)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.10/dist-packages (from fiona>=1.8->geopandas) (23.1.0)\n",
            "Requirement already satisfied: munch>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from fiona>=1.8->geopandas) (2.5.0)\n",
            "Requirement already satisfied: click~=8.0 in /usr/local/lib/python3.10/dist-packages (from fiona>=1.8->geopandas) (8.1.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from fiona>=1.8->geopandas) (2022.12.7)\n",
            "Requirement already satisfied: cligj>=0.5 in /usr/local/lib/python3.10/dist-packages (from fiona>=1.8->geopandas) (0.7.2)\n",
            "Requirement already satisfied: click-plugins>=1.0 in /usr/local/lib/python3.10/dist-packages (from fiona>=1.8->geopandas) (1.1.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.0->geopandas) (1.22.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.0->geopandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.0->geopandas) (2022.7.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from munch>=2.3.2->fiona>=1.8->geopandas) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install geospark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gUcTxDt2tLKH",
        "outputId": "73428cee-834a-4759-e589-acbb4b2b7e9d"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: geospark in /usr/local/lib/python3.10/dist-packages (1.3.1)\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (from geospark) (3.4.0)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.10/dist-packages (from geospark) (23.1.0)\n",
            "Requirement already satisfied: shapely in /usr/local/lib/python3.10/dist-packages (from geospark) (2.0.1)\n",
            "Requirement already satisfied: findspark in /usr/local/lib/python3.10/dist-packages (from geospark) (2.0.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark->geospark) (0.10.9.7)\n",
            "Requirement already satisfied: numpy>=1.14 in /usr/local/lib/python3.10/dist-packages (from shapely->geospark) (1.22.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import date_format\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "spark = SparkSession.builder.appName(\"ParquetReader\").getOrCreate()\n",
        "weather_df = spark.read.option(\"recursiveFileLookup\", \"true\").parquet(\"/content/drive/MyDrive/big-data-final/weather_tn.parquet\")\n",
        "\n",
        "# read a single parquet file called incidents\n",
        "incidents_df = spark.read.parquet(\"/content/drive/MyDrive/big-data-final/tracts_joined_with_points.parquet\")\n",
        "\n",
        "weather_sampled_df = weather_df.sample(fraction=0.0001, seed=42)\n",
        "\n",
        "# select a random subset of rows from the weather_sampled_df dataframe\n",
        "random_rows = weather_sampled_df.sample(fraction=0.01, seed=42)\n",
        "\n",
        "# select the datetime column and show some random values\n",
        "random_rows.select(\"datetime\").show(10)\n",
        "\n",
        "incidents_df.select(\"time_local\").show(1, 1000)\n",
        "\n",
        "df_with_new_column = incidents_df.withColumn(\n",
        "    \"date_local\",\n",
        "    date_format(\"time_local\", \"yyyy-MM-dd:HH\")\n",
        "        .cast(\"string\")\n",
        ")\n",
        "\n",
        "df_with_new_column = df_with_new_column.withColumnRenamed(\"date_local\", \"datetime\")\n",
        "\n",
        "df_with_new_column = df_with_new_column.repartition(col(\"datetime\"))\n",
        "\n",
        "weather_sampled_df = weather_sampled_df.repartition(col(\"datetime\"))\n",
        "\n",
        "# join the two dataframes on the date_local and datetime columns\n",
        "joined_df = df_with_new_column.join(\n",
        "    weather_sampled_df,\n",
        "    df_with_new_column[\"datetime\"] == weather_sampled_df[\"datetime\"],\n",
        "    \"inner\"\n",
        ")\n",
        "joined_df = joined_df.drop('datetime')\n",
        "joined_df.write.parquet(\"/content/joinedtest.parquet\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3lRu2xbTI46P",
        "outputId": "f774751a-d99a-4c6c-9cfd-dfd78aadc119"
      },
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+\n",
            "|     datetime|\n",
            "+-------------+\n",
            "|2013-07-24:09|\n",
            "|2016-07-18:20|\n",
            "|2012-07-24:23|\n",
            "|2012-05-29:02|\n",
            "|2015-07-12:23|\n",
            "|2017-08-23:00|\n",
            "|2011-08-23:15|\n",
            "|2020-04-01:06|\n",
            "|2020-04-05:07|\n",
            "|2018-04-15:03|\n",
            "+-------------+\n",
            "only showing top 10 rows\n",
            "\n",
            "+-----------------------+\n",
            "|             time_local|\n",
            "+-----------------------+\n",
            "|2017-01-01 01:59:29.507|\n",
            "+-----------------------+\n",
            "only showing top 1 row\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import date_format\n",
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import DoubleType\n",
        "from pyspark.sql.functions import struct\n",
        "\n",
        "spark = SparkSession.builder.appName(\"ParquetReader\").getOrCreate()\n",
        "weather_df = spark.read.parquet(\"/content/drive/MyDrive/big-data-final/joined_weather.parquet\")\n",
        "\n",
        "df = weather_df.sample(fraction=0.0001, seed=42)\n",
        "\n",
        "# Step 1: Define UDF for Euclidean distance calculation\n",
        "euclidean_distance_udf = udf(lambda lon1, lat1, lon2, lat2: ((lon1 - lon2)**2 + (lat1 - lat2)**2)**0.5, DoubleType())\n",
        "\n",
        "# Step 2: Group DataFrame by Incident_ID\n",
        "grouped_df = df.groupBy('Incident_ID')\n",
        "\n",
        "# Step 3: Define function to return row with lowest Euclidean distance in each group\n",
        "def get_min_distance_row(group_df):\n",
        "    # Calculate Euclidean distance for each row in the group\n",
        "    distance_col = euclidean_distance_udf(\n",
        "        group_df['longitude'], group_df['latitude'], \n",
        "        group_df['gps_coordinate_longitude'], group_df['gps_coordinate_latitude']\n",
        "    ).alias('distance')\n",
        "\n",
        "    # Create a struct column with the distance and all original columns\n",
        "    all_columns = [struct(group_df[col]).alias(col) for col in group_df.columns]\n",
        "    all_columns.append(distance_col)\n",
        "    struct_col = struct(all_columns).alias('all_columns')\n",
        "\n",
        "    # Select the row with the lowest Euclidean distance\n",
        "    return group_df.select(struct_col).orderBy('distance').limit(1)\n",
        "\n",
        "# Step 4: Apply function to each group and collect resulting rows into new DataFrame\n",
        "min_distance_df = grouped_df.applyInPandas(get_min_distance_row, schema=df.schema)"
      ],
      "metadata": {
        "id": "qb3-zHmqxG12",
        "outputId": "2b678510-bfc5-43c5-b593-b423ddd8b6f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        }
      },
      "execution_count": 188,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Exception while sending command.\n",
            "Traceback (most recent call last):\n",
            "  File \"spark-3.2.4-bin-hadoop3.2/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py\", line 516, in send_command\n",
            "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
            "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"spark-3.2.4-bin-hadoop3.2/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
            "    response = connection.send_command(command)\n",
            "  File \"spark-3.2.4-bin-hadoop3.2/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py\", line 539, in send_command\n",
            "    raise Py4JNetworkError(\n",
            "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "Py4JError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-188-14543a3b4865>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstruct\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mspark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ParquetReader\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mweather_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/big-data-final/joined_weather.parquet\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.2.4-bin-hadoop3.2/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    231\u001b[0m                     \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m                     \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msessionState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetConfString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mspark-3.2.4-bin-hadoop3.2/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.2.4-bin-hadoop3.2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mspark-3.2.4-bin-hadoop3.2/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    332\u001b[0m                     format(target_id, \".\", name, value))\n\u001b[1;32m    333\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m             raise Py4JError(\n\u001b[0m\u001b[1;32m    335\u001b[0m                 \u001b[0;34m\"An error occurred while calling {0}{1}{2}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m                 format(target_id, \".\", name))\n",
            "\u001b[0;31mPy4JError\u001b[0m: An error occurred while calling o25.sessionState"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "min_distance_df.head()"
      ],
      "metadata": {
        "id": "gMflZz8dzlXJ",
        "outputId": "7b5490d8-38bf-4a88-a463-d8194d45b2e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 780
        }
      },
      "execution_count": 186,
      "outputs": [
        {
          "output_type": "error",
          "ename": "PythonException",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPythonException\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-186-c242e66bdf7c>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmin_distance_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/spark-3.2.4-bin-hadoop3.2/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mhead\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m   1601\u001b[0m         \"\"\"\n\u001b[1;32m   1602\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1603\u001b[0;31m             \u001b[0mrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1604\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mrs\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1605\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.2.4-bin-hadoop3.2/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mhead\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m   1603\u001b[0m             \u001b[0mrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1604\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mrs\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1605\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1607\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfirst\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.2.4-bin-hadoop3.2/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m    742\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Alice'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Bob'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m         \"\"\"\n\u001b[0;32m--> 744\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    745\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtail\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.2.4-bin-hadoop3.2/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    691\u001b[0m         \"\"\"\n\u001b[1;32m    692\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 693\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectToPython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    694\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBatchedSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPickleSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mspark-3.2.4-bin-hadoop3.2/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.2.4-bin-hadoop3.2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"<ipython-input-184-14543a3b4865>\", line 25, in get_min_distance_row\n  File \"/content/spark-3.2.4-bin-hadoop3.2/python/pyspark/sql/udf.py\", line 318, in wrapper\n    ... def add_one(s: pd.Series) -> pd.Series:\n  File \"/content/spark-3.2.4-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/sql/udf.py\", line 177, in __call__\n    judf = self._judf\n  File \"/content/spark-3.2.4-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/sql/udf.py\", line 161, in _judf\n    self._judf_placeholder = self._create_judf()\n  File \"/content/spark-3.2.4-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/sql/udf.py\", line 167, in _create_judf\n    spark = SparkSession.builder.getOrCreate()\n  File \"/content/spark-3.2.4-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/sql/session.py\", line 228, in getOrCreate\n    sc = SparkContext.getOrCreate(sparkConf)\n  File \"/content/spark-3.2.4-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/context.py\", line 392, in getOrCreate\n    SparkContext(conf=conf or SparkConf())\n  File \"/content/spark-3.2.4-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/context.py\", line 136, in __init__\n    SparkContext._assert_on_driver()\n  File \"/content/spark-3.2.4-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/context.py\", line 1271, in _assert_on_driver\n    raise RuntimeError(\"SparkContext should only be created and accessed on the driver.\")\nRuntimeError: SparkContext should only be created and accessed on the driver.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pdf.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 716
        },
        "id": "bui44qPMNgJG",
        "outputId": "3fca50bd-5046-4ae1-9ee5-b6bd17238749"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                          ID_Original   latitude  longitude emdCardNumber  \\\n",
              "0  ObjectId(5c607bae95fad01b9df02c82)  35.999451 -86.693431          29B1   \n",
              "1  ObjectId(5c60761d95fad01b9df02c79)  36.238125 -86.783028         29D2P   \n",
              "2  ObjectId(5cf1bb8195fad0113178cbc8)  36.119166 -86.728632         29D2P   \n",
              "3  ObjectId(59d3a99008f47311c8922ceb)  36.263097 -86.690496          29B1   \n",
              "4  ObjectId(59d3a99008f47311c8922ce8)  36.151630 -86.768547          29A2   \n",
              "\n",
              "                 time_utc              time_local  response_time_sec  \\\n",
              "0 2019-02-10 19:25:17.730 2019-02-10 13:25:17.730              707.0   \n",
              "1 2019-02-10 19:02:53.547 2019-02-10 13:02:53.547              323.0   \n",
              "2 2019-05-31 23:38:11.593 2019-05-31 18:38:11.593              359.0   \n",
              "3 2017-02-25 17:10:38.000 2017-02-25 11:10:38.000              258.0   \n",
              "4 2017-02-25 17:00:09.000 2017-02-25 11:00:09.000              275.0   \n",
              "\n",
              "   day_of_week  weekend_or_not  \\\n",
              "0            6               1   \n",
              "1            6               1   \n",
              "2            4               0   \n",
              "3            5               1   \n",
              "4            5               1   \n",
              "\n",
              "                                            geometry  ...  precip  clouds  \\\n",
              "0  [1, 1, 0, 0, 0, 123, 39, 110, 46, 97, 172, 85,...  ...     0.5   100.0   \n",
              "1  [1, 1, 0, 0, 0, 185, 24, 135, 35, 29, 178, 85,...  ...     0.5   100.0   \n",
              "2  [1, 1, 0, 0, 0, 48, 206, 198, 231, 161, 174, 8...  ...     0.0    68.0   \n",
              "3  [1, 1, 0, 0, 0, 151, 67, 197, 23, 49, 172, 85,...  ...     3.0   100.0   \n",
              "4  [1, 1, 0, 0, 0, 86, 37, 120, 224, 47, 177, 85,...  ...     3.0   100.0   \n",
              "\n",
              "             ts  icon   code      description gps_coordinate_latitude  \\\n",
              "0  1.549804e+09  c04n  804.0  Overcast clouds                  36.617   \n",
              "1  1.549804e+09  c04n  804.0  Overcast clouds                  36.617   \n",
              "2  1.559326e+09  c03d  803.0    Broken clouds                  35.030   \n",
              "3  1.488020e+09  r01n  500.0       Light rain                  36.859   \n",
              "4  1.488020e+09  r01n  500.0       Light rain                  36.859   \n",
              "\n",
              "  gps_coordinate_longitude spatial_id        days  \n",
              "0                  -87.417  Oak Grove  2019-02-10  \n",
              "1                  -87.417  Oak Grove  2019-02-10  \n",
              "2                  -85.200  Ridgeside  2019-05-31  \n",
              "3                  -83.358     Loyall  2017-02-25  \n",
              "4                  -83.358     Loyall  2017-02-25  \n",
              "\n",
              "[5 rows x 63 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-174717a0-d0ce-409d-9148-af27a5d292e9\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID_Original</th>\n",
              "      <th>latitude</th>\n",
              "      <th>longitude</th>\n",
              "      <th>emdCardNumber</th>\n",
              "      <th>time_utc</th>\n",
              "      <th>time_local</th>\n",
              "      <th>response_time_sec</th>\n",
              "      <th>day_of_week</th>\n",
              "      <th>weekend_or_not</th>\n",
              "      <th>geometry</th>\n",
              "      <th>...</th>\n",
              "      <th>precip</th>\n",
              "      <th>clouds</th>\n",
              "      <th>ts</th>\n",
              "      <th>icon</th>\n",
              "      <th>code</th>\n",
              "      <th>description</th>\n",
              "      <th>gps_coordinate_latitude</th>\n",
              "      <th>gps_coordinate_longitude</th>\n",
              "      <th>spatial_id</th>\n",
              "      <th>days</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ObjectId(5c607bae95fad01b9df02c82)</td>\n",
              "      <td>35.999451</td>\n",
              "      <td>-86.693431</td>\n",
              "      <td>29B1</td>\n",
              "      <td>2019-02-10 19:25:17.730</td>\n",
              "      <td>2019-02-10 13:25:17.730</td>\n",
              "      <td>707.0</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>[1, 1, 0, 0, 0, 123, 39, 110, 46, 97, 172, 85,...</td>\n",
              "      <td>...</td>\n",
              "      <td>0.5</td>\n",
              "      <td>100.0</td>\n",
              "      <td>1.549804e+09</td>\n",
              "      <td>c04n</td>\n",
              "      <td>804.0</td>\n",
              "      <td>Overcast clouds</td>\n",
              "      <td>36.617</td>\n",
              "      <td>-87.417</td>\n",
              "      <td>Oak Grove</td>\n",
              "      <td>2019-02-10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ObjectId(5c60761d95fad01b9df02c79)</td>\n",
              "      <td>36.238125</td>\n",
              "      <td>-86.783028</td>\n",
              "      <td>29D2P</td>\n",
              "      <td>2019-02-10 19:02:53.547</td>\n",
              "      <td>2019-02-10 13:02:53.547</td>\n",
              "      <td>323.0</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>[1, 1, 0, 0, 0, 185, 24, 135, 35, 29, 178, 85,...</td>\n",
              "      <td>...</td>\n",
              "      <td>0.5</td>\n",
              "      <td>100.0</td>\n",
              "      <td>1.549804e+09</td>\n",
              "      <td>c04n</td>\n",
              "      <td>804.0</td>\n",
              "      <td>Overcast clouds</td>\n",
              "      <td>36.617</td>\n",
              "      <td>-87.417</td>\n",
              "      <td>Oak Grove</td>\n",
              "      <td>2019-02-10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ObjectId(5cf1bb8195fad0113178cbc8)</td>\n",
              "      <td>36.119166</td>\n",
              "      <td>-86.728632</td>\n",
              "      <td>29D2P</td>\n",
              "      <td>2019-05-31 23:38:11.593</td>\n",
              "      <td>2019-05-31 18:38:11.593</td>\n",
              "      <td>359.0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>[1, 1, 0, 0, 0, 48, 206, 198, 231, 161, 174, 8...</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>68.0</td>\n",
              "      <td>1.559326e+09</td>\n",
              "      <td>c03d</td>\n",
              "      <td>803.0</td>\n",
              "      <td>Broken clouds</td>\n",
              "      <td>35.030</td>\n",
              "      <td>-85.200</td>\n",
              "      <td>Ridgeside</td>\n",
              "      <td>2019-05-31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ObjectId(59d3a99008f47311c8922ceb)</td>\n",
              "      <td>36.263097</td>\n",
              "      <td>-86.690496</td>\n",
              "      <td>29B1</td>\n",
              "      <td>2017-02-25 17:10:38.000</td>\n",
              "      <td>2017-02-25 11:10:38.000</td>\n",
              "      <td>258.0</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>[1, 1, 0, 0, 0, 151, 67, 197, 23, 49, 172, 85,...</td>\n",
              "      <td>...</td>\n",
              "      <td>3.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>1.488020e+09</td>\n",
              "      <td>r01n</td>\n",
              "      <td>500.0</td>\n",
              "      <td>Light rain</td>\n",
              "      <td>36.859</td>\n",
              "      <td>-83.358</td>\n",
              "      <td>Loyall</td>\n",
              "      <td>2017-02-25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ObjectId(59d3a99008f47311c8922ce8)</td>\n",
              "      <td>36.151630</td>\n",
              "      <td>-86.768547</td>\n",
              "      <td>29A2</td>\n",
              "      <td>2017-02-25 17:00:09.000</td>\n",
              "      <td>2017-02-25 11:00:09.000</td>\n",
              "      <td>275.0</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>[1, 1, 0, 0, 0, 86, 37, 120, 224, 47, 177, 85,...</td>\n",
              "      <td>...</td>\n",
              "      <td>3.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>1.488020e+09</td>\n",
              "      <td>r01n</td>\n",
              "      <td>500.0</td>\n",
              "      <td>Light rain</td>\n",
              "      <td>36.859</td>\n",
              "      <td>-83.358</td>\n",
              "      <td>Loyall</td>\n",
              "      <td>2017-02-25</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 63 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-174717a0-d0ce-409d-9148-af27a5d292e9')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-174717a0-d0ce-409d-9148-af27a5d292e9 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-174717a0-d0ce-409d-9148-af27a5d292e9');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3y2A34y__Dz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ce0978a-7623-4c28-d9ba-77e54aaa8a16"
      },
      "source": [
        "# execute locally and ensure everything works. If it works you should get the 1_count.out/part-00000 file. \n",
        "!spark-submit --packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.4.7 join_weather.py\n",
        "# note the cell magic command %%file 1_count.py is used to create a local copy of the content of cell as a file 1_count.py on colab"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ":: loading settings :: url = jar:file:/content/spark-3.2.4-bin-hadoop3.2/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
            "Ivy Default Cache set to: /root/.ivy2/cache\n",
            "The jars for the packages stored in: /root/.ivy2/jars\n",
            "org.apache.spark#spark-streaming-kafka-0-8_2.11 added as a dependency\n",
            ":: resolving dependencies :: org.apache.spark#spark-submit-parent-00d8d5e3-1606-49e4-8531-a7de9e11cab3;1.0\n",
            "\tconfs: [default]\n",
            "\tfound org.apache.spark#spark-streaming-kafka-0-8_2.11;2.4.7 in central\n",
            "\tfound org.apache.kafka#kafka_2.11;0.8.2.1 in central\n",
            "\tfound org.scala-lang.modules#scala-xml_2.11;1.0.2 in central\n",
            "\tfound com.yammer.metrics#metrics-core;2.2.0 in central\n",
            "\tfound org.slf4j#slf4j-api;1.7.16 in central\n",
            "\tfound org.scala-lang.modules#scala-parser-combinators_2.11;1.1.0 in central\n",
            "\tfound com.101tec#zkclient;0.3 in central\n",
            "\tfound log4j#log4j;1.2.17 in central\n",
            "\tfound org.apache.kafka#kafka-clients;0.8.2.1 in central\n",
            "\tfound net.jpountz.lz4#lz4;1.2.0 in central\n",
            "\tfound org.xerial.snappy#snappy-java;1.1.7.5 in central\n",
            "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
            ":: resolution report :: resolve 1464ms :: artifacts dl 62ms\n",
            "\t:: modules in use:\n",
            "\tcom.101tec#zkclient;0.3 from central in [default]\n",
            "\tcom.yammer.metrics#metrics-core;2.2.0 from central in [default]\n",
            "\tlog4j#log4j;1.2.17 from central in [default]\n",
            "\tnet.jpountz.lz4#lz4;1.2.0 from central in [default]\n",
            "\torg.apache.kafka#kafka-clients;0.8.2.1 from central in [default]\n",
            "\torg.apache.kafka#kafka_2.11;0.8.2.1 from central in [default]\n",
            "\torg.apache.spark#spark-streaming-kafka-0-8_2.11;2.4.7 from central in [default]\n",
            "\torg.scala-lang.modules#scala-parser-combinators_2.11;1.1.0 from central in [default]\n",
            "\torg.scala-lang.modules#scala-xml_2.11;1.0.2 from central in [default]\n",
            "\torg.slf4j#slf4j-api;1.7.16 from central in [default]\n",
            "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
            "\torg.xerial.snappy#snappy-java;1.1.7.5 from central in [default]\n",
            "\t---------------------------------------------------------------------\n",
            "\t|                  |            modules            ||   artifacts   |\n",
            "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
            "\t---------------------------------------------------------------------\n",
            "\t|      default     |   12  |   0   |   0   |   0   ||   12  |   0   |\n",
            "\t---------------------------------------------------------------------\n",
            ":: retrieving :: org.apache.spark#spark-submit-parent-00d8d5e3-1606-49e4-8531-a7de9e11cab3\n",
            "\tconfs: [default]\n",
            "\t0 artifacts copied, 12 already retrieved (0kB/39ms)\n",
            "23/05/01 02:51:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
            "23/05/01 02:51:57 INFO SparkContext: Running Spark version 3.2.4\n",
            "23/05/01 02:51:57 INFO ResourceUtils: ==============================================================\n",
            "23/05/01 02:51:57 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "23/05/01 02:51:57 INFO ResourceUtils: ==============================================================\n",
            "23/05/01 02:51:57 INFO SparkContext: Submitted application: join_weather\n",
            "23/05/01 02:51:57 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "23/05/01 02:51:57 INFO ResourceProfile: Limiting resource is cpu\n",
            "23/05/01 02:51:57 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "23/05/01 02:51:57 INFO SecurityManager: Changing view acls to: root\n",
            "23/05/01 02:51:57 INFO SecurityManager: Changing modify acls to: root\n",
            "23/05/01 02:51:57 INFO SecurityManager: Changing view acls groups to: \n",
            "23/05/01 02:51:57 INFO SecurityManager: Changing modify acls groups to: \n",
            "23/05/01 02:51:57 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\n",
            "23/05/01 02:51:58 INFO Utils: Successfully started service 'sparkDriver' on port 36985.\n",
            "23/05/01 02:51:58 INFO SparkEnv: Registering MapOutputTracker\n",
            "23/05/01 02:51:58 INFO SparkEnv: Registering BlockManagerMaster\n",
            "23/05/01 02:51:58 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "23/05/01 02:51:58 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "23/05/01 02:51:58 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "23/05/01 02:51:58 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-ca28b09b-7788-49a5-a554-00c265626cf3\n",
            "23/05/01 02:51:58 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB\n",
            "23/05/01 02:51:58 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "23/05/01 02:51:59 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
            "23/05/01 02:51:59 INFO Utils: Successfully started service 'SparkUI' on port 4041.\n",
            "23/05/01 02:51:59 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://a39e9f25f235:4041\n",
            "23/05/01 02:51:59 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar at spark://a39e9f25f235:36985/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1682909517067\n",
            "23/05/01 02:51:59 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar at spark://a39e9f25f235:36985/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1682909517067\n",
            "23/05/01 02:51:59 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar at spark://a39e9f25f235:36985/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1682909517067\n",
            "23/05/01 02:51:59 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar at spark://a39e9f25f235:36985/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1682909517067\n",
            "23/05/01 02:51:59 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar at spark://a39e9f25f235:36985/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1682909517067\n",
            "23/05/01 02:51:59 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar at spark://a39e9f25f235:36985/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1682909517067\n",
            "23/05/01 02:51:59 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar at spark://a39e9f25f235:36985/jars/com.101tec_zkclient-0.3.jar with timestamp 1682909517067\n",
            "23/05/01 02:51:59 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar at spark://a39e9f25f235:36985/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1682909517067\n",
            "23/05/01 02:51:59 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar at spark://a39e9f25f235:36985/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1682909517067\n",
            "23/05/01 02:51:59 INFO SparkContext: Added JAR file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar at spark://a39e9f25f235:36985/jars/log4j_log4j-1.2.17.jar with timestamp 1682909517067\n",
            "23/05/01 02:51:59 INFO SparkContext: Added JAR file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar at spark://a39e9f25f235:36985/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1682909517067\n",
            "23/05/01 02:51:59 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar at spark://a39e9f25f235:36985/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1682909517067\n",
            "23/05/01 02:51:59 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar at file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1682909517067\n",
            "23/05/01 02:51:59 INFO Utils: Copying /root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar to /tmp/spark-37ed4284-9c8e-42f5-af9c-e583914cc73d/userFiles-8f407a3a-f7ad-4414-b75c-315e3e68d965/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar\n",
            "23/05/01 02:51:59 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar at file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1682909517067\n",
            "23/05/01 02:51:59 INFO Utils: Copying /root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar to /tmp/spark-37ed4284-9c8e-42f5-af9c-e583914cc73d/userFiles-8f407a3a-f7ad-4414-b75c-315e3e68d965/org.apache.kafka_kafka_2.11-0.8.2.1.jar\n",
            "23/05/01 02:51:59 INFO SparkContext: Added file file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar at file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1682909517067\n",
            "23/05/01 02:51:59 INFO Utils: Copying /root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-37ed4284-9c8e-42f5-af9c-e583914cc73d/userFiles-8f407a3a-f7ad-4414-b75c-315e3e68d965/org.spark-project.spark_unused-1.0.0.jar\n",
            "23/05/01 02:51:59 INFO SparkContext: Added file file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar at file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1682909517067\n",
            "23/05/01 02:51:59 INFO Utils: Copying /root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar to /tmp/spark-37ed4284-9c8e-42f5-af9c-e583914cc73d/userFiles-8f407a3a-f7ad-4414-b75c-315e3e68d965/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar\n",
            "23/05/01 02:51:59 INFO SparkContext: Added file file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar at file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1682909517067\n",
            "23/05/01 02:51:59 INFO Utils: Copying /root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar to /tmp/spark-37ed4284-9c8e-42f5-af9c-e583914cc73d/userFiles-8f407a3a-f7ad-4414-b75c-315e3e68d965/com.yammer.metrics_metrics-core-2.2.0.jar\n",
            "23/05/01 02:51:59 INFO SparkContext: Added file file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar at file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1682909517067\n",
            "23/05/01 02:51:59 INFO Utils: Copying /root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar to /tmp/spark-37ed4284-9c8e-42f5-af9c-e583914cc73d/userFiles-8f407a3a-f7ad-4414-b75c-315e3e68d965/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar\n",
            "23/05/01 02:51:59 INFO SparkContext: Added file file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar at file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar with timestamp 1682909517067\n",
            "23/05/01 02:51:59 INFO Utils: Copying /root/.ivy2/jars/com.101tec_zkclient-0.3.jar to /tmp/spark-37ed4284-9c8e-42f5-af9c-e583914cc73d/userFiles-8f407a3a-f7ad-4414-b75c-315e3e68d965/com.101tec_zkclient-0.3.jar\n",
            "23/05/01 02:51:59 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar at file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1682909517067\n",
            "23/05/01 02:51:59 INFO Utils: Copying /root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar to /tmp/spark-37ed4284-9c8e-42f5-af9c-e583914cc73d/userFiles-8f407a3a-f7ad-4414-b75c-315e3e68d965/org.apache.kafka_kafka-clients-0.8.2.1.jar\n",
            "23/05/01 02:51:59 INFO SparkContext: Added file file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar at file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1682909517067\n",
            "23/05/01 02:51:59 INFO Utils: Copying /root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar to /tmp/spark-37ed4284-9c8e-42f5-af9c-e583914cc73d/userFiles-8f407a3a-f7ad-4414-b75c-315e3e68d965/org.slf4j_slf4j-api-1.7.16.jar\n",
            "23/05/01 02:51:59 INFO SparkContext: Added file file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar at file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar with timestamp 1682909517067\n",
            "23/05/01 02:51:59 INFO Utils: Copying /root/.ivy2/jars/log4j_log4j-1.2.17.jar to /tmp/spark-37ed4284-9c8e-42f5-af9c-e583914cc73d/userFiles-8f407a3a-f7ad-4414-b75c-315e3e68d965/log4j_log4j-1.2.17.jar\n",
            "23/05/01 02:51:59 INFO SparkContext: Added file file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar at file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1682909517067\n",
            "23/05/01 02:51:59 INFO Utils: Copying /root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar to /tmp/spark-37ed4284-9c8e-42f5-af9c-e583914cc73d/userFiles-8f407a3a-f7ad-4414-b75c-315e3e68d965/net.jpountz.lz4_lz4-1.2.0.jar\n",
            "23/05/01 02:51:59 INFO SparkContext: Added file file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar at file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1682909517067\n",
            "23/05/01 02:51:59 INFO Utils: Copying /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar to /tmp/spark-37ed4284-9c8e-42f5-af9c-e583914cc73d/userFiles-8f407a3a-f7ad-4414-b75c-315e3e68d965/org.xerial.snappy_snappy-java-1.1.7.5.jar\n",
            "23/05/01 02:51:59 INFO Executor: Starting executor ID driver on host a39e9f25f235\n",
            "23/05/01 02:51:59 INFO Executor: Fetching file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar with timestamp 1682909517067\n",
            "23/05/01 02:51:59 INFO Utils: /root/.ivy2/jars/log4j_log4j-1.2.17.jar has been previously copied to /tmp/spark-37ed4284-9c8e-42f5-af9c-e583914cc73d/userFiles-8f407a3a-f7ad-4414-b75c-315e3e68d965/log4j_log4j-1.2.17.jar\n",
            "23/05/01 02:51:59 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1682909517067\n",
            "23/05/01 02:52:00 INFO Utils: /root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar has been previously copied to /tmp/spark-37ed4284-9c8e-42f5-af9c-e583914cc73d/userFiles-8f407a3a-f7ad-4414-b75c-315e3e68d965/org.apache.kafka_kafka-clients-0.8.2.1.jar\n",
            "23/05/01 02:52:00 INFO Executor: Fetching file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1682909517067\n",
            "23/05/01 02:52:00 INFO Utils: /root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar has been previously copied to /tmp/spark-37ed4284-9c8e-42f5-af9c-e583914cc73d/userFiles-8f407a3a-f7ad-4414-b75c-315e3e68d965/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar\n",
            "23/05/01 02:52:00 INFO Executor: Fetching file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1682909517067\n",
            "23/05/01 02:52:00 INFO Utils: /root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar has been previously copied to /tmp/spark-37ed4284-9c8e-42f5-af9c-e583914cc73d/userFiles-8f407a3a-f7ad-4414-b75c-315e3e68d965/org.slf4j_slf4j-api-1.7.16.jar\n",
            "23/05/01 02:52:00 INFO Executor: Fetching file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1682909517067\n",
            "23/05/01 02:52:00 INFO Utils: /root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar has been previously copied to /tmp/spark-37ed4284-9c8e-42f5-af9c-e583914cc73d/userFiles-8f407a3a-f7ad-4414-b75c-315e3e68d965/net.jpountz.lz4_lz4-1.2.0.jar\n",
            "23/05/01 02:52:00 INFO Executor: Fetching file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1682909517067\n",
            "23/05/01 02:52:00 INFO Utils: /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar has been previously copied to /tmp/spark-37ed4284-9c8e-42f5-af9c-e583914cc73d/userFiles-8f407a3a-f7ad-4414-b75c-315e3e68d965/org.xerial.snappy_snappy-java-1.1.7.5.jar\n",
            "23/05/01 02:52:00 INFO Executor: Fetching file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1682909517067\n",
            "23/05/01 02:52:00 INFO Utils: /root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar has been previously copied to /tmp/spark-37ed4284-9c8e-42f5-af9c-e583914cc73d/userFiles-8f407a3a-f7ad-4414-b75c-315e3e68d965/org.spark-project.spark_unused-1.0.0.jar\n",
            "23/05/01 02:52:00 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1682909517067\n",
            "23/05/01 02:52:00 INFO Utils: /root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar has been previously copied to /tmp/spark-37ed4284-9c8e-42f5-af9c-e583914cc73d/userFiles-8f407a3a-f7ad-4414-b75c-315e3e68d965/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar\n",
            "23/05/01 02:52:00 INFO Executor: Fetching file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1682909517067\n",
            "23/05/01 02:52:00 INFO Utils: /root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar has been previously copied to /tmp/spark-37ed4284-9c8e-42f5-af9c-e583914cc73d/userFiles-8f407a3a-f7ad-4414-b75c-315e3e68d965/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar\n",
            "23/05/01 02:52:00 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1682909517067\n",
            "23/05/01 02:52:00 INFO Utils: /root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar has been previously copied to /tmp/spark-37ed4284-9c8e-42f5-af9c-e583914cc73d/userFiles-8f407a3a-f7ad-4414-b75c-315e3e68d965/org.apache.kafka_kafka_2.11-0.8.2.1.jar\n",
            "23/05/01 02:52:00 INFO Executor: Fetching file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar with timestamp 1682909517067\n",
            "23/05/01 02:52:00 INFO Utils: /root/.ivy2/jars/com.101tec_zkclient-0.3.jar has been previously copied to /tmp/spark-37ed4284-9c8e-42f5-af9c-e583914cc73d/userFiles-8f407a3a-f7ad-4414-b75c-315e3e68d965/com.101tec_zkclient-0.3.jar\n",
            "23/05/01 02:52:00 INFO Executor: Fetching file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1682909517067\n",
            "23/05/01 02:52:00 INFO Utils: /root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar has been previously copied to /tmp/spark-37ed4284-9c8e-42f5-af9c-e583914cc73d/userFiles-8f407a3a-f7ad-4414-b75c-315e3e68d965/com.yammer.metrics_metrics-core-2.2.0.jar\n",
            "23/05/01 02:52:00 INFO Executor: Fetching spark://a39e9f25f235:36985/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1682909517067\n",
            "23/05/01 02:52:00 INFO TransportClientFactory: Successfully created connection to a39e9f25f235/172.28.0.12:36985 after 66 ms (0 ms spent in bootstraps)\n",
            "23/05/01 02:52:00 INFO Utils: Fetching spark://a39e9f25f235:36985/jars/net.jpountz.lz4_lz4-1.2.0.jar to /tmp/spark-37ed4284-9c8e-42f5-af9c-e583914cc73d/userFiles-8f407a3a-f7ad-4414-b75c-315e3e68d965/fetchFileTemp1845592996888419932.tmp\n",
            "23/05/01 02:52:00 INFO Utils: /tmp/spark-37ed4284-9c8e-42f5-af9c-e583914cc73d/userFiles-8f407a3a-f7ad-4414-b75c-315e3e68d965/fetchFileTemp1845592996888419932.tmp has been previously copied to /tmp/spark-37ed4284-9c8e-42f5-af9c-e583914cc73d/userFiles-8f407a3a-f7ad-4414-b75c-315e3e68d965/net.jpountz.lz4_lz4-1.2.0.jar\n",
            "23/05/01 02:52:00 INFO Executor: Adding file:/tmp/spark-37ed4284-9c8e-42f5-af9c-e583914cc73d/userFiles-8f407a3a-f7ad-4414-b75c-315e3e68d965/net.jpountz.lz4_lz4-1.2.0.jar to class loader\n",
            "23/05/01 02:52:00 INFO Executor: Fetching spark://a39e9f25f235:36985/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1682909517067\n",
            "23/05/01 02:52:00 INFO Utils: Fetching spark://a39e9f25f235:36985/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar to /tmp/spark-37ed4284-9c8e-42f5-af9c-e583914cc73d/userFiles-8f407a3a-f7ad-4414-b75c-315e3e68d965/fetchFileTemp2438324021291985974.tmp\n",
            "23/05/01 02:52:00 INFO Utils: /tmp/spark-37ed4284-9c8e-42f5-af9c-e583914cc73d/userFiles-8f407a3a-f7ad-4414-b75c-315e3e68d965/fetchFileTemp2438324021291985974.tmp has been previously copied to /tmp/spark-37ed4284-9c8e-42f5-af9c-e583914cc73d/userFiles-8f407a3a-f7ad-4414-b75c-315e3e68d965/org.apache.kafka_kafka_2.11-0.8.2.1.jar\n",
            "23/05/01 02:52:00 INFO Executor: Adding file:/tmp/spark-37ed4284-9c8e-42f5-af9c-e583914cc73d/userFiles-8f407a3a-f7ad-4414-b75c-315e3e68d965/org.apache.kafka_kafka_2.11-0.8.2.1.jar to class loader\n",
            "23/05/01 02:52:00 INFO Executor: Fetching spark://a39e9f25f235:36985/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1682909517067\n",
            "23/05/01 02:52:00 INFO Utils: Fetching spark://a39e9f25f235:36985/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-37ed4284-9c8e-42f5-af9c-e583914cc73d/userFiles-8f407a3a-f7ad-4414-b75c-315e3e68d965/fetchFileTemp8122741103145626158.tmp\n",
            "23/05/01 02:52:00 INFO Utils: /tmp/spark-37ed4284-9c8e-42f5-af9c-e583914cc73d/userFiles-8f407a3a-f7ad-4414-b75c-315e3e68d965/fetchFileTemp8122741103145626158.tmp has been previously copied to /tmp/spark-37ed4284-9c8e-42f5-af9c-e583914cc73d/userFiles-8f407a3a-f7ad-4414-b75c-315e3e68d965/org.spark-project.spark_unused-1.0.0.jar\n",
            "23/05/01 02:52:00 INFO Executor: Adding file:/tmp/spark-37ed4284-9c8e-42f5-af9c-e583914cc73d/userFiles-8f407a3a-f7ad-4414-b75c-315e3e68d965/org.spark-project.spark_unused-1.0.0.jar to class loader\n",
            "23/05/01 02:52:00 INFO Executor: Fetching spark://a39e9f25f235:36985/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1682909517067\n",
            "23/05/01 02:52:00 INFO Utils: Fetching spark://a39e9f25f235:36985/jars/com.yammer.metrics_metrics-core-2.2.0.jar to /tmp/spark-37ed4284-9c8e-42f5-af9c-e583914cc73d/userFiles-8f407a3a-f7ad-4414-b75c-315e3e68d965/fetchFileTemp3108989460976161881.tmp\n",
            "23/05/01 02:52:00 INFO Utils: /tmp/spark-37ed4284-9c8e-42f5-af9c-e583914cc73d/userFiles-8f407a3a-f7ad-4414-b75c-315e3e68d965/fetchFileTemp3108989460976161881.tmp has been previously copied to /tmp/spark-37ed4284-9c8e-42f5-af9c-e583914cc73d/userFiles-8f407a3a-f7ad-4414-b75c-315e3e68d965/com.yammer.metrics_metrics-core-2.2.0.jar\n",
            "23/05/01 02:52:00 INFO Executor: Adding file:/tmp/spark-37ed4284-9c8e-42f5-af9c-e583914cc73d/userFiles-8f407a3a-f7ad-4414-b75c-315e3e68d965/com.yammer.metrics_metrics-core-2.2.0.jar to class loader\n",
            "23/05/01 02:52:00 INFO Executor: Fetching spark://a39e9f25f235:36985/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1682909517067\n",
            "23/05/01 02:52:00 INFO Utils: Fetching spark://a39e9f25f235:36985/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar to /tmp/spark-37ed4284-9c8e-42f5-af9c-e583914cc73d/userFiles-8f407a3a-f7ad-4414-b75c-315e3e68d965/fetchFileTemp5213761705135251514.tmp\n",
            "23/05/01 02:52:00 INFO Utils: /tmp/spark-37ed4284-9c8e-42f5-af9c-e583914cc73d/userFiles-8f407a3a-f7ad-4414-b75c-315e3e68d965/fetchFileTemp5213761705135251514.tmp has been previously copied to /tmp/spark-37ed4284-9c8e-42f5-af9c-e583914cc73d/userFiles-8f407a3a-f7ad-4414-b75c-315e3e68d965/org.apache.kafka_kafka-clients-0.8.2.1.jar\n",
            "23/05/01 02:52:00 INFO Executor: Adding file:/tmp/spark-37ed4284-9c8e-42f5-af9c-e583914cc73d/userFiles-8f407a3a-f7ad-4414-b75c-315e3e68d965/org.apache.kafka_kafka-clients-0.8.2.1.jar to class loader\n",
            "23/05/01 02:52:00 INFO Executor: Fetching spark://a39e9f25f235:36985/jars/com.101tec_zkclient-0.3.jar with timestamp 1682909517067\n",
            "23/05/01 02:52:00 INFO Utils: Fetching spark://a39e9f25f235:36985/jars/com.101tec_zkclient-0.3.jar to /tmp/spark-37ed4284-9c8e-42f5-af9c-e583914cc73d/userFiles-8f407a3a-f7ad-4414-b75c-315e3e68d965/fetchFileTemp7908771309769375352.tmp\n",
            "23/05/01 02:52:00 INFO Utils: /tmp/spark-37ed4284-9c8e-42f5-af9c-e583914cc73d/userFiles-8f407a3a-f7ad-4414-b75c-315e3e68d965/fetchFileTemp7908771309769375352.tmp has been previously copied to /tmp/spark-37ed4284-9c8e-42f5-af9c-e583914cc73d/userFiles-8f407a3a-f7ad-4414-b75c-315e3e68d965/com.101tec_zkclient-0.3.jar\n",
            "23/05/01 02:52:00 INFO Executor: Adding file:/tmp/spark-37ed4284-9c8e-42f5-af9c-e583914cc73d/userFiles-8f407a3a-f7ad-4414-b75c-315e3e68d965/com.101tec_zkclient-0.3.jar to class loader\n",
            "23/05/01 02:52:00 INFO Executor: Fetching spark://a39e9f25f235:36985/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1682909517067\n",
            "23/05/01 02:52:00 INFO Utils: Fetching spark://a39e9f25f235:36985/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar to /tmp/spark-37ed4284-9c8e-42f5-af9c-e583914cc73d/userFiles-8f407a3a-f7ad-4414-b75c-315e3e68d965/fetchFileTemp6752634748578366841.tmp\n",
            "23/05/01 02:52:00 INFO Utils: /tmp/spark-37ed4284-9c8e-42f5-af9c-e583914cc73d/userFiles-8f407a3a-f7ad-4414-b75c-315e3e68d965/fetchFileTemp6752634748578366841.tmp has been previously copied to /tmp/spark-37ed4284-9c8e-42f5-af9c-e583914cc73d/userFiles-8f407a3a-f7ad-4414-b75c-315e3e68d965/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar\n",
            "23/05/01 02:52:00 INFO Executor: Adding file:/tmp/spark-37ed4284-9c8e-42f5-af9c-e583914cc73d/userFiles-8f407a3a-f7ad-4414-b75c-315e3e68d965/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar to class loader\n",
            "23/05/01 02:52:00 INFO Executor: Fetching spark://a39e9f25f235:36985/jars/log4j_log4j-1.2.17.jar with timestamp 1682909517067\n",
            "23/05/01 02:52:00 INFO Utils: Fetching spark://a39e9f25f235:36985/jars/log4j_log4j-1.2.17.jar to /tmp/spark-37ed4284-9c8e-42f5-af9c-e583914cc73d/userFiles-8f407a3a-f7ad-4414-b75c-315e3e68d965/fetchFileTemp7559439034639584516.tmp\n",
            "23/05/01 02:52:00 INFO Utils: /tmp/spark-37ed4284-9c8e-42f5-af9c-e583914cc73d/userFiles-8f407a3a-f7ad-4414-b75c-315e3e68d965/fetchFileTemp7559439034639584516.tmp has been previously copied to /tmp/spark-37ed4284-9c8e-42f5-af9c-e583914cc73d/userFiles-8f407a3a-f7ad-4414-b75c-315e3e68d965/log4j_log4j-1.2.17.jar\n",
            "23/05/01 02:52:00 INFO Executor: Adding file:/tmp/spark-37ed4284-9c8e-42f5-af9c-e583914cc73d/userFiles-8f407a3a-f7ad-4414-b75c-315e3e68d965/log4j_log4j-1.2.17.jar to class loader\n",
            "23/05/01 02:52:00 INFO Executor: Fetching spark://a39e9f25f235:36985/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1682909517067\n",
            "23/05/01 02:52:00 INFO Utils: Fetching spark://a39e9f25f235:36985/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar to /tmp/spark-37ed4284-9c8e-42f5-af9c-e583914cc73d/userFiles-8f407a3a-f7ad-4414-b75c-315e3e68d965/fetchFileTemp4386075284586289126.tmp\n",
            "23/05/01 02:52:00 INFO Utils: /tmp/spark-37ed4284-9c8e-42f5-af9c-e583914cc73d/userFiles-8f407a3a-f7ad-4414-b75c-315e3e68d965/fetchFileTemp4386075284586289126.tmp has been previously copied to /tmp/spark-37ed4284-9c8e-42f5-af9c-e583914cc73d/userFiles-8f407a3a-f7ad-4414-b75c-315e3e68d965/org.xerial.snappy_snappy-java-1.1.7.5.jar\n",
            "23/05/01 02:52:00 INFO Executor: Adding file:/tmp/spark-37ed4284-9c8e-42f5-af9c-e583914cc73d/userFiles-8f407a3a-f7ad-4414-b75c-315e3e68d965/org.xerial.snappy_snappy-java-1.1.7.5.jar to class loader\n",
            "23/05/01 02:52:00 INFO Executor: Fetching spark://a39e9f25f235:36985/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1682909517067\n",
            "23/05/01 02:52:00 INFO Utils: Fetching spark://a39e9f25f235:36985/jars/org.slf4j_slf4j-api-1.7.16.jar to /tmp/spark-37ed4284-9c8e-42f5-af9c-e583914cc73d/userFiles-8f407a3a-f7ad-4414-b75c-315e3e68d965/fetchFileTemp5623674441186726575.tmp\n",
            "23/05/01 02:52:00 INFO Utils: /tmp/spark-37ed4284-9c8e-42f5-af9c-e583914cc73d/userFiles-8f407a3a-f7ad-4414-b75c-315e3e68d965/fetchFileTemp5623674441186726575.tmp has been previously copied to /tmp/spark-37ed4284-9c8e-42f5-af9c-e583914cc73d/userFiles-8f407a3a-f7ad-4414-b75c-315e3e68d965/org.slf4j_slf4j-api-1.7.16.jar\n",
            "23/05/01 02:52:00 INFO Executor: Adding file:/tmp/spark-37ed4284-9c8e-42f5-af9c-e583914cc73d/userFiles-8f407a3a-f7ad-4414-b75c-315e3e68d965/org.slf4j_slf4j-api-1.7.16.jar to class loader\n",
            "23/05/01 02:52:00 INFO Executor: Fetching spark://a39e9f25f235:36985/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1682909517067\n",
            "23/05/01 02:52:00 INFO Utils: Fetching spark://a39e9f25f235:36985/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar to /tmp/spark-37ed4284-9c8e-42f5-af9c-e583914cc73d/userFiles-8f407a3a-f7ad-4414-b75c-315e3e68d965/fetchFileTemp5710083242806370510.tmp\n",
            "23/05/01 02:52:00 INFO Utils: /tmp/spark-37ed4284-9c8e-42f5-af9c-e583914cc73d/userFiles-8f407a3a-f7ad-4414-b75c-315e3e68d965/fetchFileTemp5710083242806370510.tmp has been previously copied to /tmp/spark-37ed4284-9c8e-42f5-af9c-e583914cc73d/userFiles-8f407a3a-f7ad-4414-b75c-315e3e68d965/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar\n",
            "23/05/01 02:52:00 INFO Executor: Adding file:/tmp/spark-37ed4284-9c8e-42f5-af9c-e583914cc73d/userFiles-8f407a3a-f7ad-4414-b75c-315e3e68d965/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar to class loader\n",
            "23/05/01 02:52:00 INFO Executor: Fetching spark://a39e9f25f235:36985/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1682909517067\n",
            "23/05/01 02:52:00 INFO Utils: Fetching spark://a39e9f25f235:36985/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar to /tmp/spark-37ed4284-9c8e-42f5-af9c-e583914cc73d/userFiles-8f407a3a-f7ad-4414-b75c-315e3e68d965/fetchFileTemp7428464246886210626.tmp\n",
            "23/05/01 02:52:00 INFO Utils: /tmp/spark-37ed4284-9c8e-42f5-af9c-e583914cc73d/userFiles-8f407a3a-f7ad-4414-b75c-315e3e68d965/fetchFileTemp7428464246886210626.tmp has been previously copied to /tmp/spark-37ed4284-9c8e-42f5-af9c-e583914cc73d/userFiles-8f407a3a-f7ad-4414-b75c-315e3e68d965/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar\n",
            "23/05/01 02:52:00 INFO Executor: Adding file:/tmp/spark-37ed4284-9c8e-42f5-af9c-e583914cc73d/userFiles-8f407a3a-f7ad-4414-b75c-315e3e68d965/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar to class loader\n",
            "23/05/01 02:52:00 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39593.\n",
            "23/05/01 02:52:00 INFO NettyBlockTransferService: Server created on a39e9f25f235:39593\n",
            "23/05/01 02:52:00 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "23/05/01 02:52:00 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, a39e9f25f235, 39593, None)\n",
            "23/05/01 02:52:00 INFO BlockManagerMasterEndpoint: Registering block manager a39e9f25f235:39593 with 366.3 MiB RAM, BlockManagerId(driver, a39e9f25f235, 39593, None)\n",
            "23/05/01 02:52:00 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, a39e9f25f235, 39593, None)\n",
            "23/05/01 02:52:00 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, a39e9f25f235, 39593, None)\n",
            "23/05/01 02:52:01 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
            "23/05/01 02:52:01 INFO SharedState: Warehouse path is 'file:/content/spark-warehouse'.\n",
            "23/05/01 02:52:03 INFO InMemoryFileIndex: It took 825 ms to list leaf files for 1 paths.\n",
            "23/05/01 02:52:03 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0\n",
            "23/05/01 02:52:03 INFO DAGScheduler: Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "23/05/01 02:52:03 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)\n",
            "23/05/01 02:52:03 INFO DAGScheduler: Parents of final stage: List()\n",
            "23/05/01 02:52:03 INFO DAGScheduler: Missing parents: List()\n",
            "23/05/01 02:52:03 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/05/01 02:52:04 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 104.9 KiB, free 366.2 MiB)\n",
            "23/05/01 02:52:04 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 37.6 KiB, free 366.2 MiB)\n",
            "23/05/01 02:52:04 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on a39e9f25f235:39593 (size: 37.6 KiB, free: 366.3 MiB)\n",
            "23/05/01 02:52:04 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1474\n",
            "23/05/01 02:52:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/05/01 02:52:04 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
            "23/05/01 02:52:04 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (a39e9f25f235, executor driver, partition 0, PROCESS_LOCAL, 4695 bytes) taskResourceAssignments Map()\n",
            "23/05/01 02:52:04 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
            "23/05/01 02:52:05 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 3161 bytes result sent to driver\n",
            "23/05/01 02:52:05 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1033 ms on a39e9f25f235 (executor driver) (1/1)\n",
            "23/05/01 02:52:05 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
            "23/05/01 02:52:05 INFO DAGScheduler: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 1.306 s\n",
            "23/05/01 02:52:05 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "23/05/01 02:52:05 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
            "23/05/01 02:52:05 INFO DAGScheduler: Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 1.412343 s\n",
            "23/05/01 02:52:06 INFO BlockManagerInfo: Removed broadcast_0_piece0 on a39e9f25f235:39593 in memory (size: 37.6 KiB, free: 366.3 MiB)\n",
            "23/05/01 02:52:07 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.\n",
            "23/05/01 02:52:07 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0\n",
            "23/05/01 02:52:07 INFO DAGScheduler: Got job 1 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "23/05/01 02:52:07 INFO DAGScheduler: Final stage: ResultStage 1 (parquet at NativeMethodAccessorImpl.java:0)\n",
            "23/05/01 02:52:07 INFO DAGScheduler: Parents of final stage: List()\n",
            "23/05/01 02:52:07 INFO DAGScheduler: Missing parents: List()\n",
            "23/05/01 02:52:07 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[3] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/05/01 02:52:07 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 104.9 KiB, free 366.2 MiB)\n",
            "23/05/01 02:52:07 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 37.6 KiB, free 366.2 MiB)\n",
            "23/05/01 02:52:07 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on a39e9f25f235:39593 (size: 37.6 KiB, free: 366.3 MiB)\n",
            "23/05/01 02:52:07 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1474\n",
            "23/05/01 02:52:07 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/05/01 02:52:07 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0\n",
            "23/05/01 02:52:07 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (a39e9f25f235, executor driver, partition 0, PROCESS_LOCAL, 4624 bytes) taskResourceAssignments Map()\n",
            "23/05/01 02:52:07 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)\n",
            "23/05/01 02:52:08 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2668 bytes result sent to driver\n",
            "23/05/01 02:52:08 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 69 ms on a39e9f25f235 (executor driver) (1/1)\n",
            "23/05/01 02:52:08 INFO DAGScheduler: ResultStage 1 (parquet at NativeMethodAccessorImpl.java:0) finished in 0.111 s\n",
            "23/05/01 02:52:08 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "23/05/01 02:52:08 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
            "23/05/01 02:52:08 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n",
            "23/05/01 02:52:08 INFO DAGScheduler: Job 1 finished: parquet at NativeMethodAccessorImpl.java:0, took 0.125619 s\n",
            "23/05/01 02:52:08 INFO SparkUI: Stopped Spark web UI at http://a39e9f25f235:4041\n",
            "23/05/01 02:52:08 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "23/05/01 02:52:08 INFO MemoryStore: MemoryStore cleared\n",
            "23/05/01 02:52:08 INFO BlockManager: BlockManager stopped\n",
            "23/05/01 02:52:08 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "23/05/01 02:52:08 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "23/05/01 02:52:08 INFO SparkContext: Successfully stopped SparkContext\n",
            "23/05/01 02:52:08 INFO ShutdownHookManager: Shutdown hook called\n",
            "23/05/01 02:52:08 INFO ShutdownHookManager: Deleting directory /tmp/spark-37ed4284-9c8e-42f5-af9c-e583914cc73d\n",
            "23/05/01 02:52:08 INFO ShutdownHookManager: Deleting directory /tmp/spark-37ed4284-9c8e-42f5-af9c-e583914cc73d/pyspark-c958a8aa-4ede-4160-b9b6-06339e95c8a5\n",
            "23/05/01 02:52:09 INFO ShutdownHookManager: Deleting directory /tmp/spark-816bc7d5-5e8d-4649-a07b-3f1383fec87b\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup AWS Credentials and fill them here. Make sure you do not save this information back to github"
      ],
      "metadata": {
        "id": "dyeTsQLV-B-u"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YDL78heZ__Dz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15303fa0-0025-4d99-dfee-ed5b87681239"
      },
      "source": [
        "%%file join_weather.py\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import date_format, col\n",
        "\n",
        "#create spark context. This is very important. Do this similarly for the other parts\n",
        "# Note to read a file directly from s3 into an rdd you may have to do something like this\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  # replace this line with the s3 pass when testing over EMR (? check proj)\n",
        "  spark = SparkSession.builder.appName(\"join_weather\").getOrCreate()\n",
        "\n",
        "  try:\n",
        "    # review the page rank example for how to use the map operation\n",
        "    # review word count for reduce and add\n",
        "    # see how we use map to parse each row\n",
        "    weather_df = spark.read.option(\"recursiveFileLookup\", \"true\").parquet(\"s3://bd-final/weather_tn.parquet/\")\n",
        "\n",
        "    # read a single parquet file called incidents\n",
        "    incidents_df = spark.read.parquet(\"s3://bd-final/tracts_joined_with_points.parquet\")\n",
        "\n",
        "    df_with_new_column = incidents_df.withColumn(\n",
        "        \"date_local\",\n",
        "        date_format(\"time_local\", \"yyyy-MM-dd:HH\")\n",
        "            .cast(\"string\")\n",
        "    )\n",
        "\n",
        "    df_with_new_column = df_with_new_column.withColumnRenamed(\"date_local\", \"datetime\")\n",
        "\n",
        "    df_with_new_column = df_with_new_column.repartition(col(\"datetime\"))\n",
        "\n",
        "    weather_df = weather_df.repartition(col(\"datetime\"))\n",
        "\n",
        "    joined_df = df_with_new_column.join(\n",
        "        weather_df,\n",
        "        df_with_new_column[\"datetime\"] == weather_df[\"datetime\"],\n",
        "        \"inner\"\n",
        "    )\n",
        "    # repartitioned = joined_df.repartition(10000)\n",
        "    joined_df = joined_df.drop('datetime')\n",
        "    joined_df.coalesce(1).write.parquet(\"s3://bd-final/outputs/joined_weather.parquet\")\n",
        "    # row_count = joined_df.count()\n",
        "    # print(row_count)\n",
        "    # total_size = joined_df.rdd.map(lambda row: len(row.asDict().values())) \\\n",
        "    #                .reduce(lambda x, y: x + y) * df.rdd.getNumPartitions()\n",
        "\n",
        "    # print(\"DataFrame size: {} rows, {} bytes\".format(row_count, total_size))\n",
        "    # joined_df = joined_df.repartition(30)\n",
        "    # joined_df.write.format(\"parquet\").save(\"s3://bd-final/outputs/\")\n",
        "    #  # Getting the schema as a string\n",
        "    # schema_rdd = spark.sparkContext.parallelize([joined_df.schema.treeString()])\n",
        "    # s3_bucket_path = \"s3://bd-final/outputs/\"\n",
        "    # data = [(\"Alice\", 25), (\"Bob\", 30), (\"Charlie\", 35)]\n",
        "    # df = spark.createDataFrame(data, [\"name\", \"age\"])\n",
        "\n",
        "    # # Write the DataFrame to Parquet format\n",
        "    # df.write.parquet(\"s3://bd-final/example.parquet\")\n",
        "\n",
        " \n",
        "  finally:\n",
        "    # very important: stop the context. Otherwise you may get an error that context is still alive. if you are on colab just restart the runtime if you face problem\n",
        "    #finally is used to make sure the context is stopped even with errors\n",
        "    spark.stop()\n",
        "  \n",
        "\n",
        " \n",
        "  \n",
        "  pass"
      ],
      "execution_count": 177,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting join_weather.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bMEsJXTU__Dx"
      },
      "source": [
        "# Please fill your aws credential information here\n",
        "credentials = {\n",
        "    'region_name': 'us-east-1',\n",
        "    'aws_access_key_id': 'ASIA6CQF7SPNHQDLHCPM',\n",
        "    'aws_secret_access_key': 'cE7C/8qCtOlLk22fKbYyMY9TCqk6znODTm7p54Gk',\n",
        "    'aws_session_token': 'FwoGZXIvYXdzEKT//////////wEaDOpjULEGoLKvMbP3XCLOAb1F01bRrNrc+ezxgFpfjPsKc7OxiNTQFSx4YH8HmaV0PvyxSAfujGk5FmsKF63mwOtsIQcr19Bxef8fLRVsin91H2PAaUg0jh2mWkL5Ywd2xpvhBoEm8p7JA9lXF+3zINi9YXu9qCR80t2hEHpbAU7Z4hYk9Mk1FhJApWjzszMLRp5LO32uBfme6aJju3IV8LAiHQnvolPxVsIx3fwdU8LoTeWgTLVaMEr3oa12d3XJoIYYxKdOVuU8OmdRyW8eU0Ns137JvFTjIrWSr/FwKKDUvKIGMi3IPQDoXUxoSrO93NFKvdu0lOvId7Hn4tziuEWIezaNazCER9rjegYo9Tgga7M='\n",
        "}"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yRJjVwtO__Dx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dba48597-ee36-4815-b727-4f9a03a85148"
      },
      "source": [
        "!pip install boto3\n",
        "import boto3, json\n",
        "\n",
        "session = boto3.session.Session(**credentials)\n",
        "s3 = session.client('s3')"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting boto3\n",
            "  Downloading boto3-1.26.123-py3-none-any.whl (135 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m135.6/135.6 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting s3transfer<0.7.0,>=0.6.0\n",
            "  Downloading s3transfer-0.6.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m79.6/79.6 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting botocore<1.30.0,>=1.29.123\n",
            "  Downloading botocore-1.29.123-py3-none-any.whl (10.7 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.7/10.7 MB\u001b[0m \u001b[31m53.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore<1.30.0,>=1.29.123->boto3) (2.8.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /usr/local/lib/python3.10/dist-packages (from botocore<1.30.0,>=1.29.123->boto3) (1.26.15)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.30.0,>=1.29.123->boto3) (1.16.0)\n",
            "Installing collected packages: jmespath, botocore, s3transfer, boto3\n",
            "Successfully installed boto3-1.26.123 botocore-1.29.123 jmespath-1.0.1 s3transfer-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Upload Data to S3"
      ],
      "metadata": {
        "id": "Mss_SpU89_eo"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lljwHyOy__D2"
      },
      "source": [
        "# replae with your EMR cluster ID\n",
        "CLUSTER_ID = 'j-1SKG37QM55G26'\n",
        "\n",
        "def submit_job(app_name, pyfile_uri):\n",
        "    emr = session.client('emr')\n",
        "    emr.add_job_flow_steps(JobFlowId=CLUSTER_ID, Steps=[{\n",
        "        'Name': app_name,\n",
        "        'ActionOnFailure': 'CANCEL_AND_WAIT',\n",
        "        'HadoopJarStep': {\n",
        "            'Args': ['spark-submit',\n",
        "                     '--master', 'yarn',\n",
        "                     '--deploy-mode', 'cluster',\n",
        "                     pyfile_uri],\n",
        "            'Jar': 'command-runner.jar'\n",
        "        }}])"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pCzOGzTj__D1"
      },
      "source": [
        "# upload script to S3\n",
        "s3.upload_file(Filename='join_weather.py', Bucket='bd-final', Key='scripts/join_weather.py')"
      ],
      "execution_count": 178,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eoqkfcEn__D1"
      },
      "source": [
        "# submit spark job to emr\n",
        "submit_job(app_name='join_weather', pyfile_uri='s3://bd-final/scripts/join_weather.py')"
      ],
      "execution_count": 179,
      "outputs": []
    }
  ]
}